import uvicorn
from fastapi import FastAPI, Request, HTTPException
from fastapi.responses import JSONResponse
import logging
import os
import re
import json
from typing import List, Dict, Any
import sys
from contextlib import asynccontextmanager

try:
    import nltk
    from nltk.tokenize import sent_tokenize
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')
except ImportError:
    print("NLTK –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –µ–≥–æ –≤ –≤–∞—à–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ: pip install nltk")
    sys.exit(1)


try:
    import numpy as np
except ImportError:
    print("NumPy –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –µ–≥–æ –≤ –≤–∞—à–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ: pip install numpy")
    sys.exit(1)

project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(project_root)

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π, –∞ –Ω–µ 'copy' —Ñ–∞–π–ª, –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞—è, —á—Ç–æ –º—ã –∏—Ö —Å–ª–∏–ª–∏
from scripts.services.giga_chat_copy import GigaChatAPI

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è ---
GIGACHAT_AUTH_TOKEN = "ZmU1MTI4YWYtMzc0My00ZmU1LThhNzEtMmUyZGI0ZjQzMDlhOmQ1OWQzZmI1LTcwOWItNDEyNS04MGU1LTUwNzFlOTQ3ODk5Zg=="
MAX_DEPTH = 2
TOP_LEVEL_CHUNK_TARGET_SIZE = 20000
LOWER_LEVEL_CHUNK_TARGET_SIZE = 5000
CACHE_HIT_THRESHOLD = 0.99
CACHE_SHORTCUT_THRESHOLD = 0.92

# –§–∞–π–ª—ã —Å –¥–∞–Ω–Ω—ã–º–∏
KNOWLEDGE_BASE_FILE = os.path.join(os.path.dirname(__file__), "knowledge_base.json")
CHUNKS_DATABASE_FILE = os.path.join(os.path.dirname(__file__), "chunks_database.json")

# --- –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ (–∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ) ---
CHUNKS_DB: Dict[str, str] = {}
KNOWLEDGE_BASE: List[Dict[str, Any]] = []
gigachat_client = GigaChatAPI()


@asynccontextmanager
async def lifespan(app: FastAPI):
    logger.info("üöÄ –°–µ—Ä–≤–µ—Ä –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è...")
    global CHUNKS_DB, KNOWLEDGE_BASE
    
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
    if os.path.exists(CHUNKS_DATABASE_FILE):
        with open(CHUNKS_DATABASE_FILE, 'r', encoding='utf-8') as f:
            CHUNKS_DB = json.load(f)
        logger.info(f"‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ '{CHUNKS_DATABASE_FILE}' —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ ({len(CHUNKS_DB)} —á–∞–Ω–∫–æ–≤).")
    else:
        logger.error(f"–ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –§–∞–π–ª —Å –±–∞–∑–æ–π —á–∞–Ω–∫–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω: {CHUNKS_DATABASE_FILE}")
        logger.error("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–ø—É—Å—Ç–∏—Ç–µ —Å–Ω–∞—á–∞–ª–∞ scripts/indexer.py –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –±–∞–∑—ã —á–∞–Ω–∫–æ–≤.")
    
    # 2. –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π (–∫—ç—à–∞ –≤–æ–ø—Ä–æ—Å–æ–≤)
    if os.path.exists(KNOWLEDGE_BASE_FILE):
        with open(KNOWLEDGE_BASE_FILE, 'r', encoding='utf-8') as f:
            KNOWLEDGE_BASE = json.load(f)
        logger.info(f"‚úÖ –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π '{KNOWLEDGE_BASE_FILE}' —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ ({len(KNOWLEDGE_BASE)} –∑–∞–ø–∏—Å–µ–π).")
    
    # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ–∫–µ–Ω–∞ GigaChat
    if not await gigachat_client.get_token(GIGACHAT_AUTH_TOKEN):
        logger.error("–ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ç–æ–∫–µ–Ω GigaChat –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ.")
    
    logger.info("‚ú® Startup complete.")
    yield
    # –ö–æ–¥ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–µ —Å–µ—Ä–≤–µ—Ä–∞ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
    logger.info("üõë –°–µ—Ä–≤–µ—Ä –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è...")


app = FastAPI(title="Pre-indexed RAG MCP Server", version="2.0.0", lifespan=lifespan)


TOOLS_LIST = [
    {
        "name": "answer_question",
        "description": "–ü—Ä–∏–Ω–∏–º–∞–µ—Ç –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –Ω–∞—Ö–æ–¥–∏—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "query": {"type": "string", "description": "–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"}
            },
            "required": ["query"]
        }
    }
]

# --- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ---
def cosine_similarity(v1, v2):
    vec1 = np.array(v1)
    vec2 = np.array(v2)
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    if norm1 == 0 or norm2 == 0:
        return 0.0
    return np.dot(vec1, vec2) / (norm1 * norm2)

def save_knowledge_base():
    with open(KNOWLEDGE_BASE_FILE, 'w', encoding='utf-8') as f:
        json.dump(KNOWLEDGE_BASE, f, ensure_ascii=False, indent=2)

def split_text_into_chunks(text: str, target_size: int) -> List[str]:
    sentences = sent_tokenize(text)
    chunks = []
    current_chunk = ""
    for sentence in sentences:
        if len(current_chunk) + len(sentence) + 1 > target_size and current_chunk:
            chunks.append(current_chunk)
            current_chunk = sentence
        else:
            current_chunk = sentence if not current_chunk else current_chunk + " " + sentence
    if current_chunk:
        chunks.append(current_chunk)
    return chunks

async def route_chunks(question: str, chunks: List[Dict[str, Any]], scratchpad: str, depth: int) -> Dict[str, Any]:
    logger.info(f"==== –†–û–£–¢–ò–ù–ì –ù–ê –ì–õ–£–ë–ò–ù–ï {depth} | –û—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è {len(chunks)} —á–∞–Ω–∫–æ–≤ ====")
    system_message = ("–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç-–Ω–∞–≤–∏–≥–∞—Ç–æ—Ä –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –≤—ã–±—Ä–∞—Ç—å —á–∞–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ —Å –Ω–∞–∏–±–æ–ª—å—à–µ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é —Å–æ–¥–µ—Ä–∂–∞—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. "
                      "–ë—É–¥—å –∏–∑–±–∏—Ä–∞—Ç–µ–ª—å–Ω—ã–º, –Ω–æ –Ω–µ –ø—Ä–æ–ø—É—Å–∫–∞–π –≤–∞–∂–Ω–æ–µ. –¢—ã –û–ë–Ø–ó–ê–ù –æ—Ç–≤–µ—Ç–∏—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ: –†–ê–°–°–£–ñ–î–ï–ù–ò–ï: [—Ç–≤–æ–∏ –º—ã—Å–ª–∏] –í–´–ë–û–†: [JSON-–æ–±—ä–µ–∫—Ç —Å –∫–ª—é—á–æ–º 'chunk_ids']")
    user_message = f"–í–û–ü–†–û–°: {question}\n\n"
    if scratchpad:
        user_message += f"–ü–†–ï–î–´–î–£–©–ò–ï –†–ê–°–°–£–ñ–î–ï–ù–ò–Ø (SCRATCHPAD):\n{scratchpad}\n\n"
    user_message += "–î–û–°–¢–£–ü–ù–´–ï –ß–ê–ù–ö–ò:\n\n"
    for chunk in chunks:
        preview = chunk['text'][:500]
        user_message += f"--- –ß–ê–ù–ö ID: {chunk['id']} ---\n{preview}...\n\n"
    
    response = await gigachat_client.get_chat_completion(system_message, user_message)
    response_text = response.get('response', '')

    reasoning, selection = "", []
    try:
        reasoning_match = re.search(r"–†–ê–°–°–£–ñ–î–ï–ù–ò–ï:(.*?)(–í–´–ë–û–†:|$)", response_text, re.DOTALL | re.IGNORECASE)
        if reasoning_match: reasoning = reasoning_match.group(1).strip()
        selection_match = re.search(r"–í–´–ë–û–†:(.*)", response_text, re.DOTALL | re.IGNORECASE)
        if selection_match:
            json_str = re.sub(r'^```json\n?|\n?```$', '', selection_match.group(1).strip())
            selection = json.loads(json_str).get('chunk_ids', [])
        logger.info(f"–†–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏: {reasoning}")
        logger.info(f"–í—ã–±—Ä–∞–Ω–Ω—ã–µ —á–∞–Ω–∫–∏: {selection}")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Ç–≤–µ—Ç–∞ LLM-—Ä–æ—É—Ç–µ—Ä–∞: {e}\n–û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏: {response_text}")
        return {"selected_ids": [], "scratchpad": scratchpad}

    updated_scratchpad = f"{scratchpad}\n\n–ì–õ–£–ë–ò–ù–ê {depth} –†–ê–°–°–£–ñ–î–ï–ù–ò–ï:\n{reasoning}".strip()
    return {"selected_ids": selection, "scratchpad": updated_scratchpad}

async def synthesize_answer(question: str, context: str) -> str:
    logger.info(f"–°–∏–Ω—Ç–µ–∑ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ {len(context)} —Å–∏–º–≤–æ–ª–æ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.")
    system_message = "–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç-–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç. –û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –ò–°–ö–õ–Æ–ß–ò–¢–ï–õ–¨–ù–û –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–º –Ω–∏–∂–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏. –ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –Ω–∏—á–µ–≥–æ –æ—Ç —Å–µ–±—è. –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç –Ω–µ–ª—å–∑—è –Ω–∞–π—Ç–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ, —Ç–∞–∫ –∏ —Å–∫–∞–∂–∏."
    user_message = f"–ö–û–ù–¢–ï–ö–°–¢:\n{context}\n\n–í–û–ü–†–û–°: {question}"
    final_response = await gigachat_client.get_chat_completion(system_message, user_message)
    return final_response.get('response', "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç.")

async def execute_shortcut_rag(question: str, source_chunk_ids: List[str]) -> str:
    logger.info(f"–ó–∞–ø—É—Å–∫ —É—Å–∫–æ—Ä–µ–Ω–Ω–æ–≥–æ RAG —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º {len(source_chunk_ids)} –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤.")
    context_texts = []
    for chunk_id in source_chunk_ids:
        text = CHUNKS_DB.get(chunk_id)
        if text:
            context_texts.append(text)
        else:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ç–µ–∫—Å—Ç –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ ID —á–∞–Ω–∫–∞: {chunk_id}")

    if not context_texts:
        logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö ID. –ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ–ª–Ω—ã–π RAG.")
        rag_result = await execute_full_rag(question)
        return rag_result['answer']

    context = "\n\n---\n\n".join(context_texts)
    return await synthesize_answer(question, context)

async def execute_full_rag(question: str) -> Dict[str, Any]:
    logger.info(f"–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–≥–æ RAG-—Ü–∏–∫–ª–∞ –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞: '{question}'")
    scratchpad = ""
    top_level_chunks = [{'id': chunk_id, 'text': chunk_text} for chunk_id, chunk_text in CHUNKS_DB.items()]
    current_chunks = top_level_chunks
    for depth in range(MAX_DEPTH + 1):
        result = await route_chunks(question, current_chunks, scratchpad, depth)
        scratchpad, selected_ids = result["scratchpad"], result["selected_ids"]
        selected_chunks = [c for c in current_chunks if c['id'] in selected_ids]
        if not selected_chunks:
            logger.warning(f"–ù–∞–≤–∏–≥–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –Ω–∞ –≥–ª—É–±–∏–Ω–µ {depth}: LLM-—Ä–æ—É—Ç–µ—Ä –Ω–µ –≤—ã–±—Ä–∞–ª –Ω–∏ –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞.")
            current_chunks = []
            break
        current_chunks = selected_chunks
        if depth == MAX_DEPTH:
            break
        next_level_chunks = []
        for chunk in current_chunks:
            sub_chunks = split_text_into_chunks(chunk['text'], LOWER_LEVEL_CHUNK_TARGET_SIZE)
            for i, sub_chunk_text in enumerate(sub_chunks):
                next_level_chunks.append({"id": f"{chunk['id']}_{i}", "text": sub_chunk_text})
        if not next_level_chunks:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –ø–æ–¥-—á–∞–Ω–∫–∏ –Ω–∞ –≥–ª—É–±–∏–Ω–µ {depth}. –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â–∏–µ —á–∞–Ω–∫–∏.")
            break
        current_chunks = next_level_chunks
    if not current_chunks:
        return {"answer": "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —è –Ω–µ —Å–º–æ–≥ –Ω–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –ø–æ –≤–∞—à–µ–º—É –≤–æ–ø—Ä–æ—Å—É.", "source_chunk_ids": []}
    final_context = "\n\n---\n\n".join([c['text'] for c in current_chunks])
    final_answer = await synthesize_answer(question, final_context)
    final_chunk_ids = [c['id'] for c in current_chunks]
    return {"answer": final_answer, "source_chunk_ids": final_chunk_ids}

@app.post("/")
async def json_rpc_handler(request: Request):
    body = await request.json()
    if "jsonrpc" not in body or body["jsonrpc"] != "2.0" or "method" not in body or "id" not in body:
        return JSONResponse(status_code=400, content={"jsonrpc": "2.0", "id": body.get("id"), "error": {"code": -32600, "message": "Invalid Request"}})
    request_id = body["id"]
    method = body["method"]
    params = body.get("params", {})
    try:
        if method == "tools/list": result = {"tools": TOOLS_LIST}
        elif method == "tools/call": result = await handle_tools_call(params)
        else: raise HTTPException(status_code=404, detail=f"Method '{body.get('method')}' not found")
        return JSONResponse(content={"jsonrpc": "2.0", "id": request_id, "result": result})
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –º–µ—Ç–æ–¥–∞ '{method}': {e}", exc_info=True)
        return JSONResponse(status_code=500, content={"jsonrpc": "2.0", "id": request_id, "error": {"code": -32603, "message": "Internal Error", "data": str(e)}})

async def handle_tools_call(params: dict):
    tool_name = params.get("name")
    arguments = params.get("arguments", {})
    if tool_name == "answer_question":
        query = arguments.get("query")
        if not query: raise ValueError("–î–ª—è 'answer_question' —Ç—Ä–µ–±—É–µ—Ç—Å—è –∞—Ä–≥—É–º–µ–Ω—Ç 'query'")
        if not gigachat_client.access_token:
             raise Exception("–¢–æ–∫–µ–Ω GigaChat –Ω–µ –±—ã–ª –ø–æ–ª—É—á–µ–Ω. –°–µ—Ä–≤–µ—Ä –Ω–µ –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ.")
        query_vector = await gigachat_client.get_embedding(query)
        if not query_vector: raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞.")
        best_match = None
        highest_similarity = -1.0
        for entry in KNOWLEDGE_BASE:
            similarity = cosine_similarity(query_vector, entry['vector'])
            if similarity > highest_similarity:
                highest_similarity = similarity
                best_match = entry
        final_answer = ""
        if best_match and highest_similarity >= CACHE_SHORTCUT_THRESHOLD:
            if highest_similarity >= CACHE_HIT_THRESHOLD:
                logger.info(f"CACHE HIT: –°—Ö–æ–¥—Å—Ç–≤–æ ({highest_similarity:.2f}). –û—Ç–¥–∞–µ–º –≥–æ—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç.")
                final_answer = best_match['answer']
            else:
                logger.info(f"SHORTCUT: –°—Ö–æ–¥—Å—Ç–≤–æ ({highest_similarity:.2f}). –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥–æ—Ç–æ–≤—ã–µ —á–∞–Ω–∫–∏.")
                final_answer = await execute_shortcut_rag(query, best_match['source_chunk_ids'])
        else:
            logger.info("CACHE MISS: –ü–æ—Ö–æ–∂–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –∑–∞–ø—É—Å–∫–∞–µ–º –ø–æ–ª–Ω—ã–π RAG-—Ü–∏–∫–ª.")
            rag_result = await execute_full_rag(query)
            final_answer = rag_result['answer']
            not_found_message = "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —è –Ω–µ —Å–º–æ–≥ –Ω–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é"
            if not_found_message not in final_answer:
                KNOWLEDGE_BASE.append({
                    "question": query,
                    "vector": query_vector,
                    "answer": final_answer,
                    "source_chunk_ids": rag_result['source_chunk_ids']
                })
                save_knowledge_base()
                logger.info("–ù–æ–≤—ã–π —É—Å–ø–µ—à–Ω—ã–π –æ—Ç–≤–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π.")
        return {"content": [{"type": "text", "text": json.dumps(final_answer)}], "isError": False}
    else:
        raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ –∏–º—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞: {tool_name}")

if __name__ == "__main__":
    print("üöÄ –ó–∞–ø—É—Å–∫ Pre-indexed RAG MCP Server –Ω–∞ http://localhost:8002")
    uvicorn.run(app, host="0.0.0.0", port=8002)
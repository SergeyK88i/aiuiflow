import os
import json
import sys
from typing import List

# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å NLTK –∏ —Å–∫–∞—á–∞—Ç—å –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∞–Ω–Ω—ã–µ
try:
    import nltk
    from nltk.tokenize import sent_tokenize
    nltk.data.find('tokenizers/punkt')
except LookupError:
    print("NLTK 'punkt' –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –°–∫–∞—á–∏–≤–∞–µ–º...")
    nltk.download('punkt')
    from nltk.tokenize import sent_tokenize

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–æ–≤, –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.append(project_root)

# --- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è ---
# –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
DOCS_DIR = os.path.join(project_root, 'docs')
# –ò–º—è —Ñ–∞–π–ª–∞, –∫—É–¥–∞ –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã —á–∞–Ω–∫–∏
CHUNKS_OUTPUT_FILE = os.path.join(os.path.dirname(__file__), 'chunks_database.json')
# –¶–µ–ª–µ–≤–æ–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö (–¥–æ–ª–∂–µ–Ω —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å –∫–æ–Ω—Ñ–∏–≥–æ–º —Å–µ—Ä–≤–µ—Ä–∞)
CHUNK_TARGET_SIZE = 20000


def split_text_into_chunks(text: str, target_size: int) -> List[str]:
    """–î–µ–ª–∏—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏, —Å–æ—Ö—Ä–∞–Ω—è—è —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π."""
    sentences = sent_tokenize(text, language='russian') # –£–∫–∞–∑—ã–≤–∞–µ–º —è–∑—ã–∫ –¥–ª—è –ª—É—á—à–µ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
    chunks = []
    current_chunk = ""
    for sentence in sentences:
        if len(current_chunk) + len(sentence) + 1 > target_size and current_chunk:
            chunks.append(current_chunk.strip())
            current_chunk = sentence
        else:
            current_chunk += " " + sentence
    if current_chunk:
        chunks.append(current_chunk.strip())
    return chunks

def main():
    """
    –ß–∏—Ç–∞–µ—Ç –≤—Å–µ markdown-–¥–æ–∫—É–º–µ–Ω—Ç—ã, –Ω–∞—Ä–µ–∑–∞–µ—Ç –∏—Ö –Ω–∞ —á–∞–Ω–∫–∏ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç
    –≤ JSON-—Ñ–∞–π–ª –≤ –≤–∏–¥–µ —Å–ª–æ–≤–∞—Ä—è {chunk_id: chunk_text}.
    """
    print("üöÄ –ó–∞–ø—É—Å–∫ –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä–∞...")
    if not os.path.exists(DOCS_DIR):
        print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {DOCS_DIR}")
        return

    chunks_database = {}
    
    for filename in os.listdir(DOCS_DIR):
        if filename.endswith(".md"):
            file_path = os.path.join(DOCS_DIR, filename)
            print(f"üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {filename}")
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    text = f.read()
                
                chunks = split_text_into_chunks(text, CHUNK_TARGET_SIZE)
                
                for i, chunk_text in enumerate(chunks):
                    chunk_id = f"{filename}_{i}"
                    chunks_database[chunk_id] = chunk_text
                
                print(f"   -> –°–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤.")

            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞ {filename}: {e}")

    try:
        with open(CHUNKS_OUTPUT_FILE, 'w', encoding='utf-8') as f:
            json.dump(chunks_database, f, ensure_ascii=False, indent=2)
        print(f"\n‚úÖ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {CHUNKS_OUTPUT_FILE}")
        print(f"   –í—Å–µ–≥–æ —Å–æ–∑–¥–∞–Ω–æ {len(chunks_database)} —á–∞–Ω–∫–æ–≤.")
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ —Å —á–∞–Ω–∫–∞–º–∏: {e}")


if __name__ == "__main__":
    main()

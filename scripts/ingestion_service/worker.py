import asyncio
import asyncpg
import logging
import os
import json
import argparse # <-- Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð¿Ð°Ñ€ÑÐµÑ€ Ð°Ñ€Ð³ÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²
from typing import Dict, Any

# Ð­Ñ‚Ð¸ Ð¼Ð¾Ð´ÑƒÐ»Ð¸ Ð¼Ñ‹ ÑÐ¾Ð·Ð´Ð°Ð´Ð¸Ð¼ Ð½Ð° ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ñ… ÑˆÐ°Ð³Ð°Ñ…
from .loaders import load_data_from_source
from .processing import process_text_to_chunks

# --- ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

DATABASE_URL = os.getenv("DATABASE_URL", "postgresql://user:password@localhost:5432/dbname")
WORKER_SLEEP_INTERVAL = 10  # Ð¡ÐµÐºÑƒÐ½Ð´

async def process_job(job: Dict[str, Any], db_pool: asyncpg.Pool):
    """ÐžÑÐ½Ð¾Ð²Ð½Ð°Ñ Ð»Ð¾Ð³Ð¸ÐºÐ° Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ð¾Ð´Ð½Ð¾Ð¹ Ð·Ð°Ð´Ð°Ñ‡Ð¸."""
    job_id = job['id']
    logger.info(f"[Job {job_id}] ÐÐ°Ñ‡Ð¸Ð½Ð°ÑŽ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÑƒ Ð·Ð°Ð´Ð°Ñ‡Ð¸...")
    logs = [f"[{job_id}] Worker started processing."]

    try:
        # --- Ð¨Ð°Ð³ 1: Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ (Extract) ---
        logger.info(f"[Job {job_id}] Ð¨Ð°Ð³ 1: Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸Ð· {job['source_url']}")
        raw_text = await load_data_from_source(job['source_type'], job['source_url'])
        logs.append(f"Successfully extracted {len(raw_text)} characters.")

        # --- Ð¨Ð°Ð³ 2: Ð¢Ñ€Ð°Ð½ÑÑ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ (Transform) ---
        logger.info(f"[Job {job_id}] Ð¨Ð°Ð³ 2: ÐÐ°Ñ€ÐµÐ·ÐºÐ° Ð½Ð° Ñ‡Ð°Ð½ÐºÐ¸ Ð¸ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¾Ð².")
        # Ð’ ÑÑ‚Ð¾Ð¹ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸ Ð±ÑƒÐ´ÐµÑ‚ Ð²ÑÑ Ð»Ð¾Ð³Ð¸ÐºÐ°: Ð½Ð°Ñ€ÐµÐ·ÐºÐ°, Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ, ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¸
        chunks_to_insert = await process_text_to_chunks(raw_text, str(job['source_url']))
        logs.append(f"Processed into {len(chunks_to_insert)} chunks.")

        # --- Ð¨Ð°Ð³ 3: Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° (Load) ---
        logger.info(f"[Job {job_id}] Ð¨Ð°Ð³ 3: Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ñ‡Ð°Ð½ÐºÐ¾Ð² Ð² Ð±Ð°Ð·Ñƒ Ð´Ð°Ð½Ð½Ñ‹Ñ….")
        async with db_pool.acquire() as connection:
            # Ð£Ð´Ð°Ð»ÑÐµÐ¼ ÑÑ‚Ð°Ñ€Ñ‹Ðµ Ñ‡Ð°Ð½ÐºÐ¸ Ð´Ð»Ñ ÑÑ‚Ð¾Ð³Ð¾ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¸Ð·Ð±ÐµÐ¶Ð°Ñ‚ÑŒ Ð´ÑƒÐ±Ð»ÐµÐ¹
            await connection.execute("DELETE FROM chunks WHERE doc_name = $1", str(job['source_url']))
            
            # Ð“Ð¾Ñ‚Ð¾Ð²Ð¸Ð¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð´Ð»Ñ Ð¼Ð°ÑÑÐ¾Ð²Ð¾Ð¹ Ð²ÑÑ‚Ð°Ð²ÐºÐ¸
            data_to_insert = [
                (c['id'], c['doc_name'], c['chunk_sequence_num'], c['header_1'], c['header_2'], c['chunk_text'], c['embedding'])
                for c in chunks_to_insert
            ]

            # Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÐ¼ Ð¼Ð°ÑÑÐ¾Ð²ÑƒÑŽ Ð²ÑÑ‚Ð°Ð²ÐºÑƒ
            await connection.copy_records_to_table(
                'chunks',
                records=data_to_insert,
                columns=['id', 'doc_name', 'chunk_sequence_num', 'header_1', 'header_2', 'chunk_text', 'embedding']
            )
        logs.append(f"Successfully saved {len(chunks_to_insert)} chunks to the database.")
        
        # --- Ð¤Ð¸Ð½Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ ---
        final_status = 'completed'
        logger.info(f"[Job {job_id}] âœ… Ð—Ð°Ð´Ð°Ñ‡Ð° ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð°.")

    except Exception as e:
        logger.error(f"[Job {job_id}] âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ð¸ Ð·Ð°Ð´Ð°Ñ‡Ð¸: {e}", exc_info=True)
        logs.append(f"ERROR: {str(e)}")
        final_status = 'failed'
    
    # ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ ÑÑ‚Ð°Ñ‚ÑƒÑ Ð¸ Ð»Ð¾Ð³ Ð·Ð°Ð´Ð°Ñ‡Ð¸ Ð² Ð‘Ð”
    async with db_pool.acquire() as connection:
        await connection.execute(
            "UPDATE ingestion_jobs SET status = $1, logs = $2, finished_at = NOW() WHERE id = $3",
            final_status, '\n'.join(logs), job_id
        )

async def main_loop(queue_name: str):
    """Ð‘ÐµÑÐºÐ¾Ð½ÐµÑ‡Ð½Ñ‹Ð¹ Ñ†Ð¸ÐºÐ» Ð²Ð¾Ñ€ÐºÐµÑ€Ð° Ð´Ð»Ñ Ð¿Ð¾Ð¸ÑÐºÐ° Ð¸ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ð·Ð°Ð´Ð°Ñ‡ Ð² ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ð¾Ð¹ Ð¾Ñ‡ÐµÑ€ÐµÐ´Ð¸."""
    logger.info(f"ðŸ› ï¸ Ð’Ð¾Ñ€ÐºÐµÑ€ Ð·Ð°Ð¿ÑƒÑ‰ÐµÐ½. Ð¡Ð»ÑƒÑˆÐ°ÑŽ Ð¾Ñ‡ÐµÑ€ÐµÐ´ÑŒ: '{queue_name}'...")
    db_pool = await asyncpg.create_pool(DATABASE_URL)

    while True:
        try:
            async with db_pool.acquire() as connection:
                # Ð˜Ñ‰ÐµÐ¼ Ð¸ Ð±Ð»Ð¾ÐºÐ¸Ñ€ÑƒÐµÐ¼ Ð·Ð°Ð´Ð°Ñ‡Ñƒ Ð¸Ð· ÐÐÐ¨Ð•Ð™ ÐžÐ§Ð•Ð Ð•Ð”Ð˜
                job = await connection.fetchrow(
                    """
                    SELECT id, source_url, source_type FROM ingestion_jobs
                    WHERE status = 'pending' AND queue_name = $1
                    ORDER BY created_at
                    FOR UPDATE SKIP LOCKED
                    LIMIT 1
                    """,
                    queue_name
                )
                if job:
                    await connection.execute("UPDATE ingestion_jobs SET status = 'processing' WHERE id = $1", job['id'])
            
            if job:
                await process_job(job, db_pool)
            else:
                # Ð•ÑÐ»Ð¸ Ð·Ð°Ð´Ð°Ñ‡ Ð½ÐµÑ‚, Ð¶Ð´ÐµÐ¼
                await asyncio.sleep(WORKER_SLEEP_INTERVAL)

        except Exception as e:
            logger.error(f"ÐšÑ€Ð¸Ñ‚Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ° Ð² Ð¾ÑÐ½Ð¾Ð²Ð½Ð¾Ð¼ Ñ†Ð¸ÐºÐ»Ðµ Ð²Ð¾Ñ€ÐºÐµÑ€Ð° (Ð¾Ñ‡ÐµÑ€ÐµÐ´ÑŒ '{queue_name}'): {e}", exc_info=True)
            await asyncio.sleep(WORKER_SLEEP_INTERVAL) # Ð–Ð´ÐµÐ¼ Ð¿ÐµÑ€ÐµÐ´ Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€Ð½Ð¾Ð¹ Ð¿Ð¾Ð¿Ñ‹Ñ‚ÐºÐ¾Ð¹

if __name__ == "__main__":
    # --- ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ Ð°Ñ€Ð³ÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð² ÐºÐ¾Ð¼Ð°Ð½Ð´Ð½Ð¾Ð¹ ÑÑ‚Ñ€Ð¾ÐºÐ¸ ---
    parser = argparse.ArgumentParser(description="Ð—Ð°Ð¿ÑƒÑÐºÐ°ÐµÑ‚ Ð²Ð¾Ñ€ÐºÐµÑ€ Ð´Ð»Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ð·Ð°Ð´Ð°Ñ‡ Ð¸Ð· ÑƒÐºÐ°Ð·Ð°Ð½Ð½Ð¾Ð¹ Ð¾Ñ‡ÐµÑ€ÐµÐ´Ð¸.")
    parser.add_argument("--queue", type=str, required=True, help="Ð˜Ð¼Ñ Ð¾Ñ‡ÐµÑ€ÐµÐ´Ð¸ Ð´Ð»Ñ Ð¿Ñ€Ð¾ÑÐ»ÑƒÑˆÐ¸Ð²Ð°Ð½Ð¸Ñ (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, 'pdf' Ð¸Ð»Ð¸ 'website')")
    args = parser.parse_args()

    try:
        asyncio.run(main_loop(args.queue))
    except KeyboardInterrupt:
        logger.info(f"Ð’Ð¾Ñ€ÐºÐµÑ€ Ð´Ð»Ñ Ð¾Ñ‡ÐµÑ€ÐµÐ´Ð¸ '{args.queue}' Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½ Ð²Ñ€ÑƒÑ‡Ð½ÑƒÑŽ.")
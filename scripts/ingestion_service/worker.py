import asyncio
import asyncpg
import logging
import os
import json
from typing import Dict, Any

# –≠—Ç–∏ –º–æ–¥—É–ª–∏ –º—ã —Å–æ–∑–¥–∞–¥–∏–º –Ω–∞ —Å–ª–µ–¥—É—é—â–∏—Ö —à–∞–≥–∞—Ö
from .data_loaders import load_data_from_source
from .processing import process_text_to_chunks

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∞ ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

DATABASE_URL = os.getenv("DATABASE_URL", "postgresql://user:password@localhost:5432/dbname")
WORKER_SLEEP_INTERVAL = 10  # –°–µ–∫—É–Ω–¥

async def process_job(job: Dict[str, Any], db_pool: asyncpg.Pool):
    """–û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–π –∑–∞–¥–∞—á–∏."""
    job_id = job['id']
    logger.info(f"[Job {job_id}] –ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É –∑–∞–¥–∞—á–∏...")
    logs = [f"[{job_id}] Worker started processing."]

    try:
        # --- –®–∞–≥ 1: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ (Extract) ---
        logger.info(f"[Job {job_id}] –®–∞–≥ 1: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {job['source_url']}")
        raw_text = await load_data_from_source(job['source_type'], job['source_url'])
        logs.append(f"Successfully extracted {len(raw_text)} characters.")

        # --- –®–∞–≥ 2: –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è (Transform) ---
        logger.info(f"[Job {job_id}] –®–∞–≥ 2: –ù–∞—Ä–µ–∑–∫–∞ –Ω–∞ —á–∞–Ω–∫–∏ –∏ –ø–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.")
        # –í —ç—Ç–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –±—É–¥–µ—Ç –≤—Å—è –ª–æ–≥–∏–∫–∞: –Ω–∞—Ä–µ–∑–∫–∞, –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ, —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        chunks_to_insert = await process_text_to_chunks(raw_text, str(job['source_url']))
        logs.append(f"Processed into {len(chunks_to_insert)} chunks.")

        # --- –®–∞–≥ 3: –ó–∞–≥—Ä—É–∑–∫–∞ (Load) ---
        logger.info(f"[Job {job_id}] –®–∞–≥ 3: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö.")
        async with db_pool.acquire() as connection:
            # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —á–∞–Ω–∫–∏ –¥–ª—è —ç—Ç–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –¥—É–±–ª–µ–π
            await connection.execute("DELETE FROM chunks WHERE doc_name = $1", str(job['source_url']))
            
            # –ì–æ—Ç–æ–≤–∏–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –º–∞—Å—Å–æ–≤–æ–π –≤—Å—Ç–∞–≤–∫–∏
            data_to_insert = [
                (c['id'], c['doc_name'], c['chunk_sequence_num'], c['header_1'], c['header_2'], c['chunk_text'], c['embedding'])
                for c in chunks_to_insert
            ]
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –º–∞—Å—Å–æ–≤—É—é –≤—Å—Ç–∞–≤–∫—É
            await connection.copy_records_to_table(
                'chunks',
                records=data_to_insert,
                columns=['id', 'doc_name', 'chunk_sequence_num', 'header_1', 'header_2', 'chunk_text', 'embedding']
            )
        logs.append(f"Successfully saved {len(chunks_to_insert)} chunks to the database.")
        
        # --- –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è ---
        final_status = 'completed'
        logger.info(f"[Job {job_id}] ‚úÖ –ó–∞–¥–∞—á–∞ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")

    except Exception as e:
        logger.error(f"[Job {job_id}] ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∑–∞–¥–∞—á–∏: {e}", exc_info=True)
        logs.append(f"ERROR: {e}")
        final_status = 'failed'
    
    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –∏ –ª–æ–≥ –∑–∞–¥–∞—á–∏ –≤ –ë–î
    async with db_pool.acquire() as connection:
        await connection.execute(
            "UPDATE ingestion_jobs SET status = $1, logs = $2, finished_at = NOW() WHERE id = $3",
            final_status, '\n'.join(logs), job_id
        )

async def main_loop():
    """–ë–µ—Å–∫–æ–Ω–µ—á–Ω—ã–π —Ü–∏–∫–ª –≤–æ—Ä–∫–µ—Ä–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–¥–∞—á."""
    logger.info("üõ†Ô∏è –í–æ—Ä–∫–µ—Ä –∑–∞–ø—É—â–µ–Ω. –ò—â—É –Ω–æ–≤—ã–µ –∑–∞–¥–∞—á–∏...")
    db_pool = await asyncpg.create_pool(DATABASE_URL)

    while True:
        try:
            async with db_pool.acquire() as connection:
                # –ò—â–µ–º –∏ —Å—Ä–∞–∑—É –±–ª–æ–∫–∏—Ä—É–µ–º –æ–¥–Ω—É –∑–∞–¥–∞—á—É, —á—Ç–æ–±—ã –¥—Ä—É–≥–∏–µ –≤–æ—Ä–∫–µ—Ä—ã –µ–µ –Ω–µ –≤–∑—è–ª–∏
                job = await connection.fetchrow(
                    """
                    SELECT id, source_url, source_type FROM ingestion_jobs
                    WHERE status = 'pending'
                    ORDER BY created_at
                    FOR UPDATE SKIP LOCKED
                    LIMIT 1
                    """
                )
                if job:
                    await connection.execute("UPDATE ingestion_jobs SET status = 'processing' WHERE id = $1", job['id'])
            
            if job:
                await process_job(job, db_pool)
            else:
                # –ï—Å–ª–∏ –∑–∞–¥–∞—á –Ω–µ—Ç, –∂–¥–µ–º
                await asyncio.sleep(WORKER_SLEEP_INTERVAL)

        except Exception as e:
            logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Ü–∏–∫–ª–µ –≤–æ—Ä–∫–µ—Ä–∞: {e}", exc_info=True)
            await asyncio.sleep(WORKER_SLEEP_INTERVAL) # –ñ–¥–µ–º –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–æ–π

if __name__ == "__main__":
    try:
        asyncio.run(main_loop())
    except KeyboardInterrupt:
        logger.info("–í–æ—Ä–∫–µ—Ä –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤—Ä—É—á–Ω—É—é.")

import uvicorn
from fastapi import FastAPI, Request, HTTPException
from fastapi.responses import JSONResponse
import logging
import os
import re
import json
from typing import List, Dict, Any
import sys

# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å numpy. –ï—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç, —ç—Ç–æ –≤—ã–∑–æ–≤–µ—Ç –æ—à–∏–±–∫—É –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ.
# –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –µ–≥–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –≤ —Å–≤–æ–µ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ.
try:
    import numpy as np
except ImportError:
    print("NumPy –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –µ–≥–æ –≤ –≤–∞—à–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ: pip install numpy")
    sys.exit(1)

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–æ–≤
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(project_root)

# –†–µ—à–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã —Å SSL –¥–ª—è NLTK
try:
    import nltk
    import ssl
    try:
        _create_unverified_https_context = ssl._create_unverified_context
    except AttributeError:
        pass
    else:
        ssl._create_default_https_context = _create_unverified_https_context
    nltk.download('punkt_tab', quiet=True)
    from nltk.tokenize import sent_tokenize
except ImportError:
    print("NLTK –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install nltk")
    sys.exit(1)

from scripts.services.giga_chat import GigaChatAPI

# --- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è ---
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

GIGACHAT_AUTH_TOKEN = "ZmU1MTI4YWYtMzc0My00ZmU1LThhNzEtMmUyZGI0ZjQzMDlhOmQ1OWQzZmI1LTcwOWItNDEyNS04MGU1LTUwNzFlOTQ3ODk5Zg=="
MAX_DEPTH = 2
TOP_LEVEL_CHUNK_TARGET_SIZE = 20000
LOWER_LEVEL_CHUNK_TARGET_SIZE = 5000

CACHE_HIT_THRESHOLD = 0.99
CACHE_SHORTCUT_THRESHOLD = 0.92

KNOWLEDGE_BASE_FILE = "knowledge_base.json"

app = FastAPI(title="Advanced RAG MCP Server", version="1.0.0")

# --- –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ ---
DOCUMENT_STORE: Dict[str, str] = {}
KNOWLEDGE_BASE: List[Dict[str, Any]] = []
gigachat_client = GigaChatAPI()

# --- –û–ø–∏—Å–∞–Ω–∏–µ "—É–º–µ–Ω–∏–π" —Å–µ—Ä–≤–µ—Ä–∞ ---
TOOLS_LIST = [
    {
        "name": "answer_question",
        "description": "–ü—Ä–∏–Ω–∏–º–∞–µ—Ç –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –Ω–∞—Ö–æ–¥–∏—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç.",
        "inputSchema": {
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"
                }
            },
            "required": ["query"]
        }
    }
]

# --- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ---
def cosine_similarity(v1, v2):
    vec1 = np.array(v1)
    vec2 = np.array(v2)
    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))

def save_knowledge_base():
    with open(KNOWLEDGE_BASE_FILE, 'w', encoding='utf-8') as f:
        json.dump(KNOWLEDGE_BASE, f, ensure_ascii=False, indent=2)

def split_text_into_chunks(text: str, target_size: int) -> List[str]:
    sentences = sent_tokenize(text)
    chunks = []
    current_chunk = ""
    for sentence in sentences:
        if len(current_chunk) + len(sentence) + 1 > target_size and current_chunk:
            chunks.append(current_chunk)
            current_chunk = sentence
        else:
            current_chunk = sentence if not current_chunk else current_chunk + " " + sentence
    if current_chunk:
        chunks.append(current_chunk)
    return chunks

# --- –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ RAG ---

async def route_chunks(question: str, chunks: List[Dict[str, Any]], scratchpad: str, depth: int) -> Dict[str, Any]:
    logger.info(f"==== –†–û–£–¢–ò–ù–ì –ù–ê –ì–õ–£–ë–ò–ù–ï {depth} | –û—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è {len(chunks)} —á–∞–Ω–∫–æ–≤ ====")
    system_message = ("–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç-–Ω–∞–≤–∏–≥–∞—Ç–æ—Ä –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –≤—ã–±—Ä–∞—Ç—å —á–∞–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ —Å –Ω–∞–∏–±–æ–ª—å—à–µ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é —Å–æ–¥–µ—Ä–∂–∞—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. "
                      "–ë—É–¥—å –∏–∑–±–∏—Ä–∞—Ç–µ–ª—å–Ω—ã–º, –Ω–æ –Ω–µ –ø—Ä–æ–ø—É—Å–∫–∞–π –≤–∞–∂–Ω–æ–µ. –¢—ã –û–ë–Ø–ó–ê–ù –æ—Ç–≤–µ—Ç–∏—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ: –†–ê–°–°–£–ñ–î–ï–ù–ò–ï: [—Ç–≤–æ–∏ –º—ã—Å–ª–∏] –í–´–ë–û–†: [JSON-–æ–±—ä–µ–∫—Ç —Å –∫–ª—é—á–æ–º 'chunk_ids']")
    user_message = f"–í–û–ü–†–û–°: {question}\n\n"
    if scratchpad:
        user_message += f"–ü–†–ï–î–´–î–£–©–ò–ï –†–ê–°–°–£–ñ–î–ï–ù–ò–Ø (SCRATCHPAD):\n{scratchpad}\n\n"
    user_message += "–î–û–°–¢–£–ü–ù–´–ï –ß–ê–ù–ö–ò:\n\n"
    for chunk in chunks:
        preview = chunk['text'][:500]
        user_message += f"--- –ß–ê–ù–ö ID: {chunk['id']} ---\n{preview}...\n\n"
    
    response = await gigachat_client.get_chat_completion(system_message, user_message)
    response_text = response.get('response', '')

    reasoning, selection = "", []
    try:
        reasoning_match = re.search(r"–†–ê–°–°–£–ñ–î–ï–ù–ò–ï:(.*?)(–í–´–ë–û–†:|$)", response_text, re.DOTALL | re.IGNORECASE)
        if reasoning_match: reasoning = reasoning_match.group(1).strip()
        selection_match = re.search(r"–í–´–ë–û–†:(.*)", response_text, re.DOTALL | re.IGNORECASE)
        if selection_match:
            json_str = re.sub(r'^```json\n?|\n?```$', '', selection_match.group(1).strip())
            selection = json.loads(json_str).get('chunk_ids', [])
        logger.info(f"–†–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏: {reasoning}")
        logger.info(f"–í—ã–±—Ä–∞–Ω–Ω—ã–µ —á–∞–Ω–∫–∏: {selection}")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Ç–≤–µ—Ç–∞ LLM-—Ä–æ—É—Ç–µ—Ä–∞: {e}\n–û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏: {response_text}")
        return {"selected_ids": [], "scratchpad": scratchpad}

    updated_scratchpad = f"{scratchpad}\n\n–ì–õ–£–ë–ò–ù–ê {depth} –†–ê–°–°–£–ñ–î–ï–ù–ò–ï:\n{reasoning}".strip()
    return {"selected_ids": selection, "scratchpad": updated_scratchpad}

async def synthesize_answer(question: str, context: str) -> str:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞."""
    logger.info(f"–°–∏–Ω—Ç–µ–∑ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ {len(context)} —Å–∏–º–≤–æ–ª–æ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.")
    system_message = "–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç-–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç. –û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –ò–°–ö–õ–Æ–ß–ò–¢–ï–õ–¨–ù–û –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–º –Ω–∏–∂–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏. –ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –Ω–∏—á–µ–≥–æ –æ—Ç —Å–µ–±—è. –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç –Ω–µ–ª—å–∑—è –Ω–∞–π—Ç–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ, —Ç–∞–∫ –∏ —Å–∫–∞–∂–∏."
    user_message = f"–ö–û–ù–¢–ï–ö–°–¢:\n{context}\n\n–í–û–ü–†–û–°: {question}"
    final_response = await gigachat_client.get_chat_completion(system_message, user_message)
    return final_response.get('response', "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç.")

async def execute_full_rag(question: str) -> Dict[str, Any]:
    logger.info(f"–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ RAG-—Ü–∏–∫–ª–∞ –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞: '{question}'")
    scratchpad = ""
    top_level_chunks = []
    for doc_name, text in DOCUMENT_STORE.items():
        for i, chunk_text in enumerate(split_text_into_chunks(text, TOP_LEVEL_CHUNK_TARGET_SIZE)):
            top_level_chunks.append({"id": f"{doc_name}_{i}", "text": chunk_text})
    
    current_chunks = top_level_chunks
    for depth in range(MAX_DEPTH + 1):
        result = await route_chunks(question, current_chunks, scratchpad, depth)
        scratchpad, selected_ids = result["scratchpad"], result["selected_ids"]
        selected_chunks = [c for c in current_chunks if c['id'] in selected_ids]
        if not selected_chunks:
            logger.warning("–ù–∞–≤–∏–≥–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: –Ω–∞ –æ–¥–Ω–æ–º –∏–∑ —É—Ä–æ–≤–Ω–µ–π –Ω–µ –±—ã–ª–æ –≤—ã–±—Ä–∞–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞.")
            current_chunks = [] # –Ø–≤–Ω–æ –æ–±–Ω—É–ª—è–µ–º, —á—Ç–æ–±—ã –≤–µ—Ä–Ω—É—Ç—å –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç
            break
        current_chunks = selected_chunks
        if depth == MAX_DEPTH:
            break
        next_level_chunks = []
        for chunk in current_chunks:
            for i, sub_chunk_text in enumerate(split_text_into_chunks(chunk['text'], LOWER_LEVEL_CHUNK_TARGET_SIZE)):
                next_level_chunks.append({"id": f"{chunk['id']}_{i}", "text": sub_chunk_text})
        current_chunks = next_level_chunks

    if not current_chunks:
        return {"answer": "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —è –Ω–µ —Å–º–æ–≥ –Ω–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –ø–æ –≤–∞—à–µ–º—É –≤–æ–ø—Ä–æ—Å—É.", "source_chunk_ids": []}

    final_context = "\n\n---\n\n".join([c['text'] for c in current_chunks])
    final_answer = await synthesize_answer(question, final_context)
    final_chunk_ids = [c['id'] for c in current_chunks]
    return {"answer": final_answer, "source_chunk_ids": final_chunk_ids}

async def execute_shortcut_rag(question: str, source_chunk_ids: List[str]) -> str:
    logger.info(f"–ó–∞–ø—É—Å–∫ —É—Å–∫–æ—Ä–µ–Ω–Ω–æ–≥–æ RAG —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º {len(source_chunk_ids)} –≥–æ—Ç–æ–≤—ã—Ö —á–∞–Ω–∫–æ–≤.")
    # –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –¥–æ–ª–∂–Ω–∞ –±—É–¥–µ—Ç –Ω–∞—Ö–æ–¥–∏—Ç—å —Ç–µ–∫—Å—Ç —á–∞–Ω–∫–æ–≤ –ø–æ –∏—Ö ID. –í –Ω–∞—à–µ–π –ø—Ä–æ—Å—Ç–æ–π –º–æ–¥–µ–ª–∏ —ç—Ç–æ —Å–ª–æ–∂–Ω–æ.
    # –ü–æ—ç—Ç–æ–º—É –º—ã —Å–∏–º—É–ª–∏—Ä—É–µ–º, –ø—Ä–æ—Å—Ç–æ –≥–µ–Ω–µ—Ä–∏—Ä—É—è –Ω–æ–≤—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞.
    # –í —Ä–µ–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ —Å –ë–î –∑–¥–µ—Å—å –±—ã–ª –±—ã –ø–æ–∏—Å–∫ –ø–æ ID.
    context = "\n\n---\n\n".join(DOCUMENT_STORE.values())
    return await synthesize_answer(question, context)

# --- –û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ HTTP –∏ JSON-RPC ---

@app.on_event("startup")
async def on_startup():
    logger.info("üöÄ –°–µ—Ä–≤–µ—Ä –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è...")
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    docs_path = os.path.join(project_root, 'docs')
    if os.path.exists(docs_path):
        for filename in os.listdir(docs_path):
            if filename.endswith(".md"):
                file_path = os.path.join(docs_path, filename)
                with open(file_path, 'r', encoding='utf-8') as f:
                    DOCUMENT_STORE[filename] = f.read()
                    logger.info(f"‚úÖ –î–æ–∫—É–º–µ–Ω—Ç '{filename}' —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω.")
    # 2. –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
    if os.path.exists(KNOWLEDGE_BASE_FILE):
        with open(KNOWLEDGE_BASE_FILE, 'r', encoding='utf-8') as f:
            global KNOWLEDGE_BASE
            KNOWLEDGE_BASE = json.load(f)
            logger.info(f"‚úÖ –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π '{KNOWLEDGE_BASE_FILE}' —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ ({len(KNOWLEDGE_BASE)} –∑–∞–ø–∏—Å–µ–π).")
    # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ–∫–µ–Ω–∞ GigaChat
    if not await gigachat_client.get_token(GIGACHAT_AUTH_TOKEN):
        logger.error("–ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ç–æ–∫–µ–Ω GigaChat –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ. –°–µ—Ä–≤–µ—Ä –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ.")

@app.post("/")
async def json_rpc_handler(request: Request):
    body = await request.json()
    if "jsonrpc" not in body or body["jsonrpc"] != "2.0" or "method" not in body or "id" not in body:
        return JSONResponse(status_code=400, content={"jsonrpc": "2.0", "id": body.get("id"), "error": {"code": -32600, "message": "Invalid Request"}})
    request_id = body["id"]
    method = body["method"]
    params = body.get("params", {})
    try:
        if method == "tools/list": result = {"tools": TOOLS_LIST}
        elif method == "tools/call": result = await handle_tools_call(params)
        else: raise HTTPException(status_code=404, detail=f"Method '{method}' not found")
        return JSONResponse(content={"jsonrpc": "2.0", "id": request_id, "result": result})
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –º–µ—Ç–æ–¥–∞ '{method}': {e}", exc_info=True)
        return JSONResponse(status_code=500, content={"jsonrpc": "2.0", "id": request_id, "error": {"code": -32603, "message": "Internal Error", "data": str(e)}})

async def handle_tools_call(params: dict):
    tool_name = params.get("name")
    arguments = params.get("arguments", {})
    if tool_name == "answer_question":
        query = arguments.get("query")
        if not query: raise ValueError("–î–ª—è 'answer_question' —Ç—Ä–µ–±—É–µ—Ç—Å—è –∞—Ä–≥—É–º–µ–Ω—Ç 'query'")
        if not gigachat_client.access_token:
             raise Exception("–¢–æ–∫–µ–Ω GigaChat –Ω–µ –±—ã–ª –ø–æ–ª—É—á–µ–Ω –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ, –Ω–µ –º–æ–≥—É –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –∑–∞–ø—Ä–æ—Å.")

        query_vector = await gigachat_client.get_embedding(query)
        if not query_vector: raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞.")

        # –ò—â–µ–º –≤ –∫—ç—à–µ
        best_match = None
        highest_similarity = -1.0
        for entry in KNOWLEDGE_BASE:
            similarity = cosine_similarity(query_vector, entry['vector'])
            if similarity > highest_similarity:
                highest_similarity = similarity
                best_match = entry

        final_answer = ""
        if best_match and highest_similarity >= CACHE_SHORTCUT_THRESHOLD:
            if highest_similarity >= CACHE_HIT_THRESHOLD:
                logger.info(f"CACHE HIT: –°—Ö–æ–¥—Å—Ç–≤–æ ({highest_similarity:.2f}) –æ—á–µ–Ω—å –≤—ã—Å–æ–∫–æ–µ —Å –≤–æ–ø—Ä–æ—Å–æ–º '{best_match['question']}'. –û—Ç–¥–∞–µ–º –≥–æ—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç.")
                final_answer = best_match['answer']
            else:
                logger.info(f"SHORTCUT: –°—Ö–æ–¥—Å—Ç–≤–æ ({highest_similarity:.2f}) —Å—Ä–µ–¥–Ω–µ–µ —Å –≤–æ–ø—Ä–æ—Å–æ–º '{best_match['question']}'. –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥–æ—Ç–æ–≤—ã–µ —á–∞–Ω–∫–∏.")
                final_answer = await execute_shortcut_rag(query, best_match['source_chunk_ids'])
        else:
            logger.info("CACHE MISS: –ü–æ—Ö–æ–∂–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –∑–∞–ø—É—Å–∫–∞–µ–º –ø–æ–ª–Ω—ã–π RAG-—Ü–∏–∫–ª.")
            rag_result = await execute_full_rag(query)
            final_answer = rag_result['answer']

            # –ü–†–û–í–ï–†–ö–ê: –ù–µ –∫—ç—à–∏—Ä—É–µ–º –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã
            not_found_message = "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —è –Ω–µ —Å–º–æ–≥ –Ω–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é"
            if not_found_message in final_answer:
                logger.warning("–û—Ç–≤–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω. –†–µ–∑—É–ª—å—Ç–∞—Ç –Ω–µ –±—É–¥–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω –≤ –∫—ç—à.")
            else:
                KNOWLEDGE_BASE.append({
                    "question": query,
                    "vector": query_vector,
                    "answer": final_answer,
                    "source_chunk_ids": rag_result['source_chunk_ids']
                })
                save_knowledge_base()
                logger.info("–ù–æ–≤—ã–π —É—Å–ø–µ—à–Ω—ã–π –æ—Ç–≤–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π.")
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º JSON-—ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å—Ç—Ä–æ–∫—É –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        return {"content": [{"type": "text", "text": json.dumps(final_answer)}], "isError": False}
    else:
        raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ –∏–º—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞: {tool_name}")

if __name__ == "__main__":
    print("üöÄ –ó–∞–ø—É—Å–∫ Advanced RAG MCP Server –Ω–∞ http://localhost:8002")
    uvicorn.run(app, host="0.0.0.0", port=8002)("CACHE MISS: –ü–æ—Ö–æ–∂–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –∑–∞–ø—É—Å–∫–∞–µ–º –ø–æ–ª–Ω—ã–π RAG-—Ü–∏–∫–ª.")
            rag_result = await execute_full_rag(query)
            final_answer = rag_result['answer']

            # –ü–†–û–í–ï–†–ö–ê: –ù–µ –∫—ç—à–∏—Ä—É–µ–º –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã
            not_found_message = "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —è –Ω–µ —Å–º–æ–≥ –Ω–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é"
            if not_found_message in final_answer:
                logger.warning("–û—Ç–≤–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω. –†–µ–∑—É–ª—å—Ç–∞—Ç –Ω–µ –±—É–¥–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω –≤ –∫—ç—à.")
            else:
                KNOWLEDGE_BASE.append({
                    "question": query,
                    "vector": query_vector,
                    "answer": final_answer,
                    "source_chunk_ids": rag_result['source_chunk_ids']
                })
                save_knowledge_base()
                logger.info("–ù–æ–≤—ã–π —É—Å–ø–µ—à–Ω—ã–π –æ—Ç–≤–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π.")
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º JSON-—ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å—Ç—Ä–æ–∫—É –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        return {"content": [{"type": "text", "text": json.dumps(final_answer)}], "isError": False}
    else:
        raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ –∏–º—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞: {tool_name}")

if __name__ == "__main__":
    print("üöÄ –ó–∞–ø—É—Å–∫ Advanced RAG MCP Server –Ω–∞ http://localhost:8002")
    uvicorn.run(app, host="0.0.0.0", port=8002)
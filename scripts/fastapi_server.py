import aiohttp
from fastapi import FastAPI, HTTPException,Request, Body, Header, status
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Dict, Any, List, Optional, Tuple
import requests
import uuid
import json
import asyncio
import logging
from datetime import datetime, timedelta
import re
import hashlib
import os
import time

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –ù–û–í–û–ï: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è workflows –∏ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –Ω–∏–º
WORKFLOWS_FILE = "saved_workflows.json"

def _save_workflows_to_disk():
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç–µ–∫—É—â–∏–µ workflows –≤ JSON —Ñ–∞–π–ª."""
    with open(WORKFLOWS_FILE, "w", encoding="utf-8") as f:
        json.dump(saved_workflows, f, ensure_ascii=False, indent=4)
    logger.info(f"üíæ Workflows —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {WORKFLOWS_FILE}")

def _load_workflows_from_disk():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç workflows –∏–∑ JSON —Ñ–∞–π–ª–∞ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ."""
    if os.path.exists(WORKFLOWS_FILE):
        with open(WORKFLOWS_FILE, "r", encoding="utf-8") as f:
            global saved_workflows
            saved_workflows = json.load(f)
            logger.info(f"‚úÖ Workflows –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ {WORKFLOWS_FILE}")

app = FastAPI(title="N8N Clone API", version="1.0.0")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # –í –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ —É–∫–∞–∑–∞—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–æ–º–µ–Ω—ã
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –•—Ä–∞–Ω–∏–ª–∏—â–µ –¥–ª—è –∞–∫—Ç–∏–≤–Ω—ã—Ö —Ç–∞–π–º–µ—Ä–æ–≤
active_timers = {}
# –î–æ–±–∞–≤—å—Ç–µ –≥–ª–æ–±–∞–ª—å–Ω—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è workflow –ø–æ—Å–ª–µ –¥—Ä—É–≥–∏—Ö –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö (–ø—Ä–∏–º–µ—Ä–Ω–æ —Å—Ç—Ä–æ–∫–∞ 30)
saved_workflows = {}
# –î–æ–±–∞–≤—å—Ç–µ —ç—Ç—É –≥–ª–æ–±–∞–ª—å–Ω—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –Ω–æ–¥
node_execution_results = {}

goto_execution_counter = {} 

# –†–∞—Å—à–∏—Ä—å—Ç–µ –≥–ª–æ–±–∞–ª—å–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ webhook_triggers (–æ–∫–æ–ª–æ —Å—Ç—Ä–æ–∫–∏ 40)
webhook_triggers: Dict[str, Dict[str, Any]] = {}
# –î–æ–±–∞–≤—å—Ç–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤–µ–±—Ö—É–∫–æ–≤
webhook_stats: Dict[str, Dict[str, Any]] = {}

# –ú–æ–¥–µ–ª–∏ –¥–∞–Ω–Ω—ã—Ö
class NodeConfig(BaseModel):
    authToken: Optional[str] = None
    systemMessage: Optional[str] = None
    userMessage: Optional[str] = None
    clearHistory: Optional[bool] = False
    to: Optional[str] = None
    subject: Optional[str] = None
    body: Optional[str] = None
    query: Optional[str] = None
    connection: Optional[str] = None
    url: Optional[str] = None
    method: Optional[str] = None
    headers: Optional[str] = None
    interval: Optional[int] = None
    timezone: Optional[str] = None
    waitForAll: Optional[bool] = True 
    mergeStrategy: Optional[str] = "combine_text"
    separator: Optional[str] = "\n\n---\n\n"
    # –î–ª—è Request Iterator –Ω–æ–¥—ã
    baseUrl: Optional[str] = None
    executionMode: Optional[str] = "sequential" # New: 'sequential' or 'parallel'
    commonHeaders: Optional[str] = None # New: JSON string for common headers
    # –î–ª—è If/Else –Ω–æ–¥—ã
    conditionType: Optional[str] = "equals"
    fieldPath: Optional[str] = "output.text"
    compareValue: Optional[str] = ""
    caseSensitive: Optional[bool] = False
    maxGotoIterations: Optional[int] = 3  # –ó–∞—â–∏—Ç–∞ –æ—Ç –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã—Ö —Ü–∏–∫–ª–æ–≤
    # –ù–û–í–û–ï: –î–ª—è Dispatcher –Ω–æ–¥—ã
    routes: Optional[Dict[str, Any]] = None
    useAI: Optional[bool] = True
    dispatcherAuthToken: Optional[str] = None # –¢–æ–∫–µ–Ω –¥–ª—è GigaChat –≤–Ω—É—Ç—Ä–∏ –¥–∏—Å–ø–µ—Ç—á–µ—Ä–∞
    dispatcher_type: Optional[str] = "router"  # "router" –∏–ª–∏ "orchestrator"
    enable_planning: Optional[bool] = False
    available_workflows: Optional[Dict[str, Any]] = {}
    session_storage: Optional[str] = "memory"

class Node(BaseModel):
    id: str
    type: str
    position: Dict[str, float]
    data: Dict[str, Any]

class Connection(BaseModel):
    id: str
    source: str
    target: str
    data: Optional[Dict[str, Any]] = {}  # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥–¥–µ—Ä–∂–∫—É data –¥–ª—è –º–µ—Ç–æ–∫

class WorkflowExecuteRequest(BaseModel):
    nodes: List[Node]
    connections: List[Connection]
    startNodeId: Optional[str] = None

class ExecutionResult(BaseModel):
    success: bool
    result: Optional[Dict[str, Any]] = None
    error: Optional[str] = None
    logs: List[Dict[str, Any]] = []

# –î–æ–±–∞–≤—å—Ç–µ —ç—Ç—É –º–æ–¥–µ–ª—å –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–æ–¥–µ–ª–µ–π (–ø—Ä–∏–º–µ—Ä–Ω–æ —Å—Ç—Ä–æ–∫–∞ 50-60)
class WorkflowSaveRequest(BaseModel):
    name: str
    nodes: List[Node]
    connections: List[Connection]

# –ù–û–í–û–ï: –ú–æ–¥–µ–ª—å –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è workflow
class WorkflowUpdateRequest(BaseModel):
    nodes: List[Node]
    connections: List[Connection]

# –î–æ–±–∞–≤—å—Ç–µ –Ω–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö (–æ–∫–æ–ª–æ —Å—Ç—Ä–æ–∫–∏ 80)
class WebhookCreateRequest(BaseModel):
    workflow_id: str
    name: str
    description: Optional[str] = None
    auth_required: Optional[bool] = False
    allowed_ips: Optional[List[str]] = None

class WebhookInfo(BaseModel):
    webhook_id: str
    workflow_id: str
    name: str
    description: Optional[str] = None
    created_at: str
    url: str
    auth_required: bool = False
    allowed_ips: Optional[List[str]] = None
    call_count: int = 0
    last_called: Optional[str] = None

class SetupTimerRequest(BaseModel):
    node: Node
    workflow_id: str
# GigaChat API –∫–ª–∞—Å—Å
class GigaChatAPI:
    def __init__(self):
        self.access_token = None
        self.conversation_history = []
        
    async def get_token(self, auth_token: str, scope: str = 'GIGACHAT_API_PERS') -> bool:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞ –¥–æ—Å—Ç—É–ø–∞"""
        rq_uid = str(uuid.uuid4())
        url = "https://ngw.devices.sberbank.ru:9443/api/v2/oauth"

         # --- –ù–û–í–´–ô –ë–õ–û–ö –î–õ–Ø –ù–ê–î–ï–ñ–ù–û–°–¢–ò ---
        # –ù–∞ —Å–ª—É—á–∞–π, –µ—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–ª—É—á–∞–π–Ω–æ —Å–∫–æ–ø–∏—Ä–æ–≤–∞–ª "Basic " –≤–º–µ—Å—Ç–µ —Å —Ç–æ–∫–µ–Ω–æ–º
        if auth_token and auth_token.lower().startswith('basic '):
            logger.warning("‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω –ø—Ä–µ—Ñ–∏–∫—Å 'Basic ' –≤ —Ç–æ–∫–µ–Ω–µ. –£–¥–∞–ª—è—é –µ–≥–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏.")
            auth_token = auth_token[6:]
        # --- –ö–û–ù–ï–¶ –ù–û–í–û–ì–û –ë–õ–û–ö–ê ---

        headers = {
            'Content-Type': 'application/x-www-form-urlencoded',
            'Accept': 'application/json',
            'RqUID': rq_uid,
            'Authorization': f'Basic {auth_token}'
        }
        payload = {'scope': scope}

        try:
            # --- –ù–û–í–û–ï –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –î–õ–Ø –û–¢–õ–ê–î–ö–ò ---
            logger.info(f"üîë –ü–æ–ø—ã—Ç–∫–∞ –ø–æ–ª—É—á–∏—Ç—å —Ç–æ–∫–µ–Ω. URL: {url}")
            logger.info(f"üìã –û—Ç–ø—Ä–∞–≤–ª—è–µ–º—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏: {headers}")
            # --- –ö–û–ù–ï–¶ –ù–û–í–û–ì–û –õ–û–ì–ò–†–û–í–ê–ù–ò–Ø ---
            # –í —Ä–µ–∞–ª—å–Ω–æ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ aiohttp –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            response = requests.post(url, headers=headers, data=payload, verify=False)
            if response.status_code == 200:
                self.access_token = response.json()['access_token']
                logger.info("‚úÖ GigaChat —Ç–æ–∫–µ–Ω –ø–æ–ª—É—á–µ–Ω —É—Å–ø–µ—à–Ω–æ")
                return True
            else:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∞: {response.status_code}")
                # --- –°–ê–ú–û–ï –í–ê–ñ–ù–û–ï: –õ–û–ì–ò–†–£–ï–ú –¢–ï–õ–û –û–¢–í–ï–¢–ê ---
                try:
                    error_details = response.json()
                    logger.error(f"üîç –î–µ—Ç–∞–ª–∏ –æ—à–∏–±–∫–∏ –æ—Ç GigaChat: {error_details}")
                except json.JSONDecodeError:
                    logger.error(f"üîç –û—Ç–≤–µ—Ç –æ—Ç GigaChat (–Ω–µ JSON): {response.text}")
                # --- –ö–û–ù–ï–¶ –í–ê–ñ–ù–û–ì–û –ë–õ–û–ö–ê ---
                return False
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Ç–æ–∫–µ–Ω–∞: {str(e)}")
            return False

    async def get_chat_completion(self, system_message: str, user_message: str) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –æ—Ç GigaChat"""
        if not self.access_token:
            raise Exception("–¢–æ–∫–µ–Ω –¥–æ—Å—Ç—É–ø–∞ –Ω–µ –ø–æ–ª—É—á–µ–Ω")

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è
        messages = [{"role": "system", "content": system_message}]
        messages.extend(self.conversation_history)
        messages.append({"role": "user", "content": user_message})

        url = "https://gigachat.devices.sberbank.ru/api/v1/chat/completions"
        payload = {
            "model": "GigaChat-Pro",
            "messages": messages,
            "temperature": 1,
            "top_p": 0.1,
            "n": 1,
            "stream": False,
            "max_tokens": 512,
            "repetition_penalty": 1,
            "update_interval": 0
        }
        headers = {
            'Content-Type': 'application/json',
            'Accept': 'application/json',
            'Authorization': f'Bearer {self.access_token}'
        }

        try:
            response = requests.post(url, headers=headers, data=json.dumps(payload), verify=False)
            if response.status_code == 200:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
                self.conversation_history.append({"role": "user", "content": user_message})
                assistant_response = response.json()['choices'][0]['message']['content']
                self.conversation_history.append({"role": "assistant", "content": assistant_response})
                
                logger.info(f"‚úÖ –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –æ—Ç GigaChat")
                logger.info(f"ü§ñ –û–¢–í–ï–¢: {assistant_response}")
                return {
                    "success": True,
                    "response": assistant_response,
                    "user_message": user_message,
                    "system_message": system_message,
                    "conversation_length": len(self.conversation_history)
                }
            else:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ API GigaChat: {response.status_code}")
                return {
                    "success": False,
                    "error": f"API Error: {response.status_code}",
                    "response": None
                }
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ GigaChat: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "response": None
            }

    def clear_history(self):
        """–û—á–∏—Å—Ç–∫–∞ –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞"""
        self.conversation_history = []
        logger.info("üóëÔ∏è –ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞ –æ—á–∏—â–µ–Ω–∞")

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏ –∑–∞–ø—É—Å–∫–∞ —Ç–∞–π–º–µ—Ä–∞
async def create_timer(timer_id: str, node_id: str, interval: int, workflow_info: Dict[str, Any]):
    """–°–æ–∑–¥–∞–µ—Ç –∏ –∑–∞–ø—É—Å–∫–∞–µ—Ç –Ω–æ–≤—ã–π —Ç–∞–π–º–µ—Ä (–ª–æ–≥–∏–∫–∞ –ø–æ—á—Ç–∏ –Ω–µ –∏–∑–º–µ–Ω–∏–ª–∞—Å—å)"""
    if timer_id in active_timers and active_timers[timer_id]["task"] is not None:
        active_timers[timer_id]["task"].cancel()
        logger.info(f"üõë –û—Ç–º–µ–Ω–µ–Ω —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ç–∞–π–º–µ—Ä {timer_id} –¥–ª—è –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏—è")

    task = asyncio.create_task(timer_task(timer_id, node_id, interval, workflow_info))

    active_timers[timer_id] = {
        "node_id": node_id,
        "interval": interval,
        "next_execution": datetime.now() + timedelta(minutes=interval),
        "task": task,
        "status": "active",
        "workflow": workflow_info  # –°–æ—Ö—Ä–∞–Ω—è–µ–º workflow_id –∏ start_node_id
    }
    logger.info(f"üïí –°–æ–∑–¥–∞–Ω/–ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω —Ç–∞–π–º–µ—Ä {timer_id} –¥–ª—è workflow '{workflow_info.get('workflow_id')}'")
    return active_timers[timer_id]

async def update_timer(timer_id: str, interval: int, workflow_info: Dict[str, Any]):
    """
    –û–±–Ω–æ–≤–ª—è–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ç–∞–π–º–µ—Ä. –¢–µ–ø–µ—Ä—å –ø—Ä–∏–Ω–∏–º–∞–µ—Ç workflow_info.
    """
    if timer_id not in active_timers:
        raise Exception(f"Timer {timer_id} not found")

    timer_info = active_timers[timer_id]
    
    # –ü—Ä–æ—Å—Ç–æ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º —Ç–∞–π–º–µ—Ä —Å –Ω–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
    await create_timer(
        timer_id,
        timer_info["node_id"],
        interval,
        workflow_info # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
    )
    logger.info(f"üîÑ –û–±–Ω–æ–≤–ª–µ–Ω —Ç–∞–π–º–µ—Ä {timer_id} —Å –Ω–æ–≤—ã–º –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–º {interval} –º–∏–Ω—É—Ç")
    return active_timers[timer_id]

async def timer_task(timer_id: str, node_id: str, interval: int, workflow_info: Dict[str, Any]):
    """
    –ó–∞–¥–∞—á–∞ —Ç–∞–π–º–µ—Ä–∞, –∫–æ—Ç–æ—Ä–∞—è –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ.
    –¢–µ–ø–µ—Ä—å –æ–Ω–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å workflow_id, –∞ –Ω–µ —Å –µ–≥–æ –∫–æ–ø–∏–µ–π.
    """
    try:
        while True:
            logger.info(f"‚è∞ –ó–∞–ø—É—â–µ–Ω–∞ –∑–∞–¥–∞—á–∞ —Ç–∞–π–º–µ—Ä–∞ {timer_id} –¥–ª—è –Ω–æ–¥—ã {node_id} —Å –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–º {interval} –º–∏–Ω—É—Ç")
            logger.info(f"‚è∞ –¢–∞–π–º–µ—Ä {timer_id} –æ–∂–∏–¥–∞–µ—Ç {interval} –º–∏–Ω—É—Ç –¥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ –∑–∞–ø—É—Å–∫–∞")
            await asyncio.sleep(interval * 60) # –ü–µ—Ä–µ–≤–æ–¥–∏–º –º–∏–Ω—É—Ç—ã –≤ —Å–µ–∫—É–Ω–¥—ã

            active_timers[timer_id]["next_execution"] = datetime.now() + timedelta(minutes=interval)
            # üî¥ –î–û–ë–ê–í–ò–¢–¨: –û—Ç–º–µ—á–∞–µ–º –Ω–∞—á–∞–ª–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è workflow
            active_timers[timer_id]["is_executing_workflow"] = True

            try:
                # 1. –ü–æ–ª—É—á–∞–µ–º ID workflow –∏–∑ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –ø—Ä–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ
                workflow_id = workflow_info.get("workflow_id")
                if not workflow_id or workflow_id not in saved_workflows:
                    logger.error(f"‚ùå Workflow —Å ID '{workflow_id}' –Ω–µ –Ω–∞–π–¥–µ–Ω. –¢–∞–π–º–µ—Ä {timer_id} –Ω–µ –º–æ–∂–µ—Ç –∑–∞–ø—É—Å—Ç–∏—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ.")
                    continue # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç–æ—Ç —Ü–∏–∫–ª

                logger.info(f"üöÄ –¢–∞–π–º–µ—Ä {timer_id} –∑–∞–ø—É—Å–∫–∞–µ—Ç workflow '{workflow_id}'")

                # 2. –ë–µ—Ä–µ–º —Å–∞–º—É—é —Å–≤–µ–∂—É—é –≤–µ—Ä—Å–∏—é workflow –∏–∑ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
                workflow_data = saved_workflows[workflow_id]

                # 3. –°–æ–∑–¥–∞–µ–º –∑–∞–ø—Ä–æ—Å –Ω–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
                workflow_request = WorkflowExecuteRequest(
                    nodes=workflow_data["nodes"],
                    connections=workflow_data["connections"],
                    startNodeId=node_id
                )

                # 4. –í—ã–ø–æ–ª–Ω—è–µ–º workflow
                start_time = datetime.now()
                result = await execute_workflow_internal(workflow_request)
                execution_time = (datetime.now() - start_time).total_seconds()

                if result.success:
                    logger.info(f"‚úÖ Workflow '{workflow_id}' –≤—ã–ø–æ–ª–Ω–µ–Ω –∑–∞ {execution_time:.2f} —Å–µ–∫—É–Ω–¥")
                else:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è workflow '{workflow_id}' —Ç–∞–π–º–µ—Ä–æ–º {timer_id}: {result.error}")

            except Exception as e:
                logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ workflow —Ç–∞–π–º–µ—Ä–æ–º {timer_id}: {str(e)}")
                import traceback
                logger.error(f"üîç –¢—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞: {traceback.format_exc()}")
            finally:
                if timer_id in active_timers:
                    active_timers[timer_id]["is_executing_workflow"] = False
                    logger.info(f"üîì –§–ª–∞–≥ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Å–Ω—è—Ç –¥–ª—è —Ç–∞–π–º–µ—Ä–∞ {timer_id}")

    except asyncio.CancelledError:
        logger.info(f"üõë –¢–∞–π–º–µ—Ä {timer_id} –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ –∑–∞–¥–∞—á–µ —Ç–∞–π–º–µ—Ä–∞ {timer_id}: {str(e)}")
        if timer_id in active_timers:
            active_timers[timer_id]["status"] = "error"



# –ó–∞–º–µ–Ω–∏ —Å–≤–æ—é —Å—Ç–∞—Ä—É—é —Ñ—É–Ω–∫—Ü–∏—é replace_templates –Ω–∞ —ç—Ç—É
def replace_templates(text: str, data: Dict[str, Any], label_to_id_map: Dict[str, str]) -> str:
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –∑–∞–º–µ–Ω–∞ —à–∞–±–ª–æ–Ω–æ–≤ –≤–∏–¥–∞ {{Node Label.path.to.value}} –∏–ª–∏ {{node-id.path.to.value}}"""
    
    # –ò–ú–ü–û–†–¢–ò–†–£–ï–ú –ú–û–î–£–õ–¨ –î–õ–Ø –†–ï–ì–£–õ–Ø–†–ù–´–• –í–´–†–ê–ñ–ï–ù–ò–ô
    import re

    def get_nested_value(obj: Dict[str, Any], path: str) -> Any:
        """–ü–æ–ª—É—á–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ –ø—É—Ç–∏ —Ç–∏–ø–∞ 'node-id.output.text' –∏–ª–∏ 'node-id.json.result[0].text'"""
        
        # –ù–æ–≤—ã–π, –±–æ–ª–µ–µ —É–º–Ω—ã–π —Ä–∞–∑–±–æ—Ä—â–∏–∫ –ø—É—Ç–∏.
        # –û–Ω —Ä–∞–∑–¥–µ–ª—è–µ—Ç –ø–æ —Ç–æ—á–∫–∞–º –∏ –ø–æ –∫–≤–∞–¥—Ä–∞—Ç–Ω—ã–º —Å–∫–æ–±–∫–∞–º, –æ—Ç–±—Ä–∞—Å—ã–≤–∞—è –ø—É—Å—Ç—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã.
        # –ù–∞–ø—Ä–∏–º–µ—Ä, 'a.b[0]' –ø—Ä–µ–≤—Ä–∞—Ç–∏—Ç—Å—è –≤ ['a', 'b', '0']
        keys = [key for key in re.split(r'[.\[\]]', path) if key]
        
        current = obj
        for key in keys:
            if isinstance(current, dict) and key in current:
                current = current[key]
            # –¢–µ–ø–µ—Ä—å –æ–Ω –ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–µ—Ç –∏–Ω–¥–µ–∫—Å –º–∞—Å—Å–∏–≤–∞
            elif isinstance(current, list) and key.isdigit():
                index = int(key)
                if 0 <= index < len(current):
                    current = current[index]
                else:
                    # –ò–Ω–¥–µ–∫—Å –∑–∞ –ø—Ä–µ–¥–µ–ª–∞–º–∏ –º–∞—Å—Å–∏–≤–∞
                    return None
            else:
                # –ö–ª—é—á –∏–ª–∏ –∏–Ω–¥–µ–∫—Å –Ω–µ –Ω–∞–π–¥–µ–Ω
                return None
        return current

    # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ {{ ... }} —Å –ø—Ä–æ–±–µ–ª–∞–º–∏
    pattern = r"\{\{\s*(.+?)\s*\}\}"

    def replacer(match):
        path = match.group(1).strip()

        # –£–±–∏—Ä–∞–µ–º 'input.', –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å (–¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
        if path.startswith('input.'):
            path = path[6:]

        # –†–∞–∑–¥–µ–ª—è–µ–º –ø—É—Ç—å –Ω–∞ –ø–µ—Ä–≤—É—é —á–∞—Å—Ç—å (–ª–µ–π–±–ª –∏–ª–∏ ID) –∏ –æ—Å—Ç–∞–ª—å–Ω–æ–µ
        parts = path.split('.', 1)
        node_identifier = parts[0]
        remaining_path = parts[1] if len(parts) > 1 else ''

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º ID –Ω–æ–¥—ã
        node_id = None
        # 1. –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º –ø–æ –ª–µ–π–±–ª—É
        if node_identifier in label_to_id_map:
            node_id = label_to_id_map[node_identifier]
        # 2. –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –∏—â–µ–º –ø–æ ID (–æ–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å)
        elif node_identifier in data:
            node_id = node_identifier
        else:
            logger.warning(f"‚ö†Ô∏è –®–∞–±–ª–æ–Ω: –Ω–æ–¥–∞ —Å –ª–µ–π–±–ª–æ–º –∏–ª–∏ ID '{node_identifier}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
            return f"{{{{ERROR: Node '{node_identifier}' not found}}}}"

        # –°–æ–±–∏—Ä–∞–µ–º –ø–æ–ª–Ω—ã–π –ø—É—Ç—å –¥–ª—è –ø–æ–∏—Å–∫–∞: 'node-id.remaining.path'
        full_path = f"{node_id}.{remaining_path}" if remaining_path else node_id

        value = get_nested_value(data, full_path)

        # –£–õ–£–ß–®–ï–ù–ò–ï: –ë–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö
        if value is None:
            logger.warning(f"‚ö†Ô∏è –®–∞–±–ª–æ–Ω: –ø—É—Ç—å '{path}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –¥–∞–Ω–Ω—ã—Ö –Ω–æ–¥—ã '{node_identifier}'. –ó–∞–º–µ–Ω–∞ –Ω–∞ –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É.")
            return ""
        
        # –ï—Å–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç - —Å–ª–æ–≤–∞—Ä—å –∏–ª–∏ —Å–ø–∏—Å–æ–∫, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –µ–≥–æ –∫–∞–∫ JSON
        if isinstance(value, (dict, list)):
            final_str = json.dumps(value, ensure_ascii=False)
        else:
            # –î–ª—è –≤—Å–µ–≥–æ –æ—Å—Ç–∞–ª—å–Ω–æ–≥–æ (—Å—Ç—Ä–æ–∫–∏, —á–∏—Å–ª–∞, –±—É–ª–µ–≤—ã) –º—ã –¥–µ–ª–∞–µ–º —Ç—Ä—é–∫:
            # 1. –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –≤ —Å—Ç—Ä–æ–∫—É (–Ω–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ —ç—Ç–æ —á–∏—Å–ª–æ).
            # 2. –ö–æ–¥–∏—Ä—É–µ–º —ç—Ç—É —Å—Ç—Ä–æ–∫—É –≤ JSON. –≠—Ç–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —ç–∫—Ä–∞–Ω–∏—Ä—É–µ—Ç –≤—Å–µ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã: \n, ", \ –∏ —Ç.–¥.
            #    'a\nb' —Å—Ç–∞–Ω–µ—Ç '"a\\nb"'
            # 3. –£–±–∏—Ä–∞–µ–º –∫—Ä–∞–π–Ω–∏–µ –∫–∞–≤—ã—á–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –¥–æ–±–∞–≤–ª—è–µ—Ç json.dumps.
            #    '"a\\nb"' —Å—Ç–∞–Ω–µ—Ç 'a\\nb'
            # –≠—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ –≤—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –±—É–¥–µ—Ç –±–µ–∑–æ–ø–∞—Å–Ω—ã–º –¥–ª—è JSON.
            final_str = json.dumps(str(value), ensure_ascii=False)
            if final_str.startswith('"') and final_str.endswith('"'):
                final_str = final_str[1:-1]

        logger.info(f"üîÑ –ó–∞–º–µ–Ω–∞ —à–∞–±–ª–æ–Ω–∞: {{{{{match.group(1)}}}}} -> {final_str[:200]}...")
        return final_str

    return re.sub(pattern, replacer, text)

# Add this helper function within the NodeExecutors class or globally if preferred
# For simplicity, let's add it before the NodeExecutors class or as a static method

async def _make_single_http_request(
    session: aiohttp.ClientSession,
    method: str,
    url: str,
    params: Optional[Dict[str, Any]] = None,
    json_body: Optional[Dict[str, Any]] = None,
    headers: Optional[Dict[str, str]] = None
) -> Dict[str, Any]:
    """
    Makes a single HTTP request and returns a structured response or mock on error.
    """
    request_details = {
        "request_url": url,
        "request_method": method,
        "request_params": params if method == "GET" else None,
        "request_body": json_body if method in ["POST", "PUT", "PATCH"] else None,
        "request_headers": headers,
    }
    try:
        logger.info(f"üåç Making {method} request to {url} with params={params}, body={json_body}, headers={headers}")
        async with session.request(
            method,
            url,
            params=params if method == "GET" else None, # Query params for GET
            json=json_body if method in ["POST", "PUT", "PATCH"] else None, # JSON body for POST/PUT etc.
            headers=headers,
            timeout=aiohttp.ClientTimeout(total=10), # 10-second timeout
            ssl=False
        ) as response:
            response_data = None
            try:
                # Try to parse as JSON, but handle cases where it might not be
                if response.content_type == 'application/json':
                    response_data = await response.json()
                else:
                    response_data = await response.text()
            except (aiohttp.ContentTypeError, json.JSONDecodeError) as json_err:
                logger.warning(f"‚ö†Ô∏è Could not parse JSON response from {url}: {json_err}. Reading as text.")
                response_data = await response.text() # Fallback to text
            except Exception as e:
                logger.error(f"üö® Error reading response content from {url}: {e}")
                response_data = f"Error reading response: {e}"


            logger.info(f"‚úÖ Response from {url}: {response.status}")
            return {
                **request_details,
                "status_code": response.status,
                "response_headers": dict(response.headers),
                "response_data": response_data,
                "success": 200 <= response.status < 300,
            }
    except aiohttp.ClientConnectorError as e:
        logger.error(f"‚ùå Connection error for {url}: {e}")
        return {
            **request_details,
            "status_code": 503, # Service Unavailable
            "response_data": {"error": "Connection Error", "details": str(e)},
            "success": False,
            "mock_reason": "Connection Error",
        }
    except asyncio.TimeoutError:
        logger.error(f"‚è∞ Timeout error for {url}")
        return {
            **request_details,
            "status_code": 504, # Gateway Timeout
            "response_data": {"error": "Timeout Error", "details": "Request timed out after 10 seconds"},
            "success": False,
            "mock_reason": "Timeout Error",
        }
    except Exception as e:
        logger.error(f"üí• Unexpected error for {url}: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return {
            **request_details,
            "status_code": 500, # Internal Server Error
            "response_data": {"error": "Unexpected Error", "details": str(e)},
            "success": False,
            "mock_reason": "Unexpected Error",
        }
    
# –ò—Å–ø–æ–ª–Ω–∏—Ç–µ–ª–∏ –Ω–æ–¥
class NodeExecutors:
    def __init__(self):
        self.gigachat_api = GigaChatAPI()

        # –î–û–ë–ê–í–ò–¢–¨ —ç—Ç–∏ —Å—Ç—Ä–æ–∫–∏:
        self.dispatcher_sessions = {}  # –•—Ä–∞–Ω–∏–ª–∏—â–µ —Å–µ—Å—Å–∏–π –¥–ª—è –≤—Å–µ—Ö –¥–∏—Å–ø–µ—Ç—á–µ—Ä–æ–≤
        logger.info("üèóÔ∏è NodeExecutors –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Å–µ—Å—Å–∏–π –¥–∏—Å–ø–µ—Ç—á–µ—Ä–æ–≤")
    
    
    async def execute_request_iterator(self, node: Node, label_to_id_map: Dict[str, str],input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ Request Iterator –Ω–æ–¥—ã –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å "–ü—Ä–∏–Ω—Ü–∏–ø–æ–º –ï–¥–∏–Ω–æ–≥–æ –†–µ–∑—É–ª—å—Ç–∞—Ç–∞".
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —è–≤–Ω—ã–π —à–∞–±–ª–æ–Ω –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
        """
        start_time = datetime.now()
        logger.info(f"Executing Request Iterator node: {node.id}")
        config = node.data.get('config', {})

        # --- 1. –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ —è–≤–Ω—ã–π —à–∞–±–ª–æ–Ω ---
        # –í UI –¥–ª—è —ç—Ç–æ–π –Ω–æ–¥—ã –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –¥–æ–±–∞–≤–∏—Ç—å –ø–æ–ª–µ "JSON Input"
        json_input_template = config.get('jsonInput', '')
        if not json_input_template:
            raise Exception("Request Iterator: 'jsonInput' template is not configured in the node settings.")

        logger.info(f"üìÑ Input template for Request Iterator: {json_input_template}")
        requests_to_make_json_str = replace_templates(json_input_template, input_data,label_to_id_map)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ —à–∞–±–ª–æ–Ω —Å—Ä–∞–±–æ—Ç–∞–ª –∏ –≤–µ—Ä–Ω—É–ª –Ω–µ–ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É
        if not requests_to_make_json_str or requests_to_make_json_str == json_input_template:
            logger.warning(f"Template '{json_input_template}' could not be resolved or resulted in an empty string. Assuming empty list of requests.")
            requests_to_make_json_str = "[]"

        # --- 2. –ü–∞—Ä—Å–∏–Ω–≥ JSON (–æ—Å—Ç–∞–µ—Ç—Å—è –∫–∞–∫ –±—ã–ª–æ, –Ω–æ —Å –±–æ–ª–µ–µ —á–∏—Å—Ç—ã–º –≤—Ö–æ–¥–æ–º) ---
        try:
            requests_list = json.loads(requests_to_make_json_str)
            if not isinstance(requests_list, list):
                if isinstance(requests_list, dict):
                    requests_list = [requests_list]
                else:
                    raise ValueError("Parsed JSON is not a list or a single request object.")
        except (json.JSONDecodeError, ValueError) as e:
            raise Exception(f"Request Iterator: Invalid JSON input after template replacement. Error: {str(e)}")

        if not requests_list:
            logger.info("Request Iterator: No requests to process from input.")
            # –î–∞–∂–µ –µ—Å–ª–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–µ—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            node_result = {
                "text": "[]",
                "json": [],
                "meta": { "executed_requests_count": 0, "successful_requests_count": 0, "failed_requests_count": 0 },
                "inputs": { "jsonInput_template": json_input_template }
            }
            return node_result

        # --- 3. –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ (–æ—Å—Ç–∞–µ—Ç—Å—è –ø–æ—á—Ç–∏ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π) ---
        base_url = config.get('baseUrl', '').rstrip('/')
        execution_mode = config.get('executionMode', 'sequential')
        common_headers_str = config.get('commonHeaders', '{}')
        try:
            parsed_common_headers = json.loads(common_headers_str) if common_headers_str else {}
        except json.JSONDecodeError:
            parsed_common_headers = {}

        all_responses = []
        tasks = []
        async with aiohttp.ClientSession() as session:
            # --- –ù–ê–ß–ê–õ–û –í–°–¢–ê–í–õ–ï–ù–ù–û–ì–û –ë–õ–û–ö–ê ---
            for req_info in requests_list:
                if not isinstance(req_info, dict):
                    logger.warning(f"Skipping invalid request item (not a dict): {req_info}")
                    all_responses.append({
                        "error": "Invalid request item format",
                        "item_data": req_info,
                        "success": False
                    })
                    continue

                endpoint = req_info.get('endpoint', '')
                if not endpoint:
                    logger.warning(f"Request Iterator: Skipping request with no endpoint: {req_info}")
                    all_responses.append({
                        "error": "Missing endpoint",
                        "item_data": req_info,
                        "success": False
                    })
                    continue
                
                if base_url and not endpoint.startswith('/') and not endpoint.lower().startswith(('http://', 'https://')):
                    final_url = f"{base_url}/{endpoint.lstrip('/')}"
                elif not base_url and not endpoint.lower().startswith(('http://', 'https://')):
                    logger.warning(f"Request Iterator: Endpoint '{endpoint}' is relative but no baseUrl is configured. Skipping.")
                    all_responses.append({
                        "error": "Relative endpoint with no baseUrl",
                        "item_data": req_info,
                        "success": False
                    })
                    continue
                elif endpoint.lower().startswith(('http://', 'https://')):
                    final_url = endpoint
                else:
                    final_url = f"{base_url}{endpoint}"

                method = req_info.get('method', 'GET').upper()
                get_params = req_info.get('params') if method == 'GET' else None
                json_body = req_info.get('body') if method in ['POST', 'PUT', 'PATCH'] else None
                specific_headers = req_info.get('headers', {})
                final_headers = {**parsed_common_headers, **specific_headers}

                task = _make_single_http_request(
                    session,
                    method,
                    final_url,
                    params=get_params,
                    json_body=json_body,
                    headers=final_headers
                )
                tasks.append(task)
            # --- –ö–û–ù–ï–¶ –í–°–¢–ê–í–õ–ï–ù–ù–û–ì–û –ë–õ–û–ö–ê ---

            if execution_mode == 'parallel' and tasks:
                all_responses = await asyncio.gather(*tasks, return_exceptions=True)
            elif tasks:
                for task_coro in tasks:
                    all_responses.append(await task_coro)

        final_responses_list = [r for r in all_responses if not isinstance(r, Exception)]
        logger.info(f"Request Iterator: Processed {len(final_responses_list)} requests.")

        # --- 4. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ ---
        execution_time_ms = int((datetime.now() - start_time).total_seconds() * 1000)
        successful_count = sum(1 for r in final_responses_list if r.get('success'))
        failed_count = len(final_responses_list) - successful_count

        node_result = {
            "text": json.dumps(final_responses_list, ensure_ascii=False, indent=2),
            "json": final_responses_list,
            "meta": {
                "node_type": node.type,
                "timestamp": datetime.now().isoformat(),
                "execution_time_ms": execution_time_ms,
                "executed_requests_count": len(final_responses_list),
                "successful_requests_count": successful_count,
                "failed_requests_count": failed_count,
            },
            "inputs": {
                "baseUrl": base_url,
                "executionMode": execution_mode,
                "jsonInput_template": json_input_template,
            }
        }

        # --- 5. –§–∏–Ω–∞–ª—å–Ω—ã–π return –ø–æ "–ó–æ–ª–æ—Ç–æ–º—É –ü—Ä–∞–≤–∏–ª—É" ---
        return node_result

    async def execute_gigachat(self, node: Node, label_to_id_map: Dict[str, str],input_data: Dict[str, Any]) -> Dict[str, Any]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ GigaChat –Ω–æ–¥—ã"""
        start_time = datetime.now()
        logger.info(f"Executing GigaChat node: {node.id}")
        config = node.data.get('config', {})
        auth_token = config.get('authToken')
        # system_message_template = config.get('systemMessage', '–¢—ã –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç')
        # user_message_template = config.get('userMessage', '')
        clear_history = config.get('clearHistory', False)

        # 1. –í—Å–µ–≥–¥–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–∑ —à–∞–±–ª–æ–Ω–æ–≤ –≤ –∫–æ–Ω—Ñ–∏–≥–µ.
        #    –¢–µ–ø–µ—Ä—å –æ–Ω–∏ –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ —Å—É—â–µ—Å—Ç–≤—É—é—Ç.
        system_message = config.get('systemMessage', '–¢—ã –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç')
        user_message = config.get('userMessage', '')

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        original_system_message = system_message
        original_user_message = user_message
        # –î–û–ë–ê–í–õ–Ø–ï–ú –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –í–•–û–î–ù–´–• –î–ê–ù–ù–´–•
        # –£–ù–ò–í–ï–†–°–ê–õ–¨–ù–ê–Ø –ó–ê–ú–ï–ù–ê –®–ê–ë–õ–û–ù–û–í
        if input_data:
            logger.info(f"üì• –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–π –Ω–æ–¥—ã: {json.dumps(input_data, ensure_ascii=False, indent=2)[:500]}...")
            
            user_message = replace_templates(original_user_message, input_data,label_to_id_map)
            # –¢–∞–∫–∂–µ –∑–∞–º–µ–Ω—è–µ–º —à–∞–±–ª–æ–Ω—ã –≤ system_message –µ—Å–ª–∏ –µ—Å—Ç—å
            system_message = replace_templates(original_system_message, input_data,label_to_id_map)
            
            if original_user_message != user_message:
                logger.info(f"üìù –°–æ–æ–±—â–µ–Ω–∏–µ –¥–æ –∑–∞–º–µ–Ω—ã: {original_user_message}")
                logger.info(f"üìù –°–æ–æ–±—â–µ–Ω–∏–µ –ø–æ—Å–ª–µ –∑–∞–º–µ–Ω—ã: {user_message}")

        if not auth_token or not user_message:
            raise Exception("GigaChat: Auth token and user message are required")

        logger.info(f"ü§ñ –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ GigaChat –Ω–æ–¥—ã: {node.id}")
        logger.info(f"üìù –í–æ–ø—Ä–æ—Å: {user_message}")
        # –û—á–∏—â–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if clear_history:
            self.gigachat_api.clear_history()

        # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–∫–µ–Ω
        if not await self.gigachat_api.get_token(auth_token):
            raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ç–æ–∫–µ–Ω –¥–æ—Å—Ç—É–ø–∞")

        # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å
        result = await self.gigachat_api.get_chat_completion(system_message, user_message)
        
        if not result.get('success'):
            raise Exception(result.get('error', 'Unknown error'))

        # --- –ù–û–í–´–ô –ë–õ–û–ö –û–ß–ò–°–¢–ö–ò ---
        import re
        raw_response_text = result.get('response', '')
        cleaned_response_text = raw_response_text
        match = re.search(r'```(json)?\s*([\s\S]*?)\s*```', raw_response_text)
        if match:
            logger.info("üßπ GigaChat –≤–µ—Ä–Ω—É–ª Markdown, –∏–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å—Ç—ã–π JSON.")
            cleaned_response_text = match.group(2)
        # --- –ö–û–ù–ï–¶ –ë–õ–û–ö–ê –û–ß–ò–°–¢–ö–ò ---
        # --- –ö–õ–Æ–ß–ï–í–û–ï –ò–ó–ú–ï–ù–ï–ù–ò–ï ---
        # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –æ—Ç–≤–µ—Ç –∫–∞–∫ JSON, –Ω–æ –Ω–µ –ª–æ–º–∞–µ–º—Å—è, –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ –æ–Ω.
        parsed_json = None
        try:
            # –ü–æ–ø—ã—Ç–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞
            parsed_json = json.loads(cleaned_response_text)
            logger.info("‚úÖ –û—Ç–≤–µ—Ç –æ—Ç GigaChat —É—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω –∫–∞–∫ JSON.")
        except json.JSONDecodeError:
            # –ï—Å–ª–∏ —ç—Ç–æ –Ω–µ JSON, –Ω–∏—á–µ–≥–æ —Å—Ç—Ä–∞—à–Ω–æ–≥–æ. –ü—Ä–æ—Å—Ç–æ –ª–æ–≥–∏—Ä—É–µ–º —ç—Ç–æ.
            logger.info("‚ÑπÔ∏è –û—Ç–≤–µ—Ç –æ—Ç GigaChat –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –≤–∞–ª–∏–¥–Ω—ã–º JSON. –ë—É–¥–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∫–∞–∫ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç.")
            pass # parsed_json –æ—Å—Ç–∞–Ω–µ—Ç—Å—è None

        # –§–æ—Ä–º–∏—Ä—É–µ–º –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–ª–µ–¥—É—é—â–∏—Ö –Ω–æ–¥
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π, —É–Ω–∏–∫–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç —ç—Ç–æ–π –Ω–æ–¥—ã
        # --- 4. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ ---
        execution_time_ms = int((datetime.now() - start_time).total_seconds() * 1000)
        
        node_result = {
            "text": cleaned_response_text,
            "json": parsed_json,
            "meta": {
                "node_type": node.type,
                "timestamp": datetime.now().isoformat(),
                "execution_time_ms": execution_time_ms,
                "conversation_length": result.get("conversation_length", 0),
                "length": len(raw_response_text),
                "words": len(raw_response_text.split()),
                "id_node": node.id
            },
            "inputs": {
                "system_message_template": original_system_message,
                "user_message_template": original_user_message,
                "final_system_message": system_message,
                "final_user_message": user_message,
                "clear_history": clear_history
            }
        }
        return node_result


    async def execute_email(self, node: Node, label_to_id_map: Dict[str, str],input_data: Dict[str, Any]) -> Dict[str, Any]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ Email –Ω–æ–¥—ã"""
        config = node.data.get('config', {})
        to = config.get('to', '')
        subject = config.get('subject', '')
        body = config.get('body', '')

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –Ω–æ–¥ –µ—Å–ª–∏ –ø–æ–ª—è –ø—É—Å—Ç—ã–µ
        if not body and input_data and 'output' in input_data:
            body = input_data['output'].get('text', 'No content from previous node')
        
        if not subject and input_data and 'output' in input_data:
            subject = f"Response: {input_data['output'].get('question', 'Workflow Result')}"

        logger.info(f"üìß –û—Ç–ø—Ä–∞–≤–∫–∞ email –Ω–∞ {to}")
        logger.info(f"üìã –¢–µ–º–∞: {subject}")
        logger.info(f"üìÑ –¢–µ–ª–æ: {body[:100]}...")

        # –ó–¥–µ—Å—å –±—É–¥–µ—Ç —Ä–µ–∞–ª—å–Ω–∞—è –æ—Ç–ø—Ä–∞–≤–∫–∞ email (–Ω–∞–ø—Ä–∏–º–µ—Ä, —á–µ—Ä–µ–∑ SendGrid, SMTP)
        # –ü–æ–∫–∞ —Å–∏–º—É–ª–∏—Ä—É–µ–º
        await asyncio.sleep(1)

        email_result = {
            "sent": True,
            "to": to,
            "subject": subject,
            "messageId": f"msg_{int(datetime.now().timestamp())}"
        }
        return email_result

    async def execute_database(self, node: Node, label_to_id_map: Dict[str, str], input_data: Dict[str, Any]) -> Dict[str, Any]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ Database –Ω–æ–¥—ã"""
        config = node.data.get('config', {})
        query = config.get('query', '')
        connection = config.get('connection', 'postgres')

        # –ê–≤—Ç–æ–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–∞ –µ—Å–ª–∏ –ø—É—Å—Ç–æ–π
        if not query and input_data and 'output' in input_data:
            text_data = input_data['output'].get('text', 'No data')[:100]
            query = f"INSERT INTO responses (text, timestamp) VALUES ('{text_data}', NOW())"

        logger.info(f"üóÑÔ∏è –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ SQL –∑–∞–ø—Ä–æ—Å–∞")
        logger.info(f"üîó –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ: {connection}")
        logger.info(f"üìù –ó–∞–ø—Ä–æ—Å: {query}")

        # –ó–¥–µ—Å—å –±—É–¥–µ—Ç —Ä–µ–∞–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ SQL –∑–∞–ø—Ä–æ—Å–∞
        # –ü–æ–∫–∞ —Å–∏–º—É–ª–∏—Ä—É–µ–º
        await asyncio.sleep(1)

        db_result = {
            "success": True,
            "rows": [
                {
                    "id": 1,
                    "text": "Sample Data",
                    "created_at": datetime.now().isoformat()
                }
            ],
            "rowCount": 1,
            "query": query,
            "connection": connection
        }

        return db_result

    async def execute_webhook(self, node: Node, label_to_id_map: Dict[str, str],input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ Webhook –Ω–æ–¥—ã —Å –Ω–æ–≤–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π –≤—ã–≤–æ–¥–∞ –∏ —è–≤–Ω—ã–º —à–∞–±–ª–æ–Ω–æ–º –¥–ª—è —Ç–µ–ª–∞ –∑–∞–ø—Ä–æ—Å–∞.
        """
        start_time = datetime.now()
        logger.info(f"Executing Webhook node: {node.id}")
        config = node.data.get('config', {})
        node_result = {}
        
        try:
            # --- 1. –ü–æ–ª—É—á–µ–Ω–∏–µ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ ---
            url_template = config.get('url', '')
            method = config.get('method', 'POST').upper()
            headers_str = config.get('headers', 'Content-Type: application/json')
            # –ù–û–í–û–ï: –ü–æ–ª—É—á–∞–µ–º —à–∞–±–ª–æ–Ω —Ç–µ–ª–∞ –∑–∞–ø—Ä–æ—Å–∞
            body_template = config.get('bodyTemplate', '{}') 

            url = replace_templates(url_template, input_data,label_to_id_map)
            if not url:
                raise Exception("Webhook: URL is required in the node settings.")

            headers = {}
            if headers_str:
                for line in headers_str.strip().split('\n'):
                    if ':' in line:
                        key, value = line.split(':', 1)
                        headers[key.strip()] = value.strip()
            
            # --- –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê: –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ payload –∏–∑ —à–∞–±–ª–æ–Ω–∞ ---
            payload = None
            if method in ['POST', 'PUT', 'PATCH']:
                # –ó–∞–ø–æ–ª–Ω—è–µ–º —à–∞–±–ª–æ–Ω –¥–∞–Ω–Ω—ã–º–∏ –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –Ω–æ–¥
                resolved_body_str = replace_templates(body_template, input_data,label_to_id_map)
                try:
                    # –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º —Å—Ç—Ä–æ–∫—É –≤ Python –æ–±—ä–µ–∫—Ç (dict/list)
                    payload = json.loads(resolved_body_str)
                except json.JSONDecodeError:
                    raise Exception(f"Invalid JSON in Request Body after template replacement. Result: {resolved_body_str}")

            logger.info(f"üåê Sending {method} to {url}")
            if payload:
                logger.info(f"üì¶ Payload: {json.dumps(payload, ensure_ascii=False, default=str)[:200]}...")

            # --- 2. –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ HTTP –∑–∞–ø—Ä–æ—Å–∞ ---
            async with aiohttp.ClientSession(headers=headers, timeout=aiohttp.ClientTimeout(total=30)) as session:
                async with session.request(method, url, json=payload, ssl=False) as response:
                    response_text = await response.text()
                    response_json = None
                    try:
                        response_json = json.loads(response_text)
                    except json.JSONDecodeError:
                        pass
                    
                    logger.info(f"‚úÖ Webhook response: {response.status}")

                    # --- 3. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ ---
                    execution_time_ms = int((datetime.now() - start_time).total_seconds() * 1000)
                    node_result = {
                        "text": response_text,
                        "json": response_json,
                        "meta": {
                            "node_type": node.type, "timestamp": datetime.now().isoformat(),
                            "execution_time_ms": execution_time_ms, "success": 200 <= response.status < 300,
                            "status_code": response.status, "response_headers": dict(response.headers),
                        },
                        "inputs": {
                            "url_template": url_template, "final_url": url, "method": method,
                            "headers": headers, "body_template": body_template, "final_payload": payload
                        }
                    }

        except aiohttp.ClientError as e:
            logger.error(f"‚ùå Connection Error in Webhook node {node.id}: {str(e)}")
            node_result = self._create_error_result(node, config, f"Connection Error: {str(e)}", "connection_error", input_data)
        except Exception as e:
            logger.error(f"‚ùå Unexpected Error in Webhook node {node.id}: {str(e)}")
            node_result = self._create_error_result(node, config, str(e), "unexpected_error", input_data)

        # --- 4. –§–∏–Ω–∞–ª—å–Ω—ã–π return –ø–æ "–ó–æ–ª–æ—Ç–æ–º—É –ü—Ä–∞–≤–∏–ª—É" ---
        return node_result

    # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ—à–∏–±–æ–∫
    def _create_error_result(self, node: Node, config: Dict, error_message: str, error_type: str, input_data: Dict) -> Dict:
        return {
            "text": error_message,
            "json": None,
            "meta": {
                "node_type": node.type, "timestamp": datetime.now().isoformat(),
                "success": False, "error_message": error_message, "error_type": error_type
            },
            "inputs": {
                "url_template": config.get('url', ''), "method": config.get('method', 'POST'),
                "headers": config.get('headers', ''), "body_template": config.get('bodyTemplate', ''),
                "input_data_snapshot": input_data # –°–Ω–∏–º–æ–∫ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –º–æ–º–µ–Ω—Ç –æ—à–∏–±–∫–∏
            }
        }


    async def execute_timer(self, node: Node, label_to_id_map: Dict[str, str], input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ Timer –Ω–æ–¥—ã –∫–∞–∫ –ü–ï–†–í–û–ì–û –®–ê–ì–ê –≤ workflow.
        –ï–µ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–∞—è –∑–∞–¥–∞—á–∞ - —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å—Ç–∞—Ä—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ.
        –û–Ω–∞ –±–æ–ª—å—à–µ –Ω–µ —É–ø—Ä–∞–≤–ª—è–µ—Ç —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ–º.
        """
        try:
            logger.info(f"‚è∞ –ù–æ–¥–∞ '–¢–∞–π–º–µ—Ä' {node.id} –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –∫–∞–∫ —á–∞—Å—Ç—å workflow.")
            current_time = datetime.now()
            config = node.data.get('config', {})
            interval = int(config.get('interval', 5))
            timezone = config.get('timezone', 'UTC')

            # –ü—Ä–æ—Å—Ç–æ —Å–æ–∑–¥–∞–µ–º –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –ø–µ—Ä–µ–¥–∞–µ–º –∏—Ö –¥–∞–ª—å—à–µ
            return {
                "success": True,
                "message": f"Workflow triggered by schedule at {current_time.isoformat()}",
                "output": {
                    "text": f"Workflow triggered by schedule at {current_time.isoformat()}",
                    "timestamp": current_time.isoformat(),
                    "interval": interval,
                    "timezone": timezone,
                    "node_id": node.id
                }
            }
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –Ω–æ–¥–µ '–¢–∞–π–º–µ—Ä' {node.id}: {str(e)}")
            return {
                "success": False,
                "error": f"Timer node execution failed: {str(e)}",
                "output": {"text": f"Timer node execution failed: {str(e)}"}
            }

    # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è, –∫–æ—Ç–æ—Ä—É—é –º–æ–∂–Ω–æ —Ä–∞–∑–º–µ—Å—Ç–∏—Ç—å –≤–Ω—É—Ç—Ä–∏ –∫–ª–∞—Å—Å–∞ NodeExecutors –∏–ª–∏ –ø–µ—Ä–µ–¥ –Ω–∏–º
    def _extract_text_from_data(self, data: Any) -> str:
        """–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –∏—â–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â–∏–π —Ç–µ–∫—Å—Ç –≤ –¥–∞–Ω–Ω—ã—Ö."""
        if isinstance(data, str):
            return data
        if not isinstance(data, dict):
            return json.dumps(data, ensure_ascii=False, indent=2)

        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–π –ø–æ–∏—Å–∫
        if 'text' in data and isinstance(data['text'], str):
            return data['text']
        if 'output' in data and isinstance(data['output'], dict) and 'text' in data['output'] and isinstance(data['output']['text'], str):
            return data['output']['text']
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –∏—â–µ–º –≤ –ª—é–±–æ–º –∑–Ω–∞—á–µ–Ω–∏–∏
        for value in data.values():
            if isinstance(value, dict):
                found_text = self._extract_text_from_data(value)
                if found_text:
                    return found_text
            elif isinstance(value, str):
                return value # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–≤–æ–µ –ø–æ–ø–∞–≤—à–µ–µ—Å—è —Å—Ç—Ä–æ–∫–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ

        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ç—Ä–æ–∫–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –≤—Å–µ–≥–æ –æ–±—ä–µ–∫—Ç–∞
        return json.dumps(data, ensure_ascii=False, indent=2)


    async def execute_join(self, node: Node,label_to_id_map: Dict[str, str], input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–π Join/Merge –Ω–æ–¥—ã, –∫–æ—Ç–æ—Ä–∞—è –Ω–∞—Ö–æ–¥–∏—Ç –æ–±—â–∏–µ –¥–∞–Ω–Ω—ã–µ,
        –∏–∑–æ–ª–∏—Ä—É–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∏ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç —á–∏—Å—Ç—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç.
        """
        config = node.data.get('config', {})
        merge_strategy = config.get('mergeStrategy', 'combine_text')
        separator = config.get('separator', '\n\n---\n\n').replace('\\n', '\n')
        
        logger.info(f"üîÄ –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–π Join/Merge –Ω–æ–¥—ã: {node.id}")
        
        inputs = input_data.get('inputs', {})
        if not inputs:
            return {**input_data, "join_result": {"error": "No inputs to join"}, "success": False}
        
        # –ï—Å–ª–∏ –≤—Ö–æ–¥ —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω, –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–±—Ä–∞—Å—ã–≤–∞–µ–º –µ–≥–æ –¥–∞–ª—å—à–µ
        if len(inputs) == 1:
            return list(inputs.values())[0]

        # --- –ú–∞–≥–∏—è –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –∑–¥–µ—Å—å ---

        # –®–∞–≥ 1: –ù–∞—Ö–æ–¥–∏–º –æ–±—â–∏–µ –¥–∞–Ω–Ω—ã–µ
        all_input_dicts = list(inputs.values())
        first_input = all_input_dicts[0]
        other_inputs = all_input_dicts[1:]
        
        common_data = {}
        for key, value in first_input.items():
            # –î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞–ø—Ä—è–º—É—é. –î–ª—è —Å–ª–æ–∂–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è deep comparison.
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∫–ª—é—á –∏ –∑–Ω–∞—á–µ–Ω–∏–µ –∏–¥–µ–Ω—Ç–∏—á–Ω—ã –≤–æ –≤—Å–µ—Ö –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –≤—Ö–æ–¥–∞—Ö
            if all(key in other and other[key] == value for other in other_inputs):
                common_data[key] = value
        
        logger.info(f"üîç –ù–∞–π–¥–µ–Ω—ã –æ–±—â–∏–µ –¥–∞–Ω–Ω—ã–µ: {list(common_data.keys())}")

        # –®–∞–≥ 2: –ò–∑–æ–ª–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–π –≤–µ—Ç–∫–∏
        unique_data_per_source = {}
        for source_id, source_dict in inputs.items():
            unique_data = {k: v for k, v in source_dict.items() if k not in common_data}
            unique_data_per_source[source_id] = unique_data

        # –®–∞–≥ 3: –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
        join_result = {}
        output_data = {}

        if merge_strategy == 'combine_text':
            texts = []
            for source_id, unique_data in unique_data_per_source.items():
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞
                text = self._extract_text_from_data(unique_data)
                texts.append(f"=== –ò—Å—Ç–æ—á–Ω–∏–∫ {source_id} ===\n{text}")
            
            combined_text = separator.join(texts)
            output_data = {
                'text': combined_text,
                'source_count': len(inputs)
            }
            logger.info(f"‚úÖ –û–±—ä–µ–¥–∏–Ω–µ–Ω–æ {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤")

        elif merge_strategy == 'merge_json':
            # –í —ç—Ç–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –º—ã –ø—Ä–æ—Å—Ç–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            output_data = {
                'json': unique_data_per_source,
                'text': json.dumps(unique_data_per_source, ensure_ascii=False, indent=2),
                'source_count': len(inputs)
            }
            logger.info(f"‚úÖ –û–±—ä–µ–¥–∏–Ω–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –≤ JSON –æ—Ç {len(inputs)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")

        else:
            raise Exception(f"Unknown merge strategy: {merge_strategy}")

        # –®–∞–≥ 4: –°–æ–±–∏—Ä–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π, –∏–¥–µ–∞–ª—å–Ω—ã–π return
        final_result = {
            **common_data,  # 1. –í—ã–Ω–æ—Å–∏–º –æ–±—â–∏–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ –≤–µ—Ä—Ö–Ω–∏–π —É—Ä–æ–≤–µ–Ω—å
            "join_result": { # 2. –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è —Å–∞–º–æ–π Join –Ω–æ–¥—ã
                "sources": unique_data_per_source,
                "metadata": {
                    "source_count": len(inputs),
                    "source_ids": list(inputs.keys()),
                    "merge_strategy": merge_strategy,
                    "merge_time": datetime.now().isoformat()
                }
            },
            "output": output_data, # 3. –î–æ–±–∞–≤–ª—è–µ–º —É–¥–æ–±–Ω—ã–π output
            "success": True
        }
        
        return final_result

    async def execute_if_else(self, node: Node,label_to_id_map: Dict[str, str], input_data: Dict[str, Any]) -> Dict[str, Any]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ If/Else –Ω–æ–¥—ã"""
        start_time = time.time()
        config = node.data.get('config', {})
        condition_type = config.get('conditionType', 'equals')
        field_path = config.get('fieldPath', 'output.text')
        compare_value = config.get('compareValue', '')
        case_sensitive = config.get('caseSensitive', False)
        
        logger.info(f"üîÄ –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ If/Else –Ω–æ–¥—ã: {node.id}")
        logger.info(f"üìã –£—Å–ª–æ–≤–∏–µ: {field_path} {condition_type} {compare_value}")
        
        # –ü–æ–ª—É—á–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ –ø—É—Ç–∏
        def get_value_by_path(data: Dict[str, Any], path: str) -> Any:
            keys = path.split('.')
            current = data
            
            for key in keys:
                if isinstance(current, dict) and key in current:
                    current = current[key]
                elif isinstance(current, list) and key.isdigit():
                    index = int(key)
                    if 0 <= index < len(current):
                        current = current[index]
                    else:
                        return None
                else:
                    return None
            
            return current
        
        actual_value = get_value_by_path(input_data, field_path)
        
        if actual_value is None and condition_type not in ['exists', 'is_empty']:
            logger.warning(f"‚ö†Ô∏è –ü–æ–ª–µ {field_path} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –¥–∞–Ω–Ω—ã—Ö")
            actual_value = ""
        
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ —Å—Ç—Ä–æ–∫–µ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è (–µ—Å–ª–∏ –Ω–µ —á–∏—Å–ª–æ)
        if condition_type not in ['greater', 'greater_equal', 'less', 'less_equal']:
            actual_value_str = str(actual_value) if actual_value is not None else ""
            compare_value_str = str(compare_value)
            
            if not case_sensitive:
                actual_value_str = actual_value_str.lower()
                compare_value_str = compare_value_str.lower()
        else:
            # –î–ª—è —á–∏—Å–ª–æ–≤—ã—Ö —Å—Ä–∞–≤–Ω–µ–Ω–∏–π
            try:
                actual_value_str = float(actual_value)
                compare_value_str = float(compare_value)
            except (ValueError, TypeError):
                actual_value_str = 0
                compare_value_str = 0
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Å–ª–æ–≤–∏–µ
        result = False
        
        if condition_type == 'equals':
            result = actual_value_str == compare_value_str
        elif condition_type == 'not_equals':
            result = actual_value_str != compare_value_str
        elif condition_type == 'contains':
            result = compare_value_str in actual_value_str
        elif condition_type == 'not_contains':
            result = compare_value_str not in actual_value_str
        elif condition_type == 'greater':
            result = actual_value_str > compare_value_str
        elif condition_type == 'greater_equal':
            result = actual_value_str >= compare_value_str
        elif condition_type == 'less':
            result = actual_value_str < compare_value_str
        elif condition_type == 'less_equal':
            result = actual_value_str <= compare_value_str
        elif condition_type == 'regex':
            import re
            try:
                result = bool(re.search(compare_value, str(actual_value)))
            except re.error:
                result = False
        elif condition_type == 'exists':
            result = actual_value is not None
        elif condition_type == 'is_empty':
            result = actual_value is None or str(actual_value).strip() == ""
        elif condition_type == 'is_not_empty':
            result = actual_value is not None and str(actual_value).strip() != ""
        
        branch = 'true' if result else 'false'
        
        logger.info(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏: {result} (–≤–µ—Ç–∫–∞: {branch})")
        logger.info(f"üìç –§–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: {actual_value}")
        logger.info(f"üìç –û–∂–∏–¥–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: {compare_value}")
        
        # --- –ö–õ–Æ–ß–ï–í–û–ï –ò–ó–ú–ï–ù–ï–ù–ò–ï ---
        # –ú—ã –Ω–µ —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π output, –∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ,
        # –¥–æ–±–∞–≤–ª—è—è –∫ –Ω–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–ª—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏.
        # –≠—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –Ω–æ–¥ –Ω–µ –±—É–¥—É—Ç –ø–æ—Ç–µ—Ä—è–Ω—ã.
        return {
            **input_data,  # <--- –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –≤—Å–µ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–∞–ª—å—à–µ
            'success': True,
            'branch': branch, # <--- –≠—Ç–æ –ø–æ–ª–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –≤—ã–±–æ—Ä–∞ —Å–ª–µ–¥—É—é—â–µ–π –Ω–æ–¥—ã
            'if_else_result': { # <--- –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–π –±–ª–æ–∫ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ If/Else –¥–ª—è —è—Å–Ω–æ—Å—Ç–∏
                'condition_met': result,
                'checked_value': str(actual_value),
                'condition': f"{field_path} {condition_type} {compare_value}",
                'node_id': node.id
            }
        }

    # –ù–û–í–û–ï: –õ–æ–≥–∏–∫–∞ –¥–ª—è –Ω–æ–¥—ã-–¥–∏—Å–ø–µ—Ç—á–µ—Ä–∞
    async def execute_dispatcher(self, node: Node, label_to_id_map: Dict[str, str], input_data: Dict[str, Any]):
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –¥–∏—Å–ø–µ—Ç—á–µ—Ä –≤ —Ä–µ–∂–∏–º–µ router –∏–ª–∏ orchestrator"""
        config = node.data.get('config', {})
        dispatcher_type = config.get('dispatcher_type', 'router')
        
        logger.info(f"üéØ Executing dispatcher {node.id} in {dispatcher_type} mode")
        
        if dispatcher_type == 'router':
            # –°—É—â–µ—Å—Ç–≤—É—é—â–∞—è –ª–æ–≥–∏–∫–∞ - –ø—Ä–æ—Å—Ç–æ–π —Ä–æ—É—Ç–∏–Ω–≥
            return await self.execute_router_dispatcher(node, label_to_id_map, input_data)
        
        elif dispatcher_type == 'orchestrator':
            # –ù–û–í–ê–Ø –ª–æ–≥–∏–∫–∞ - –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏—è
            return await self.execute_orchestrator_dispatcher(node, label_to_id_map, input_data)
        
        else:
            raise Exception(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –¥–∏—Å–ø–µ—Ç—á–µ—Ä–∞: {dispatcher_type}")
    async def execute_router_dispatcher(self, node: Node, label_to_id_map: Dict[str, str], input_data: Dict[str, Any]) -> Dict[str, Any]:
        """–ê–≥–µ–Ω—Ç-–¥–∏—Å–ø–µ—Ç—á–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å –∏ –≤—ã–±–∏—Ä–∞–µ—Ç –Ω—É–∂–Ω—ã–π workflow (—Å—Ç–∞—Ä–∞—è –ª–æ–≥–∏–∫–∞)"""
        logger.info(f"DEBUG: input_data for dispatcher: {json.dumps(input_data, ensure_ascii=False, indent=2)}")

        logger.info(f"üîÄ Executing Router Dispatcher node: {node.id}")
        config = node.data.get('config', {})
        
        # –ù–æ–≤—ã–π –±–ª–æ–∫: –ø–æ–ª—É—á–∞–µ–º —à–∞–±–ª–æ–Ω –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π
        query_template = config.get('userQueryTemplate') or '{{ input.output.text }}'

        # –î–ª—è replace_templates –Ω—É–∂–µ–Ω label_to_id_map, –∫–∞–∫ –≤ execute_workflow_internal
        # –ï—Å–ª–∏ label_to_id_map –ø—É—Å—Ç–æ–π, –º–æ–∂–Ω–æ –ø–æ–ø—ã—Ç–∞—Ç—å—Å—è –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –µ–≥–æ –∏–∑ input_data:
        if not label_to_id_map:
            # –ü–æ–ø—Ä–æ–±—É–µ–º —Å–æ–±—Ä–∞—Ç—å –∏–∑ input_data (–µ—Å–ª–∏ —ç—Ç–æ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä)
            label_to_id_map = {}
            for k, v in input_data.items():
                if isinstance(v, dict) and 'label' in v:
                    label_to_id_map[v['label']] = k

        # –ü–æ–¥—Å—Ç–∞–≤–ª—è–µ–º —à–∞–±–ª–æ–Ω
        user_query = replace_templates(query_template, input_data, label_to_id_map).strip()


        if not user_query:
            raise Exception("Dispatcher: User query not found in input data.")

        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–∞—Ä—à—Ä—É—Ç–æ–≤
        workflow_routes = config.get('routes', {})
        if not workflow_routes:
            raise Exception("Dispatcher: Routes are not configured.")

        category = 'default'
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º GigaChat –¥–ª—è —É–º–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        if config.get('useAI', True):
            auth_token = config.get('dispatcherAuthToken')
            if not auth_token:
                raise Exception("Dispatcher: GigaChat auth token is required for AI mode.")

            # classification_prompt = f"""–û–ø—Ä–µ–¥–µ–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –≤—ã–±–µ—Ä–∏ –ø–æ–¥—Ö–æ–¥—è—â–∏–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫.
            # –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {json.dumps(list(workflow_routes.keys()), ensure_ascii=False)}
            # –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {user_query}
            # –û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û –æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º - –Ω–∞–∑–≤–∞–Ω–∏–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏."""
            # –ù–æ–≤—ã–π –±–ª–æ–∫: –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞—Å—Ç–æ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
            dispatcher_prompt = config.get('dispatcherPrompt')
            DEFAULT_PROMPT = (
                "–û–ø—Ä–µ–¥–µ–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –≤—ã–±–µ—Ä–∏ –ø–æ–¥—Ö–æ–¥—è—â–∏–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫.\n"
                "–î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {–∫–∞—Ç–µ–≥–æ—Ä–∏–∏}\n"
                "–ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {–∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è}\n"
                "–û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û –æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º - –Ω–∞–∑–≤–∞–Ω–∏–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏."
            )
            categories_str = ", ".join(workflow_routes.keys())
            prompt_template = dispatcher_prompt or DEFAULT_PROMPT
            classification_prompt = prompt_template.replace("{–∫–∞—Ç–µ–≥–æ—Ä–∏–∏}", categories_str).replace("{–∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è}", user_query)
            
            # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–∫–µ–Ω –∏ –¥–µ–ª–∞–µ–º –∑–∞–ø—Ä–æ—Å
            if await self.gigachat_api.get_token(auth_token):
                gigachat_result = await self.gigachat_api.get_chat_completion(
                    "–¢—ã - –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∑–∞–ø—Ä–æ—Å–æ–≤. –û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º - –Ω–∞–∑–≤–∞–Ω–∏–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏.",
                    classification_prompt
                )
                response_text = gigachat_result.get('response', 'default').strip().lower()
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ GigaChat –≤–µ—Ä–Ω—É–ª –≤–∞–ª–∏–¥–Ω—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é
                if response_text in workflow_routes:
                    category = response_text
                else:
                    logger.warning(f"GigaChat returned an unknown category '{response_text}', using default.")
            else:
                logger.error("Dispatcher: Failed to get GigaChat token.")

        else:
            # –ü—Ä–æ—Å—Ç–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
            query_lower = user_query.lower()
            for cat_name, cat_info in workflow_routes.items():
                if cat_name != 'default' and 'keywords' in cat_info:
                    if any(keyword.lower() in query_lower for keyword in cat_info['keywords']):
                        category = cat_name
                        break
        
        selected_route = workflow_routes.get(category, workflow_routes.get('default'))
        if not selected_route:
            raise Exception(f"Dispatcher: No route found for category '{category}' and no default route is set.")

        workflow_id = selected_route['workflow_id']
        logger.info(f"üéØ –î–∏—Å–ø–µ—Ç—á–µ—Ä –≤—ã–±—Ä–∞–ª –∫–∞—Ç–µ–≥–æ—Ä–∏—é: {category} -> –ó–∞–ø—É—Å–∫ workflow: {workflow_id}")

        if workflow_id not in saved_workflows:
            raise Exception(f"Dispatcher: Target workflow '{workflow_id}' not found.")

        # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—ã–±—Ä–∞–Ω–Ω—ã–π workflow
        workflow_data = saved_workflows[workflow_id]
        workflow_request = WorkflowExecuteRequest(
            nodes=workflow_data["nodes"],
            connections=workflow_data["connections"]
        )
        
        # –ü–µ—Ä–µ–¥–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –≤—ã–±—Ä–∞–Ω–Ω—ã–π workflow
        sub_workflow_result = await execute_workflow_internal(
            workflow_request, 
            initial_input_data={
                **input_data,
                "dispatcher_info": {
                    "category": category,
                    "original_query": user_query,
                    "selected_workflow": workflow_id
                }
            }
        )

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ä–∞–±–æ—Ç—ã –ø–æ–¥-–ø—Ä–æ—Ü–µ—Å—Å–∞
        return {
            **sub_workflow_result.result,
            "success": sub_workflow_result.success,
            "dispatcher_category": category,
            "executed_workflow_id": workflow_id,
            "output": {
                "text": f"–†–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ç workflow '{workflow_id}': {sub_workflow_result.result}",
                "json": sub_workflow_result.result
            }
        }
    async def execute_orchestrator_dispatcher(self, node: Node, label_to_id_map: Dict[str, str], input_data: Dict[str, Any]):
        """–ü–ª–∞–Ω–∏—Ä—É—é—â–∏–π –¥–∏—Å–ø–µ—Ç—á–µ—Ä - —Å–æ–∑–¥–∞–µ—Ç –ø–ª–∞–Ω –∏ –∫–æ–æ—Ä–¥–∏–Ω–∏—Ä—É–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ"""
        config = node.data.get('config', {})
        session_id = input_data.get('session_id')
        dispatcher_id = node.id
        
        logger.info(f"üéº Orchestrator dispatcher {dispatcher_id} processing request")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Å–µ—Å—Å–∏–π –¥–ª—è —ç—Ç–æ–≥–æ –¥–∏—Å–ø–µ—Ç—á–µ—Ä–∞
        if dispatcher_id not in self.dispatcher_sessions:
            self.dispatcher_sessions[dispatcher_id] = {}
        
        sessions = self.dispatcher_sessions[dispatcher_id]
        
        # 1. –ï—Å–ª–∏ —ç—Ç–æ –≤–æ–∑–≤—Ä–∞—Ç –æ—Ç workflow
        if input_data.get('return_to_dispatcher'):
            logger.info(f"üì• Handling workflow return for session {session_id}")
            return await self.handle_workflow_return(dispatcher_id, sessions, input_data)
        
        # 2. –ï—Å–ª–∏ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π —Å–µ—Å—Å–∏–∏
        elif session_id and session_id in sessions:
            logger.info(f"üîÑ Continuing session {session_id}")
            return await self.handle_session_continuation(dispatcher_id, sessions, input_data)
        
        # 3. –ï—Å–ª–∏ –Ω–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å - —Å–æ–∑–¥–∞–µ–º –ø–ª–∞–Ω
        else:
            logger.info(f"üÜï Creating new session for new request")
            return await self.create_new_orchestrator_session(dispatcher_id, sessions, config, input_data)
    async def create_new_orchestrator_session(self, dispatcher_id: str, sessions: Dict, config: Dict, input_data: Dict[str, Any]):
        """–°–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—É—é —Å–µ—Å—Å–∏—é –∏ –ø–ª–∞–Ω –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è"""
        import uuid
        from datetime import datetime
        
        session_id = str(uuid.uuid4())
        user_query = input_data.get('user_query', input_data.get('message', ''))
        
        logger.info(f"üìã Creating execution plan for: {user_query}")
        
        # –°–æ–∑–¥–∞–µ–º –ø–ª–∞–Ω —á–µ—Ä–µ–∑ GigaChat
        plan = await self.create_execution_plan(config, user_query)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–µ—Å—Å–∏—é
        sessions[session_id] = {
            "plan": plan,
            "current_step": 0,
            "user_query": user_query,
            "accumulated_data": {},
            "created_at": datetime.now(),
            "dispatcher_id": dispatcher_id
        }
        
        logger.info(f"üíæ Session {session_id} created with {len(plan)} steps")
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–µ—Ä–≤—ã–π workflow
        if plan:
            first_step = plan[0]
            workflow_id = first_step.get('workflow_id')
            
            if workflow_id:
                workflow_input = {
                    **input_data,
                    "session_id": session_id,
                    "dispatcher_context": {
                        "plan": plan,
                        "step": 0,
                        "dispatcher_id": dispatcher_id
                    }
                }
                
                logger.info(f"üöÄ Launching first workflow: {workflow_id}")
                return await self.launch_workflow_by_id(workflow_id, workflow_input)
            else:
                raise Exception("–ü–µ—Ä–≤—ã–π —à–∞–≥ –ø–ª–∞–Ω–∞ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç workflow_id")
        else:
            raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –ø–ª–∞–Ω –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è")
    async def create_execution_plan(self, config: Dict, user_query: str):
        """–°–æ–∑–¥–∞–µ—Ç –ø–ª–∞–Ω –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —á–µ—Ä–µ–∑ GigaChat"""
        import json
        
        available_workflows = config.get('available_workflows', {})
        auth_token = config.get('dispatcherAuthToken', '')
        
        if not auth_token:
            raise Exception("–¢–æ–∫–µ–Ω –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è –ø–ª–∞–Ω–∏—Ä—É—é—â–µ–≥–æ –¥–∏—Å–ø–µ—Ç—á–µ—Ä–∞ –Ω–µ —É–∫–∞–∑–∞–Ω")
        
        if not available_workflows:
            raise Exception("–î–æ—Å—Ç—É–ø–Ω—ã–µ workflow –¥–ª—è –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–µ —É–∫–∞–∑–∞–Ω—ã")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö workflow
        workflows_description = "\n".join([
            f"- {wf_id}: {wf_config.get('description', '–û–ø–∏—Å–∞–Ω–∏–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç')}"
            for wf_id, wf_config in available_workflows.items()
        ])
        
        planning_prompt = f"""
        –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–æ—Å–∏—Ç: "{user_query}"
        
        –î–æ—Å—Ç—É–ø–Ω—ã–µ workflow –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:
        {workflows_description}
        
        –°–æ–∑–¥–∞–π –ø–æ—à–∞–≥–æ–≤—ã–π –ø–ª–∞–Ω –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON –º–∞—Å—Å–∏–≤–∞:
        [
            {{"workflow_id": "workflow1", "description": "—á—Ç–æ –¥–µ–ª–∞–µ—Ç —ç—Ç–æ—Ç —à–∞–≥"}},
            {{"workflow_id": "workflow2", "description": "—á—Ç–æ –¥–µ–ª–∞–µ—Ç —ç—Ç–æ—Ç —à–∞–≥"}}
        ]
        
        –ü—Ä–∞–≤–∏–ª–∞:
        1. –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ workflow_id –∏–∑ —Å–ø–∏—Å–∫–∞ –≤—ã—à–µ
        2. –°–æ–∑–¥–∞–≤–∞–π –ª–æ–≥–∏—á–Ω—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —à–∞–≥–æ–≤
        3. –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û JSON –º–∞—Å—Å–∏–≤–æ–º, –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        4. –ï—Å–ª–∏ –∑–∞–¥–∞—á–∞ –ø—Ä–æ—Å—Ç–∞—è, –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–¥–∏–Ω workflow
        """
        
        if await self.gigachat_api.get_token(auth_token):
            result = await self.gigachat_api.get_chat_completion(
                "–¢—ã –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –∑–∞–¥–∞—á. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ —Å–æ–∑–¥–∞–≤–∞–π –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–ª–∞–Ω –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∏–∑ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö workflow.",
                planning_prompt
            )
            
            try:
                # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON
                plan = json.loads(result['response'])
                
                # –í–∞–ª–∏–¥–∏—Ä—É–µ–º –ø–ª–∞–Ω
                if not isinstance(plan, list):
                    raise ValueError("–ü–ª–∞–Ω –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –º–∞—Å—Å–∏–≤–æ–º")
                
                for step in plan:
                    if not isinstance(step, dict) or 'workflow_id' not in step:
                        raise ValueError("–ö–∞–∂–¥—ã–π —à–∞–≥ –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å workflow_id")
                    
                    if step['workflow_id'] not in available_workflows:
                        raise ValueError(f"Workflow {step['workflow_id']} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö")
                
                logger.info(f"üìã –°–æ–∑–¥–∞–Ω –ø–ª–∞–Ω –∏–∑ {len(plan)} —à–∞–≥–æ–≤: {[s['workflow_id'] for s in plan]}")
                return plan
                
            except (json.JSONDecodeError, ValueError) as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –ø–ª–∞–Ω–∞: {result['response']}")
                logger.error(f"‚ùå –î–µ—Ç–∞–ª–∏ –æ—à–∏–±–∫–∏: {str(e)}")
                
                # Fallback –∫ –ø—Ä–æ—Å—Ç–æ–º—É –ø–ª–∞–Ω—É
                fallback_workflows = list(available_workflows.keys())
                if fallback_workflows:
                    fallback_plan = [{"workflow_id": fallback_workflows[0], "description": "Fallback workflow"}]
                    logger.info(f"üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback –ø–ª–∞–Ω: {fallback_plan}")
                    return fallback_plan
                else:
                    raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –ø–ª–∞–Ω –∏ –Ω–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö workflow –¥–ª—è fallback")
        
        else:
            raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å –∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞—Ç—å—Å—è –≤ GigaChat API –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–ª–∞–Ω–∞")
    async def handle_workflow_return(self, dispatcher_id: str, sessions: Dict, input_data: Dict[str, Any]):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–æ–∑–≤—Ä–∞—Ç –æ—Ç workflow"""
        session_id = input_data.get('session_id')
        
        if not session_id or session_id not in sessions:
            raise Exception(f"–°–µ—Å—Å–∏—è {session_id} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –¥–∏—Å–ø–µ—Ç—á–µ—Ä–µ {dispatcher_id}")
        
        session = sessions[session_id]
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        workflow_result = input_data.get('workflow_result', {})
        completed_workflow = input_data.get('completed_workflow', 'unknown')
        
        logger.info(f"üì• Workflow {completed_workflow} –∑–∞–≤–µ—Ä—à–µ–Ω –¥–ª—è —Å–µ—Å—Å–∏–∏ {session_id}")
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∫ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º
        session['accumulated_data'][f"step_{session['current_step']}_result"] = workflow_result
        session['accumulated_data'][f"step_{session['current_step']}_workflow"] = completed_workflow
        
        # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–º—É —à–∞–≥—É
        session['current_step'] += 1
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –µ—â–µ —à–∞–≥–∏ –≤ –ø–ª–∞–Ω–µ
        if session['current_step'] < len(session['plan']):
            # –ó–∞–ø—É—Å–∫–∞–µ–º —Å–ª–µ–¥—É—é—â–∏–π workflow
            next_step = session['plan'][session['current_step']]
            next_workflow_id = next_step.get('workflow_id')
            
            logger.info(f"‚û°Ô∏è –ü–µ—Ä–µ—Ö–æ–¥ –∫ —à–∞–≥—É {session['current_step']}: {next_workflow_id}")
            
            workflow_input = {
                "session_id": session_id,
                "user_query": session['user_query'],
                "previous_results": session['accumulated_data'],
                "dispatcher_context": {
                    "plan": session['plan'],
                    "step": session['current_step'],
                    "dispatcher_id": dispatcher_id
                }
            }
            
            return await self.launch_workflow_by_id(next_workflow_id, workflow_input)
        
        else:
            # –ü–ª–∞–Ω –≤—ã–ø–æ–ª–Ω–µ–Ω –ø–æ–ª–Ω–æ—Å—Ç—å—é
            logger.info(f"‚úÖ –ü–ª–∞–Ω –¥–ª—è —Å–µ—Å—Å–∏–∏ {session_id} –≤—ã–ø–æ–ª–Ω–µ–Ω –ø–æ–ª–Ω–æ—Å—Ç—å—é")
            
            final_result = {
                "success": True,
                "message": "–ü–ª–∞–Ω –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ",
                "session_id": session_id,
                "completed_steps": len(session['plan']),
                "results": session['accumulated_data'],
                "session_completed": True,
                "output": {
                    "text": f"–í—ã–ø–æ–ª–Ω–µ–Ω –ø–ª–∞–Ω –∏–∑ {len(session['plan'])} —à–∞–≥–æ–≤",
                    "json": session['accumulated_data']
                }
            }
            
            # –£–¥–∞–ª—è–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—É—é —Å–µ—Å—Å–∏—é
            del sessions[session_id]
            logger.info(f"üóëÔ∏è –°–µ—Å—Å–∏—è {session_id} —É–¥–∞–ª–µ–Ω–∞")
            
            return final_result
    async def handle_session_continuation(self, dispatcher_id: str, sessions: Dict, input_data: Dict[str, Any]):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π —Å–µ—Å—Å–∏–∏ (–Ω–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è)"""
        session_id = input_data.get('session_id')
        session = sessions[session_id]
        
        user_query = input_data.get('user_query', input_data.get('message', ''))
        
        logger.info(f"üîÑ –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Å–µ—Å—Å–∏–∏ {session_id}: {user_query}")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –∫ —Å–µ—Å—Å–∏–∏
        if 'additional_requests' not in session:
            session['additional_requests'] = []
        
        session['additional_requests'].append({
            "query": user_query,
            "timestamp": datetime.now().isoformat()
        })
        
        # –ü–æ–∫–∞ —á—Ç–æ –ø—Ä–æ—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–µ–∫—É—â–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏
        # –í –±—É–¥—É—â–µ–º –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É –¥–ª—è –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–ª–∞–Ω–∞
        current_step = session['current_step']
        total_steps = len(session['plan'])
        
        return {
            "success": True,
            "message": f"–ó–∞–ø—Ä–æ—Å –¥–æ–±–∞–≤–ª–µ–Ω –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π —Å–µ—Å—Å–∏–∏. –í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è —à–∞–≥ {current_step + 1} –∏–∑ {total_steps}",
            "session_id": session_id,
            "current_step": current_step,
            "total_steps": total_steps,
            "additional_request_added": True,
            "output": {
                "text": f"–í–∞—à –∑–∞–ø—Ä–æ—Å –ø—Ä–∏–Ω—è—Ç. –°–µ–π—á–∞—Å –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è —à–∞–≥ {current_step + 1} –∏–∑ {total_steps}",
                "json": {
                    "session_status": "active",
                    "progress": f"{current_step}/{total_steps}"
                }
            }
        }
    async def launch_workflow_by_id(self, workflow_id: str, input_data: Dict[str, Any]):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç workflow –ø–æ ID"""
        if workflow_id not in saved_workflows:
            raise Exception(f"Workflow {workflow_id} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö workflow")
        
        workflow_data = saved_workflows[workflow_id]
        
        logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ workflow {workflow_id}")
        
        workflow_request = WorkflowExecuteRequest(
            nodes=workflow_data["nodes"],
            connections=workflow_data["connections"]
        )
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º workflow —Å –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        result = await execute_workflow_internal(workflow_request, initial_input_data=input_data)
        
        return {
            "success": result.success,
            "workflow_id": workflow_id,
            "result": result.result,
            "error": result.error,
            "logs": result.logs,
            "output": {
                "text": f"Workflow {workflow_id} {'–≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ' if result.success else '–∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–æ–π'}",
                "json": result.result
            }
        }

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª–µ–π
executors = NodeExecutors()

# API —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã
@app.get("/")
async def root():
    return {"message": "N8N Clone API Server", "version": "1.0.0"}

@app.get("/health")
async def health_check():
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}

@app.post("/execute-node")
async def execute_node(
    node_type: str,
    node_data: Dict[str, Any],
    input_data: Dict[str, Any] = None
):
    """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω–æ–π –Ω–æ–¥—ã"""
    try:
        logger.info(f"Received request for node_type: {node_type}")
        logger.info(f"node_data: {json.dumps(node_data, indent=2)}")
        # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç –Ω–æ–¥—ã
        node = Node(
            id=node_data.get('id', 'temp'),
            type=node_type,
            position=node_data.get('position', {'x': 0, 'y': 0}),
            data=node_data.get('data', {})
        )
        logger.info(f"Created node: {node}")
        logger.info(f"Node data: {node.data}")
        logger.info(f"Config: {node.data.get('config', {})}")

        # –í—ã–±–∏—Ä–∞–µ–º –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å
        executor_map = {
            'gigachat': executors.execute_gigachat,
            'email': executors.execute_email,
            'database': executors.execute_database,
            'webhook': executors.execute_webhook,
            'timer': executors.execute_timer,
            'join': executors.execute_join,
            'request_iterator': executors.execute_request_iterator,
            'if_else': executors.execute_if_else,  # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–π –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å
            'dispatcher': executors.execute_dispatcher # –ù–û–í–û–ï
            
        }

        executor = executor_map.get(node_type)
        if not executor:
            raise HTTPException(status_code=400, detail=f"Unknown node type: {node_type}")

        # –í—ã–ø–æ–ª–Ω—è–µ–º –Ω–æ–¥—É
        result = await executor(node, {},input_data or {})
        
        return ExecutionResult(
            success=True,
            result=result,
            logs=[{
                "message": f"Node {node_type} executed successfully",
                "timestamp": datetime.now().isoformat(),
                "level": "info"
            }]
        )

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –Ω–æ–¥—ã {node_type}: {str(e)}")
        return ExecutionResult(
            success=False,
            error=str(e),
            logs=[{
                "message": f"Error executing node {node_type}: {str(e)}",
                "timestamp": datetime.now().isoformat(),
                "level": "error"
            }]
        )

# --- –ù–û–í–û–ï: CRUD –≠–Ω–¥–ø–æ–∏–Ω—Ç—ã –¥–ª—è Workflows ---

@app.get("/workflows")
async def list_workflows():
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö workflows."""
    return {
        "workflows": [
            {"id": wf_id, "name": wf_data.get("name", wf_id)}
            for wf_id, wf_data in saved_workflows.items()
        ]
    }

@app.get("/workflows/{workflow_id}")
async def get_workflow(workflow_id: str):
    """–ü–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ workflow –ø–æ –µ–≥–æ ID."""
    if workflow_id not in saved_workflows:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Workflow not found")
    return saved_workflows[workflow_id]

@app.post("/workflows", status_code=status.HTTP_201_CREATED)
async def create_workflow(request: WorkflowSaveRequest):
    """–°–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—ã–π workflow."""
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º ID –∏–∑ –∏–º–µ–Ω–∏, –¥–µ–ª–∞—è –µ–≥–æ –±–µ–∑–æ–ø–∞—Å–Ω—ã–º –¥–ª—è URL
    workflow_id = re.sub(r'[^a-z0-9_]+', '', request.name.lower().replace(" ", "_"))
    if not workflow_id:
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="Invalid workflow name, results in empty ID.")
    if workflow_id in saved_workflows:
        raise HTTPException(status_code=status.HTTP_409_CONFLICT, detail=f"Workflow with ID '{workflow_id}' already exists.")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º workflow –≤ –ø–∞–º—è—Ç–∏
    saved_workflows[workflow_id] = {
        "name": request.name,
        "nodes": [node.dict() for node in request.nodes],
        "connections": [conn.dict() for conn in request.connections],
        "created_at": datetime.now().isoformat(),
        "updated_at": datetime.now().isoformat()
    }
    _save_workflows_to_disk() # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞ –¥–∏—Å–∫
    logger.info(f"‚úÖ Workflow '{request.name}' (ID: {workflow_id}) —Å–æ–∑–¥–∞–Ω.")
    return {"success": True, "workflow_id": workflow_id, "name": request.name}

@app.put("/workflows/{workflow_id}")
async def update_workflow(workflow_id: str, request: WorkflowUpdateRequest):
    """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π workflow."""
    if workflow_id not in saved_workflows:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Workflow not found")
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
    saved_workflows[workflow_id]["nodes"] = [node.dict() for node in request.nodes]
    saved_workflows[workflow_id]["connections"] = [conn.dict() for conn in request.connections]
    saved_workflows[workflow_id]["updated_at"] = datetime.now().isoformat()
    _save_workflows_to_disk() # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞ –¥–∏—Å–∫
    logger.info(f"üîÑ Workflow '{workflow_id}' –æ–±–Ω–æ–≤–ª–µ–Ω.")
    return {"success": True, "message": f"Workflow '{workflow_id}' updated successfully."}

@app.delete("/workflows/{workflow_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_workflow(workflow_id: str):
    """–£–¥–∞–ª—è–µ—Ç workflow."""
    if workflow_id not in saved_workflows:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Workflow not found")
    
    del saved_workflows[workflow_id]
    _save_workflows_to_disk() # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞ –¥–∏—Å–∫
    logger.info(f"üóëÔ∏è Workflow '{workflow_id}' —É–¥–∞–ª–µ–Ω.")
# –î–æ–±–∞–≤—å—Ç–µ —ç—Ç–æ—Ç —ç–Ω–¥–ø–æ–∏–Ω—Ç –ø–æ—Å–ª–µ –¥—Ä—É–≥–∏—Ö —ç–Ω–¥–ø–æ–∏–Ω—Ç–æ–≤ (–ø—Ä–∏–º–µ—Ä–Ω–æ —Å—Ç—Ä–æ–∫–∞ 600-650)
# @app.post("/save-workflow")
# async def save_workflow(request: WorkflowSaveRequest):
#     """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç workflow –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ"""
#     try:
#         workflow_id = request.name.lower().replace(" ", "_")
        
#         # –°–æ—Ö—Ä–∞–Ω—è–µ–º workflow –≤ –ø–∞–º—è—Ç–∏
#         saved_workflows[workflow_id] = {
#             "name": request.name,
#             "nodes": request.nodes,
#             "connections": request.connections,
#             "updated_at": datetime.now().isoformat()
#         }
        
#         # –ú–æ–∂–Ω–æ —Ç–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –Ω–∞ –¥–∏—Å–∫ –¥–ª—è –ø–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
#         # with open(f"workflows/{workflow_id}.json", "w") as f:
#         #     json.dump(saved_workflows[workflow_id], f)
        
#         logger.info(f"‚úÖ Workflow '{request.name}' —Å–æ—Ö—Ä–∞–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ")
        
#         return {
#             "success": True,
#             "workflow_id": workflow_id,
#             "message": f"Workflow '{request.name}' saved successfully"
#         }
#     except Exception as e:
#         logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è workflow: {str(e)}")
#         return {
#             "success": False,
#             "error": str(e)
#         }

@app.post("/execute-workflow/{workflow_id}")
async def execute_saved_workflow(workflow_id: str, initial_input_data: Optional[Dict[str, Any]] = Body(None)):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π workflow –ø–æ –µ–≥–æ ID."""
    if workflow_id not in saved_workflows:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Workflow not found")

    workflow_data = saved_workflows[workflow_id]
    
    workflow_request = WorkflowExecuteRequest(
        nodes=workflow_data["nodes"],
        connections=workflow_data["connections"]
    )

    return await execute_workflow_internal(workflow_request, initial_input_data=initial_input_data)


# –î–æ–±–∞–≤—å—Ç–µ –Ω–æ–≤—ã–µ —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã –ø–æ—Å–ª–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö

@app.post("/webhooks/create")
async def create_webhook(request: WebhookCreateRequest):
    """–°–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—ã–π –≤–µ–±—Ö—É–∫ –¥–ª—è workflow"""
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ workflow
        if request.workflow_id not in saved_workflows:
            raise HTTPException(status_code=404, detail=f"Workflow {request.workflow_id} not found")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –¥–ª—è –≤–µ–±—Ö—É–∫–∞
        webhook_id = str(uuid.uuid4())
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤–µ–±—Ö—É–∫–µ
        webhook_info = {
            "webhook_id": webhook_id,
            "workflow_id": request.workflow_id,
            "name": request.name,
            "description": request.description,
            "created_at": datetime.now().isoformat(),
            "auth_required": request.auth_required,
            "allowed_ips": request.allowed_ips or [],
            "call_count": 0,
            "last_called": None
        }
        
        webhook_triggers[webhook_id] = webhook_info
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        webhook_stats[webhook_id] = {
            "total_calls": 0,
            "successful_calls": 0,
            "failed_calls": 0,
            "last_error": None,
            "call_history": []  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 10 –≤—ã–∑–æ–≤–æ–≤
        }
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π URL
        base_url = "http://localhost:8000"  # –í –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ –±–µ—Ä–∏—Ç–µ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
        webhook_url = f"{base_url}/webhooks/{webhook_id}"
        
        logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω –≤–µ–±—Ö—É–∫ {webhook_id} –¥–ª—è workflow {request.workflow_id}")
        
        return WebhookInfo(
            webhook_id=webhook_id,
            workflow_id=request.workflow_id,
            name=request.name,
            description=request.description,
            created_at=webhook_info["created_at"],
            url=webhook_url,
            auth_required=request.auth_required,
            allowed_ips=request.allowed_ips,
            call_count=0,
            last_called=None
        )
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≤–µ–±—Ö—É–∫–∞: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/webhooks/{webhook_id}")
async def trigger_webhook(
    webhook_id: str,
    request: Request,
    body: Dict[str, Any] = Body(...),
    authorization: Optional[str] = Header(None)
):
    logger.info(f"üîî Webhook {webhook_id} triggered")
    logger.info(f"üì¶ Received data: {json.dumps(body, ensure_ascii=False)[:200]}...")
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –≤—ã–∑–æ–≤–∞ –≤–µ–±—Ö—É–∫–∞"""
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –≤–µ–±—Ö—É–∫–∞
        if webhook_id not in webhook_triggers:
            raise HTTPException(status_code=404, detail="Webhook not found")
        
        webhook_info = webhook_triggers[webhook_id]
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ IP-–∞–¥—Ä–µ—Å–∞ –µ—Å–ª–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞
        client_ip = request.client.host
        if webhook_info.get("allowed_ips") and client_ip not in webhook_info["allowed_ips"]:
            logger.warning(f"‚ö†Ô∏è –ü–æ–ø—ã—Ç–∫–∞ –≤—ã–∑–æ–≤–∞ –≤–µ–±—Ö—É–∫–∞ {webhook_id} —Å –Ω–µ—Ä–∞–∑—Ä–µ—à–µ–Ω–Ω–æ–≥–æ IP: {client_ip}")
            raise HTTPException(status_code=403, detail="IP not allowed")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è
        if webhook_info.get("auth_required") and not authorization:
            raise HTTPException(status_code=401, detail="Authorization required")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        webhook_triggers[webhook_id]["call_count"] += 1
        webhook_triggers[webhook_id]["last_called"] = datetime.now().isoformat()
        webhook_stats[webhook_id]["total_calls"] += 1
        
        # –õ–æ–≥–∏—Ä—É–µ–º –≤—ã–∑–æ–≤
        logger.info(f"üîî –í–µ–±—Ö—É–∫ {webhook_id} –≤—ã–∑–≤–∞–Ω —Å –¥–∞–Ω–Ω—ã–º–∏: {json.dumps(body, ensure_ascii=False)[:200]}...")
        
        # –ü–æ–ª—É—á–∞–µ–º workflow
        workflow_id = webhook_info["workflow_id"]
        if workflow_id not in saved_workflows:
            raise HTTPException(status_code=404, detail="Associated workflow not found")
        
        workflow_data = saved_workflows[workflow_id]
        
        # –ù–∞—Ö–æ–¥–∏–º –Ω–æ–¥—É webhook_trigger –≤ workflow
        webhook_trigger_node = None
        for node in workflow_data["nodes"]:
            if node.get("type") == "webhook_trigger":
                webhook_trigger_node = node
                break
        
        if not webhook_trigger_node:
            # –ï—Å–ª–∏ –Ω–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–π –Ω–æ–¥—ã webhook_trigger, –Ω–∞—á–∏–Ω–∞–µ–º —Å –ø–µ—Ä–≤–æ–π –¥–æ—Å—Ç—É–ø–Ω–æ–π
            logger.warning(f"‚ö†Ô∏è –í workflow {workflow_id} –Ω–µ—Ç –Ω–æ–¥—ã webhook_trigger")
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–ø—Ä–æ—Å –Ω–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ workflow
        workflow_request = WorkflowExecuteRequest(
            nodes=workflow_data["nodes"],
            connections=workflow_data["connections"],
            startNodeId=webhook_trigger_node.get("id") if webhook_trigger_node else None
        )
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º workflow —Å –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        result = await execute_workflow_internal(workflow_request, initial_input_data=body)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —É—Å–ø–µ—à–Ω—ã—Ö –≤—ã–∑–æ–≤–æ–≤
        webhook_stats[webhook_id]["successful_calls"] += 1
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 10)
        call_record = {
            "timestamp": datetime.now().isoformat(),
            "success": True,
            "input_data": body,
            "result": result.result if result.success else None,
            "error": result.error if not result.success else None
        }
        
        webhook_stats[webhook_id]["call_history"].insert(0, call_record)
        webhook_stats[webhook_id]["call_history"] = webhook_stats[webhook_id]["call_history"][:10]
        
        if result.success:
            logger.info(f"‚úÖ –í–µ–±—Ö—É–∫ {webhook_id} —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–∏–ª workflow")
            return {
                "success": True,
                "webhook_id": webhook_id,
                "workflow_id": workflow_id,
                "result": result.result,
                "execution_time": datetime.now().isoformat()
            }
        else:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è workflow –¥–ª—è –≤–µ–±—Ö—É–∫–∞ {webhook_id}: {result.error}")
            webhook_stats[webhook_id]["failed_calls"] += 1
            webhook_stats[webhook_id]["last_error"] = result.error
            raise HTTPException(status_code=500, detail=result.error)
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–µ–±—Ö—É–∫–∞ {webhook_id}: {str(e)}")
        webhook_stats[webhook_id]["failed_calls"] += 1
        webhook_stats[webhook_id]["last_error"] = str(e)
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/webhooks")
async def list_webhooks():
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –≤–µ–±—Ö—É–∫–æ–≤"""
    webhooks = []
    for webhook_id, info in webhook_triggers.items():
        base_url = "http://localhost:8000"
        webhook_url = f"{base_url}/webhooks/{webhook_id}"
        
        webhooks.append(WebhookInfo(
            webhook_id=webhook_id,
            workflow_id=info["workflow_id"],
            name=info["name"],
            description=info.get("description"),
            created_at=info["created_at"],
            url=webhook_url,
            auth_required=info.get("auth_required", False),
            allowed_ips=info.get("allowed_ips"),
            call_count=info.get("call_count", 0),
            last_called=info.get("last_called")
        ))
    
    return {"webhooks": webhooks}

@app.get("/webhooks/{webhook_id}/info")
async def get_webhook_info(webhook_id: str):
    """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º –≤–µ–±—Ö—É–∫–µ"""
    if webhook_id not in webhook_triggers:
        raise HTTPException(status_code=404, detail="Webhook not found")
    
    info = webhook_triggers[webhook_id]
    stats = webhook_stats.get(webhook_id, {})
    
    base_url = "http://localhost:8000"
    webhook_url = f"{base_url}/webhooks/{webhook_id}"
    
    return {
        "webhook": WebhookInfo(
            webhook_id=webhook_id,
            workflow_id=info["workflow_id"],
            name=info["name"],
            description=info.get("description"),
            created_at=info["created_at"],
            url=webhook_url,
            auth_required=info.get("auth_required", False),
            allowed_ips=info.get("allowed_ips"),
            call_count=info.get("call_count", 0),
            last_called=info.get("last_called")
        ),
        "statistics": {
            "total_calls": stats.get("total_calls", 0),
            "successful_calls": stats.get("successful_calls", 0),
            "failed_calls": stats.get("failed_calls", 0),
            "last_error": stats.get("last_error"),
            "recent_calls": stats.get("call_history", [])[:5]
        }
    }

@app.delete("/webhooks/{webhook_id}")
async def delete_webhook(webhook_id: str):
    """–£–¥–∞–ª–∏—Ç—å –≤–µ–±—Ö—É–∫"""
    if webhook_id not in webhook_triggers:
        raise HTTPException(status_code=404, detail="Webhook not found")
    
    del webhook_triggers[webhook_id]
    if webhook_id in webhook_stats:
        del webhook_stats[webhook_id]
    
    logger.info(f"üóëÔ∏è –í–µ–±—Ö—É–∫ {webhook_id} —É–¥–∞–ª–µ–Ω")
    
    return {"message": f"Webhook {webhook_id} deleted successfully"}

@app.put("/webhooks/{webhook_id}")
async def update_webhook(webhook_id: str, request: WebhookCreateRequest):
    """–û–±–Ω–æ–≤–∏—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≤–µ–±—Ö—É–∫–∞"""
    if webhook_id not in webhook_triggers:
        raise HTTPException(status_code=404, detail="Webhook not found")
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
    webhook_triggers[webhook_id].update({
        "name": request.name,
        "description": request.description,
        "auth_required": request.auth_required,
        "allowed_ips": request.allowed_ips or []
    })
    
    logger.info(f"üîÑ –í–µ–±—Ö—É–∫ {webhook_id} –æ–±–Ω–æ–≤–ª–µ–Ω")
    
    return {"message": "Webhook updated successfully"}

async def execute_workflow_internal(request: WorkflowExecuteRequest, initial_input_data: Optional[Dict[str, Any]] = None):
    """–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è —Ñ—É–Ω–∫—Ü–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è workflow (–±–µ–∑ HTTP –æ–±–µ—Ä—Ç–∫–∏)"""
    logs = []
    global goto_execution_counter
    goto_execution_counter.clear()

    try:
        # ======================= –ù–ê–ß–ê–õ–û –ù–û–í–û–ì–û –ë–õ–û–ö–ê =======================
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å –ª–µ–π–±–ª–æ–≤ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –∫–∞—Ä—Ç—ã 'label' -> 'id'
        # –ú—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º request.nodes, —Ç–∞–∫ –∫–∞–∫ –Ω–æ–¥—ã –ø—Ä–∏—Ö–æ–¥—è—Ç –≤–Ω—É—Ç—Ä–∏ –æ–±—ä–µ–∫—Ç–∞ WorkflowExecuteRequest
        labels = [node.data.get('label', node.id) for node in request.nodes] # –ò—Å–ø–æ–ª—å–∑—É–µ–º .get –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
        if len(labels) != len(set(labels)):
            # –ù–∞—Ö–æ–¥–∏–º –¥—É–±–ª–∏–∫–∞—Ç –¥–ª—è –±–æ–ª–µ–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
            seen = set()
            duplicates = {x for x in labels if x in seen or seen.add(x)}
            raise ValueError(f"–û—à–∏–±–∫–∞: –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è –ª–µ–π–±–ª—ã –Ω–æ–¥: {', '.join(duplicates)}. –ö–∞–∂–¥–∞—è –Ω–æ–¥–∞ –¥–æ–ª–∂–Ω–∞ –∏–º–µ—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∏–º—è (label).")

        # –°–æ–∑–¥–∞–µ–º –∫–∞—Ä—Ç—É 'label' -> 'id'
        label_to_id_map = {node.data.get('label', node.id): node.id for node in request.nodes}
        # ======================== –ö–û–ù–ï–¶ –ù–û–í–û–ì–û –ë–õ–û–ö–ê ========================

        logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ workflow —Å {len(request.nodes)} –Ω–æ–¥–∞–º–∏")
        if initial_input_data:
            logger.info(f"üí° Workflow –∑–∞–ø—É—â–µ–Ω —Å –Ω–∞—á–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏: {json.dumps(initial_input_data, default=str, indent=2)[:300]}...")

        for node in request.nodes:
            logger.info(f"üìã –ù–æ–¥–∞ {node.id} —Ç–∏–ø–∞ {node.type}: {node.data.get('label', '–ë–µ–∑ –º–µ—Ç–∫–∏')}")
        logger.info(f"üîó –°–æ–µ–¥–∏–Ω–µ–Ω–∏—è: {len(request.connections)}")
        for conn in request.connections:
            logger.info(f"üîó –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ: {conn.source} -> {conn.target}")

        join_node_data = {}

        start_node = None
        if request.startNodeId:
            start_node = next((n for n in request.nodes if n.id == request.startNodeId), None)
            logger.info(f"üéØ –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–∫–∞–∑–∞–Ω–Ω—É—é —Å—Ç–∞—Ä—Ç–æ–≤—É—é –Ω–æ–¥—É: {request.startNodeId}")
        else:
            node_ids_with_inputs = {conn.target for conn in request.connections}
            startable_types = ['gigachat', 'webhook', 'timer', 'webhook_trigger']
            start_candidates = [
                n for n in request.nodes
                if n.type in startable_types and n.id not in node_ids_with_inputs
            ]
            if start_candidates:
                start_node = start_candidates[0]
                logger.info(f"üîç –ù–∞–π–¥–µ–Ω–∞ —Å—Ç–∞—Ä—Ç–æ–≤–∞—è –Ω–æ–¥–∞ –±–µ–∑ –≤—Ö–æ–¥—è—â–∏—Ö —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π: {start_node.id}")
            else:
                start_node = next((n for n in request.nodes if n.type in startable_types), None)
                logger.info(f"‚ö†Ô∏è –í—Å–µ –Ω–æ–¥—ã –∏–º–µ—é—Ç –≤—Ö–æ–¥—è—â–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è, –±–µ—Ä–µ–º –ø–µ—Ä–≤—É—é –¥–æ—Å—Ç—É–ø–Ω—É—é: {start_node.id if start_node else 'None'}")

        if not start_node:
            raise Exception("No startable node found")

        if start_node.type == "timer":
            timer_id = f"timer_{start_node.id}"
            if timer_id in active_timers:
                active_timers[timer_id]["workflow"] = {
                    "nodes": request.nodes,
                    "connections": request.connections,
                    "startNodeId": start_node.id
                }

        # –ò–ó–ú–ï–ù–ï–ù–û: –≠—Ç–æ —Ç–µ–ø–µ—Ä—å –≥–ª–∞–≤–Ω—ã–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –¥–ª—è –í–°–ï–• —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        workflow_results = {}

        async def execute_node_recursive(node_id: str,label_to_id_map: Dict[str, str], source_node_id: str = None):
            node = next((n for n in request.nodes if n.id == node_id), None)
            if not node:
                return

            # –ò–ó–ú–ï–ù–ï–ù–û: –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–µ–∫—É—â–µ–π –Ω–æ–¥—ã
            current_input_data = {}
            if source_node_id:
                # –î–ª—è –æ–±—ã—á–Ω—ã—Ö –Ω–æ–¥ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ - —ç—Ç–æ –í–ï–°–¨ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                current_input_data = workflow_results
            elif initial_input_data:
                 # –î–ª—è —Å–∞–º–æ–π –ø–µ—Ä–≤–æ–π –Ω–æ–¥—ã - —ç—Ç–æ –Ω–∞—á–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                current_input_data = initial_input_data

            # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è webhook –Ω–æ–¥—ã
            if node.type == 'webhook_trigger' and not source_node_id:
                logger.info(f"üîî Webhook –Ω–æ–¥–∞ {node_id} –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω–∞")
                # Webhook –Ω–æ–¥–∞ –ø—Ä–æ—Å—Ç–æ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç —Å–≤–æ–π –ø–µ—Ä–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                node_result = {
                    "success": True,
                    "output": current_input_data,
                    "message": "Webhook data received"
                }
                workflow_results[node_id] = node_result # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç

                # –ü–µ—Ä–µ–¥–∞–µ–º —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–ª–µ–¥—É—é—â–∏–º –Ω–æ–¥–∞–º
                next_connections = [c for c in request.connections if c.source == node_id]
                for connection in next_connections:
                    await execute_node_recursive(connection.target, label_to_id_map, node_id)
                return

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ join –Ω–æ–¥—É
            incoming_connections = [c for c in request.connections if c.target == node_id]
            if node.type == 'join' and len(incoming_connections) > 1:
                if node_id not in join_node_data:
                    join_node_data[node_id] = {
                        'expected_sources': set(c.source for c in incoming_connections),
                        'received_data': {}
                    }
                if source_node_id:
                    # –ò–ó–ú–ï–ù–ï–ù–û: –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –ø—Ä–µ–¥—ã–¥—É—â–µ–π –Ω–æ–¥—ã
                    join_node_data[node_id]['received_data'][source_node_id] = workflow_results.get(source_node_id)
                    logger.info(f"üîÄ Join –Ω–æ–¥–∞ {node_id} –ø–æ–ª—É—á–∏–ª–∞ –¥–∞–Ω–Ω—ã–µ –æ—Ç {source_node_id}")
                    logger.info(f"üìä –û–∂–∏–¥–∞–µ—Ç—Å—è: {join_node_data[node_id]['expected_sources']}")
                    logger.info(f"üìä –ü–æ–ª—É—á–µ–Ω–æ –æ—Ç: {set(join_node_data[node_id]['received_data'].keys())}")

                received_sources = set(join_node_data[node_id]['received_data'].keys())
                expected_sources = join_node_data[node_id]['expected_sources']

                if node.data.get('config', {}).get('waitForAll', True):
                    if received_sources != expected_sources:
                        logger.info(f"‚è≥ Join –Ω–æ–¥–∞ {node_id} –∂–¥–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ—Ç {expected_sources - received_sources}")
                        return # –í—ã—Ö–æ–¥–∏–º –∏ –∂–¥–µ–º, –ø–æ–∫–∞ –¥—Ä—É–≥–∏–µ –≤–µ—Ç–∫–∏ –¥–æ–π–¥—É—Ç –¥–æ —ç—Ç–æ–π –Ω–æ–¥—ã

                # –í—Å–µ –¥–∞–Ω–Ω—ã–µ –ø–æ–ª—É—á–µ–Ω—ã, —Ñ–æ—Ä–º–∏—Ä—É–µ–º –≤—Ö–æ–¥ –¥–ª—è join –Ω–æ–¥—ã
                current_input_data = {'inputs': join_node_data[node_id]['received_data']}
                del join_node_data[node_id]

            # –õ–æ–≥–∏—Ä—É–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
            logs.append({
                "message": f"Executing {node.data.get('label', node.type)}...",
                "timestamp": datetime.now().isoformat(),
                "level": "info",
                "nodeId": node_id
            })

            # –í—ã–±–∏—Ä–∞–µ–º –∏ –≤—ã–ø–æ–ª–Ω—è–µ–º –Ω–æ–¥—É
            executor_map = {
                'gigachat': executors.execute_gigachat,
                'email': executors.execute_email,
                'database': executors.execute_database,
                'webhook': executors.execute_webhook,
                'timer': executors.execute_timer,
                'join': executors.execute_join,
                'request_iterator': executors.execute_request_iterator,
                'if_else': executors.execute_if_else,
                'dispatcher': executors.execute_dispatcher
            }

            executor = executor_map.get(node.type)
            if executor:
                # –ò–ó–ú–ï–ù–ï–ù–û: –í—ã–∑—ã–≤–∞–µ–º executor —Å –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
                node_result = await executor(node, label_to_id_map,current_input_data)

                # –ò–ó–ú–ï–ù–ï–ù–û: –°–æ—Ö—Ä–∞–Ω—è–µ–º –ò–ó–û–õ–ò–†–û–í–ê–ù–ù–´–ô —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–æ–¥—ã –≤ –æ–±—â–∏–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä
                workflow_results[node_id] = node_result

                logs.append({
                    "message": f"{node.data.get('label', node.type)} completed successfully",
                    "timestamp": datetime.now().isoformat(),
                    "level": "success",
                    "nodeId": node_id
                })

                # –ù–∞—Ö–æ–¥–∏–º —Å–ª–µ–¥—É—é—â–∏–µ –Ω–æ–¥—ã –¥–ª—è –ø–µ—Ä–µ—Ö–æ–¥–∞
                next_connections = [c for c in request.connections if c.source == node_id]

                # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è If/Else –Ω–æ–¥—ã
                if node.type == 'if_else' and 'branch' in node_result:
                    branch = node_result['branch']
                    logger.info(f"üîç If/Else –Ω–æ–¥–∞ {node_id} –≤–µ—Ä–Ω—É–ª–∞ branch: {branch}")

                    filtered_connections = []
                    goto_connections = []

                    for conn in next_connections:
                        conn_label = conn.data.get('label', '').lower() if conn.data else ''
                        if conn_label == f"{branch}:goto":
                            goto_connections.append(conn)
                        elif conn_label == branch:
                            filtered_connections.append(conn)

                    if goto_connections:
                        for goto_conn in goto_connections:
                            goto_key = f"{node_id}_to_{goto_conn.target}"
                            goto_execution_counter.setdefault(goto_key, 0)
                            goto_execution_counter[goto_key] += 1
                            max_iterations = node.data.get('config', {}).get('maxGotoIterations', 10)

                            if goto_execution_counter[goto_key] > max_iterations:
                                logger.error(f"üîÑ –ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç goto –ø–µ—Ä–µ—Ö–æ–¥–æ–≤ ({max_iterations}) –¥–ª—è {goto_key}")
                                continue

                            logger.info(f"üîÑ –ê–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω goto –ø–µ—Ä–µ—Ö–æ–¥ –∫ {goto_conn.target} (–∏—Ç–µ—Ä–∞—Ü–∏—è {goto_execution_counter[goto_key]})")
                            # –ò–ó–ú–ï–ù–ï–ù–û: –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –≤—ã–∑–æ–≤ –±–µ–∑ –ø—Ä—è–º–æ–π –ø–µ—Ä–µ–¥–∞—á–∏ –¥–∞–Ω–Ω—ã—Ö
                            await execute_node_recursive(goto_conn.target, label_to_id_map,node_id,)
                        return # –ü–æ—Å–ª–µ goto –Ω–µ –≤—ã–ø–æ–ª–Ω—è–µ–º –æ–±—ã—á–Ω—ã–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è

                    for connection in filtered_connections:
                        # –ò–ó–ú–ï–ù–ï–ù–û: –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –≤—ã–∑–æ–≤ –±–µ–∑ –ø—Ä—è–º–æ–π –ø–µ—Ä–µ–¥–∞—á–∏ –¥–∞–Ω–Ω—ã—Ö
                        await execute_node_recursive(connection.target,label_to_id_map, node_id)
                else:
                    # –î–ª—è –≤—Å–µ—Ö –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Ç–∏–ø–æ–≤ –Ω–æ–¥
                    for connection in next_connections:
                        # –ò–ó–ú–ï–ù–ï–ù–û: –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –≤—ã–∑–æ–≤ –±–µ–∑ –ø—Ä—è–º–æ–π –ø–µ—Ä–µ–¥–∞—á–∏ –¥–∞–Ω–Ω—ã—Ö
                        await execute_node_recursive(connection.target, label_to_id_map,node_id)
            else:
                raise Exception(f"Unknown node type: {node.type}")

        # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Å —Å–∞–º–æ–π –ø–µ—Ä–≤–æ–π –Ω–æ–¥—ã
        await execute_node_recursive(start_node.id,label_to_id_map)

        # –ò–ó–ú–ï–ù–ï–ù–û: –í–æ–∑–≤—Ä–∞—â–∞–µ–º –í–ï–°–¨ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏, –∞ –Ω–µ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        return ExecutionResult(
            success=True,
            result=workflow_results,
            logs=logs
        )

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è workflow: {str(e)}")
        import traceback
        logger.error(traceback.format_exc()) # –î–æ–±–∞–≤–∏–º —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫—É –¥–ª—è –ª—É—á—à–µ–π –æ—Ç–ª–∞–¥–∫–∏
        return ExecutionResult(
            success=False,
            error=str(e),
            logs=logs + [{
                "message": f"Workflow execution failed: {str(e)}",
                "timestamp": datetime.now().isoformat(),
                "level": "error"
            }]
        )


@app.post("/execute-workflow")
async def execute_workflow(request: WorkflowExecuteRequest):
    return await execute_workflow_internal(request)

# –î–æ–±–∞–≤—å—Ç–µ –Ω–æ–≤—ã–π —ç–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–æ–≤ –Ω–æ–¥
@app.post("/node-status")
async def get_node_status(node_ids: List[str]):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –Ω–æ–¥"""
    results = {}
    
    for node_id in node_ids:
        if node_id in node_execution_results:
            # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —É—Å—Ç–∞—Ä–µ–ª –ª–∏ –æ–Ω (–Ω–µ —Å—Ç–∞—Ä—à–µ 5 –º–∏–Ω—É—Ç)
            result_data = node_execution_results[node_id]
            timestamp = datetime.fromisoformat(result_data["timestamp"])
            
            if datetime.now() - timestamp < timedelta(minutes=5):
                results[node_id] = result_data
                
                # –û—á–∏—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ—Å–ª–µ –æ—Ç–ø—Ä–∞–≤–∫–∏, —á—Ç–æ–±—ã –Ω–µ –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å –µ–≥–æ –ø–æ–≤—Ç–æ—Ä–Ω–æ
                # –ø—Ä–∏ —Å–ª–µ–¥—É—é—â–µ–º –∑–∞–ø—Ä–æ—Å–µ (—ç—Ç–æ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç –ø–æ–≤—Ç–æ—Ä–Ω–æ–µ –ø–æ–¥—Å–≤–µ—á–∏–≤–∞–Ω–∏–µ –Ω–æ–¥—ã)
                del node_execution_results[node_id]
    
    return {"results": results}

# –ù–æ–≤—ã–µ —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ç–∞–π–º–µ—Ä–∞–º–∏

@app.get("/timers")
async def get_timers():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –≤—Å–µ—Ö –∞–∫—Ç–∏–≤–Ω—ã—Ö —Ç–∞–π–º–µ—Ä–æ–≤"""
    timers = []
    for timer_id, timer in active_timers.items():
        timers.append({
            "id": timer_id,
            "node_id": timer["node_id"],
            "interval": timer["interval"],
            "next_execution": timer["next_execution"].isoformat(),
            "status": timer["status"]
        })
    return {"timers": timers}

@app.get("/timers/{timer_id}")
async def get_timer(timer_id: str):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º —Ç–∞–π–º–µ—Ä–µ"""
    if timer_id not in active_timers:
        raise HTTPException(status_code=404, detail=f"Timer {timer_id} not found")
    
    timer = active_timers[timer_id]
    return {
        "id": timer_id,
        "node_id": timer["node_id"],
        "interval": timer["interval"],
        "next_execution": timer["next_execution"].isoformat(),
        "status": timer["status"]
    }

@app.post("/timers/{timer_id}/pause")
async def pause_timer(timer_id: str):
    """–ü—Ä–∏–æ—Å—Ç–∞–Ω–æ–≤–∫–∞ —Ç–∞–π–º–µ—Ä–∞"""
    if timer_id not in active_timers:
        raise HTTPException(status_code=404, detail=f"Timer {timer_id} not found")
    
    # –û—Ç–º–µ–Ω—è–µ–º –∑–∞–¥–∞—á—É —Ç–∞–π–º–µ—Ä–∞
    if active_timers[timer_id]["task"] is not None:
        active_timers[timer_id]["task"].cancel()
    
    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å
    active_timers[timer_id]["status"] = "paused"
    
    logger.info(f"‚è∏Ô∏è –¢–∞–π–º–µ—Ä {timer_id} –ø—Ä–∏–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    
    return {
        "id": timer_id,
        "status": "paused",
        "message": "Timer paused successfully"
    }

@app.post("/timers/{timer_id}/resume")
async def resume_timer(timer_id: str):
    """–í–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç–∞–π–º–µ—Ä–∞"""
    if timer_id not in active_timers:
        raise HTTPException(status_code=404, detail=f"Timer {timer_id} not found")
    
    timer = active_timers[timer_id]
    
    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –∑–∞–¥–∞—á—É —Ç–∞–π–º–µ—Ä–∞
    task = asyncio.create_task(timer_task(
        timer_id, 
        timer["node_id"], 
        timer["interval"], 
        timer["workflow"]
    ))
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–∞–π–º–µ—Ä–µ
    timer["task"] = task
    timer["status"] = "active"
    timer["next_execution"] = datetime.now() + timedelta(minutes=timer["interval"])
    
    logger.info(f"‚ñ∂Ô∏è –¢–∞–π–º–µ—Ä {timer_id} –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω")
    
    return {
        "id": timer_id,
        "status": "active",
        "next_execution": timer["next_execution"].isoformat(),
        "message": "Timer resumed successfully"
    }

@app.delete("/timers/{timer_id}")
async def delete_timer(timer_id: str):
    """–£–¥–∞–ª–µ–Ω–∏–µ —Ç–∞–π–º–µ—Ä–∞"""
    if timer_id not in active_timers:
        raise HTTPException(status_code=404, detail=f"Timer {timer_id} not found")
    
    # –û—Ç–º–µ–Ω—è–µ–º –∑–∞–¥–∞—á—É —Ç–∞–π–º–µ—Ä–∞
    if active_timers[timer_id]["task"] is not None:
        active_timers[timer_id]["task"].cancel()
    
    # –£–¥–∞–ª—è–µ–º —Ç–∞–π–º–µ—Ä –∏–∑ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
    del active_timers[timer_id]
    
    logger.info(f"üóëÔ∏è –¢–∞–π–º–µ—Ä {timer_id} —É–¥–∞–ª–µ–Ω")
    
    return {
        "message": f"Timer {timer_id} deleted successfully"
    }

# @app.post("/timers/{timer_id}/update")
# async def update_timer_endpoint(timer_id: str, interval: int):
#     """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ —Ç–∞–π–º–µ—Ä–∞"""
#     if timer_id not in active_timers:
#         raise HTTPException(status_code=404, detail=f"Timer {timer_id} not found")
    
#     result = await update_timer(timer_id, interval)
    
#     logger.info(f"üîÑ –¢–∞–π–º–µ—Ä {timer_id} –æ–±–Ω–æ–≤–ª–µ–Ω —Å –Ω–æ–≤—ã–º –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–º {interval} –º–∏–Ω—É—Ç")
    
#     return result
# –î–û–ë–ê–í–õ–Ø–ï–ú: –ù–æ–≤—ã–π —ç–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Ç–∞–π–º–µ—Ä–∞ –∏–∑ UI
@app.post("/setup-timer", status_code=status.HTTP_200_OK)
async def setup_timer(request: SetupTimerRequest):
    """
    –°–æ–∑–¥–∞–µ—Ç –∏–ª–∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç —Ñ–æ–Ω–æ–≤—É—é –∑–∞–¥–∞—á—É —Ç–∞–π–º–µ—Ä–∞.
    –≠—Ç–æ—Ç —ç–Ω–¥–ø–æ–∏–Ω—Ç –¥–æ–ª–∂–µ–Ω –≤—ã–∑—ã–≤–∞—Ç—å—Å—è –∏–∑ UI –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ workflow —Å –Ω–æ–¥–æ–π —Ç–∞–π–º–µ—Ä–∞.
    """
    try:
        node = request.node
        workflow_id = request.workflow_id
        
        if node.type != 'timer':
            raise HTTPException(status_code=400, detail="Node must be of type 'timer'")

        if workflow_id not in saved_workflows:
             raise HTTPException(status_code=404, detail=f"Workflow with ID '{workflow_id}' not found.")

        config = node.data.get('config', {})
        interval = int(config.get('interval', 5))
        timer_id = f"timer_{node.id}"

        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è, –∫–æ—Ç–æ—Ä—É—é –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ñ–æ–Ω–æ–≤–∞—è –∑–∞–¥–∞—á–∞
        workflow_info_for_task = {
            "workflow_id": workflow_id,
        }

        if timer_id not in active_timers:
            await create_timer(timer_id, node.id, interval, workflow_info_for_task)
            message = f"–¢–∞–π–º–µ—Ä {timer_id} –¥–ª—è workflow '{workflow_id}' —Å–æ–∑–¥–∞–Ω."
        else:
            await update_timer(timer_id, interval, workflow_info_for_task)
            message = f"–¢–∞–π–º–µ—Ä {timer_id} –¥–ª—è workflow '{workflow_id}' –æ–±–Ω–æ–≤–ª–µ–Ω."
        
        logger.info(f"‚úÖ {message}")
        return {"success": True, "message": message, "timer_id": timer_id}
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Ç–∞–π–º–µ—Ä–∞: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/timers/{timer_id}/execute-now")
async def execute_timer_now(timer_id: str):
    """–ù–µ–º–µ–¥–ª–µ–Ω–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Ç–∞–π–º–µ—Ä–∞"""
    if timer_id not in active_timers:
        raise HTTPException(status_code=404, detail=f"Timer {timer_id} not found")
    
    timer = active_timers[timer_id]
    
    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ workflow
    workflow_info = timer["workflow"]
    
    try:
        # –°–æ–∑–¥–∞–µ–º –∑–∞–ø—Ä–æ—Å –Ω–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ workflow
        workflow_request = WorkflowExecuteRequest(
            nodes=workflow_info["nodes"],
            connections=workflow_info["connections"],
            startNodeId=timer["node_id"]
        )
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º workflow
        result = await execute_workflow_internal(workflow_request)
        
        logger.info(f"‚úÖ –¢–∞–π–º–µ—Ä {timer_id} –≤—ã–ø–æ–ª–Ω–µ–Ω –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ")
        
        return result
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ —Ç–∞–π–º–µ—Ä–∞ {timer_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/dispatcher/{dispatcher_id}/sessions")
async def get_dispatcher_sessions(dispatcher_id: str):
    """–ü–æ–ª—É—á–∏—Ç—å –∞–∫—Ç–∏–≤–Ω—ã–µ —Å–µ—Å—Å–∏–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –¥–∏—Å–ø–µ—Ç—á–µ—Ä–∞"""
    if dispatcher_id not in executors.dispatcher_sessions:
        return {"sessions": []}
    
    sessions = executors.dispatcher_sessions[dispatcher_id]
    
    sessions_info = []
    for session_id, session_data in sessions.items():
        sessions_info.append({
            "session_id": session_id,
            "current_step": session_data.get('current_step', 0),
            "total_steps": len(session_data.get('plan', [])),
            "user_query": session_data.get('user_query', ''),
            "created_at": session_data.get('created_at', '').isoformat() if session_data.get('created_at') else '',
            "plan": session_data.get('plan', [])
        })
    
    return {"sessions": sessions_info}

@app.delete("/dispatcher/{dispatcher_id}/sessions/{session_id}")
async def delete_dispatcher_session(dispatcher_id: str, session_id: str):
    """–£–¥–∞–ª–∏—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é —Å–µ—Å—Å–∏—é –¥–∏—Å–ø–µ—Ç—á–µ—Ä–∞"""
    if dispatcher_id not in executors.dispatcher_sessions:
        raise HTTPException(status_code=404, detail="–î–∏—Å–ø–µ—Ç—á–µ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω")
    
    sessions = executors.dispatcher_sessions[dispatcher_id]
    
    if session_id not in sessions:
        raise HTTPException(status_code=404, detail="–°–µ—Å—Å–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
    
    del sessions[session_id]
    logger.info(f"üóëÔ∏è –°–µ—Å—Å–∏—è {session_id} –¥–∏—Å–ø–µ—Ç—á–µ—Ä–∞ {dispatcher_id} —É–¥–∞–ª–µ–Ω–∞")
    
    return {"message": f"–°–µ—Å—Å–∏—è {session_id} —É–¥–∞–ª–µ–Ω–∞"}


# –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∑–∞–ø—É—Å–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞
@app.on_event("startup")
async def startup_event():
    """–í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ —Å–µ—Ä–≤–µ—Ä–∞"""
    _load_workflows_from_disk() # –ù–û–í–û–ï: –ó–∞–≥—Ä—É–∂–∞–µ–º workflows
    logger.info("üöÄ –°–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω")

# –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ —Å–µ—Ä–≤–µ—Ä–∞
@app.on_event("shutdown")
async def shutdown_event():
    """–í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø—Ä–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–µ —Å–µ—Ä–≤–µ—Ä–∞"""
    logger.info("üõë –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤—Å–µ—Ö —Ç–∞–π–º–µ—Ä–æ–≤...")
    
    # –û—Ç–º–µ–Ω—è–µ–º –≤—Å–µ –∞–∫—Ç–∏–≤–Ω—ã–µ —Ç–∞–π–º–µ—Ä—ã
    for timer_id, timer in active_timers.items():
        if timer["task"] is not None:
            timer["task"].cancel()
    
    logger.info("üëã –°–µ—Ä–≤–µ—Ä –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

if __name__ == "__main__":
    import uvicorn
    print("üöÄ –ó–∞–ø—É—Å–∫ FastAPI —Å–µ—Ä–≤–µ—Ä–∞...")
    print("üì° API –±—É–¥–µ—Ç –¥–æ—Å—Ç—É–ø–µ–Ω –Ω–∞: http://localhost:8000")
    print("üìö –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: http://localhost:8000/docs")
    uvicorn.run(app, host="0.0.0.0", port=8000)

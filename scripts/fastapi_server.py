import aiohttp
from fastapi import FastAPI, HTTPException,Request, Body, Header, status
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Dict, Any, List, Optional, Tuple
import requests
import uuid
import json
import asyncio
import logging
from datetime import datetime, timedelta
import re
import hashlib
import os

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –ù–û–í–û–ï: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è workflows –∏ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –Ω–∏–º
WORKFLOWS_FILE = "saved_workflows.json"

def _save_workflows_to_disk():
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç–µ–∫—É—â–∏–µ workflows –≤ JSON —Ñ–∞–π–ª."""
    with open(WORKFLOWS_FILE, "w", encoding="utf-8") as f:
        json.dump(saved_workflows, f, ensure_ascii=False, indent=4)
    logger.info(f"üíæ Workflows —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {WORKFLOWS_FILE}")

def _load_workflows_from_disk():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç workflows –∏–∑ JSON —Ñ–∞–π–ª–∞ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ."""
    if os.path.exists(WORKFLOWS_FILE):
        with open(WORKFLOWS_FILE, "r", encoding="utf-8") as f:
            global saved_workflows
            saved_workflows = json.load(f)
            logger.info(f"‚úÖ Workflows –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ {WORKFLOWS_FILE}")

app = FastAPI(title="N8N Clone API", version="1.0.0")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # –í –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ —É–∫–∞–∑–∞—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–æ–º–µ–Ω—ã
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –•—Ä–∞–Ω–∏–ª–∏—â–µ –¥–ª—è –∞–∫—Ç–∏–≤–Ω—ã—Ö —Ç–∞–π–º–µ—Ä–æ–≤
active_timers = {}
# –î–æ–±–∞–≤—å—Ç–µ –≥–ª–æ–±–∞–ª—å–Ω—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è workflow –ø–æ—Å–ª–µ –¥—Ä—É–≥–∏—Ö –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö (–ø—Ä–∏–º–µ—Ä–Ω–æ —Å—Ç—Ä–æ–∫–∞ 30)
saved_workflows = {}
# –î–æ–±–∞–≤—å—Ç–µ —ç—Ç—É –≥–ª–æ–±–∞–ª—å–Ω—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –Ω–æ–¥
node_execution_results = {}

goto_execution_counter = {} 

# –†–∞—Å—à–∏—Ä—å—Ç–µ –≥–ª–æ–±–∞–ª—å–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ webhook_triggers (–æ–∫–æ–ª–æ —Å—Ç—Ä–æ–∫–∏ 40)
webhook_triggers: Dict[str, Dict[str, Any]] = {}
# –î–æ–±–∞–≤—å—Ç–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤–µ–±—Ö—É–∫–æ–≤
webhook_stats: Dict[str, Dict[str, Any]] = {}

# –ú–æ–¥–µ–ª–∏ –¥–∞–Ω–Ω—ã—Ö
class NodeConfig(BaseModel):
    authToken: Optional[str] = None
    systemMessage: Optional[str] = None
    userMessage: Optional[str] = None
    clearHistory: Optional[bool] = False
    to: Optional[str] = None
    subject: Optional[str] = None
    body: Optional[str] = None
    query: Optional[str] = None
    connection: Optional[str] = None
    url: Optional[str] = None
    method: Optional[str] = None
    headers: Optional[str] = None
    interval: Optional[int] = None
    timezone: Optional[str] = None
    waitForAll: Optional[bool] = True 
    mergeStrategy: Optional[str] = "combine_text"
    separator: Optional[str] = "\n\n---\n\n"
    # –î–ª—è Request Iterator –Ω–æ–¥—ã
    baseUrl: Optional[str] = None
    executionMode: Optional[str] = "sequential" # New: 'sequential' or 'parallel'
    commonHeaders: Optional[str] = None # New: JSON string for common headers
    # –î–ª—è If/Else –Ω–æ–¥—ã
    conditionType: Optional[str] = "equals"
    fieldPath: Optional[str] = "output.text"
    compareValue: Optional[str] = ""
    caseSensitive: Optional[bool] = False
    maxGotoIterations: Optional[int] = 3  # –ó–∞—â–∏—Ç–∞ –æ—Ç –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã—Ö —Ü–∏–∫–ª–æ–≤
    # –ù–û–í–û–ï: –î–ª—è Dispatcher –Ω–æ–¥—ã
    routes: Optional[Dict[str, Any]] = None
    useAI: Optional[bool] = True
    dispatcherAuthToken: Optional[str] = None # –¢–æ–∫–µ–Ω –¥–ª—è GigaChat –≤–Ω—É—Ç—Ä–∏ –¥–∏—Å–ø–µ—Ç—á–µ—Ä–∞

class Node(BaseModel):
    id: str
    type: str
    position: Dict[str, float]
    data: Dict[str, Any]

class Connection(BaseModel):
    id: str
    source: str
    target: str
    data: Optional[Dict[str, Any]] = {}  # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥–¥–µ—Ä–∂–∫—É data –¥–ª—è –º–µ—Ç–æ–∫

class WorkflowExecuteRequest(BaseModel):
    nodes: List[Node]
    connections: List[Connection]
    startNodeId: Optional[str] = None

class ExecutionResult(BaseModel):
    success: bool
    result: Optional[Dict[str, Any]] = None
    error: Optional[str] = None
    logs: List[Dict[str, Any]] = []

# –î–æ–±–∞–≤—å—Ç–µ —ç—Ç—É –º–æ–¥–µ–ª—å –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–æ–¥–µ–ª–µ–π (–ø—Ä–∏–º–µ—Ä–Ω–æ —Å—Ç—Ä–æ–∫–∞ 50-60)
class WorkflowSaveRequest(BaseModel):
    name: str
    nodes: List[Node]
    connections: List[Connection]

# –ù–û–í–û–ï: –ú–æ–¥–µ–ª—å –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è workflow
class WorkflowUpdateRequest(BaseModel):
    nodes: List[Node]
    connections: List[Connection]

# –î–æ–±–∞–≤—å—Ç–µ –Ω–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö (–æ–∫–æ–ª–æ —Å—Ç—Ä–æ–∫–∏ 80)
class WebhookCreateRequest(BaseModel):
    workflow_id: str
    name: str
    description: Optional[str] = None
    auth_required: Optional[bool] = False
    allowed_ips: Optional[List[str]] = None

class WebhookInfo(BaseModel):
    webhook_id: str
    workflow_id: str
    name: str
    description: Optional[str] = None
    created_at: str
    url: str
    auth_required: bool = False
    allowed_ips: Optional[List[str]] = None
    call_count: int = 0
    last_called: Optional[str] = None

# GigaChat API –∫–ª–∞—Å—Å
class GigaChatAPI:
    def __init__(self):
        self.access_token = None
        self.conversation_history = []
        
    async def get_token(self, auth_token: str, scope: str = 'GIGACHAT_API_PERS') -> bool:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞ –¥–æ—Å—Ç—É–ø–∞"""
        rq_uid = str(uuid.uuid4())
        url = "https://ngw.devices.sberbank.ru:9443/api/v2/oauth"
        headers = {
            'Content-Type': 'application/x-www-form-urlencoded',
            'Accept': 'application/json',
            'RqUID': rq_uid,
            'Authorization': f'Basic {auth_token}'
        }
        payload = {'scope': scope}

        try:
            # –í —Ä–µ–∞–ª—å–Ω–æ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ aiohttp –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            response = requests.post(url, headers=headers, data=payload, verify=False)
            if response.status_code == 200:
                self.access_token = response.json()['access_token']
                logger.info("‚úÖ GigaChat —Ç–æ–∫–µ–Ω –ø–æ–ª—É—á–µ–Ω —É—Å–ø–µ—à–Ω–æ")
                return True
            else:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∞: {response.status_code}")
                return False
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Ç–æ–∫–µ–Ω–∞: {str(e)}")
            return False

    async def get_chat_completion(self, system_message: str, user_message: str) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –æ—Ç GigaChat"""
        if not self.access_token:
            raise Exception("–¢–æ–∫–µ–Ω –¥–æ—Å—Ç—É–ø–∞ –Ω–µ –ø–æ–ª—É—á–µ–Ω")

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è
        messages = [{"role": "system", "content": system_message}]
        messages.extend(self.conversation_history)
        messages.append({"role": "user", "content": user_message})

        url = "https://gigachat.devices.sberbank.ru/api/v1/chat/completions"
        payload = {
            "model": "GigaChat",
            "messages": messages,
            "temperature": 1,
            "top_p": 0.1,
            "n": 1,
            "stream": False,
            "max_tokens": 512,
            "repetition_penalty": 1,
            "update_interval": 0
        }
        headers = {
            'Content-Type': 'application/json',
            'Accept': 'application/json',
            'Authorization': f'Bearer {self.access_token}'
        }

        try:
            response = requests.post(url, headers=headers, data=json.dumps(payload), verify=False)
            if response.status_code == 200:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
                self.conversation_history.append({"role": "user", "content": user_message})
                assistant_response = response.json()['choices'][0]['message']['content']
                self.conversation_history.append({"role": "assistant", "content": assistant_response})
                
                logger.info(f"‚úÖ –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –æ—Ç GigaChat")
                logger.info(f"ü§ñ –û–¢–í–ï–¢: {assistant_response}")
                return {
                    "success": True,
                    "response": assistant_response,
                    "user_message": user_message,
                    "system_message": system_message,
                    "conversation_length": len(self.conversation_history)
                }
            else:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ API GigaChat: {response.status_code}")
                return {
                    "success": False,
                    "error": f"API Error: {response.status_code}",
                    "response": None
                }
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ GigaChat: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "response": None
            }

    def clear_history(self):
        """–û—á–∏—Å—Ç–∫–∞ –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞"""
        self.conversation_history = []
        logger.info("üóëÔ∏è –ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞ –æ—á–∏—â–µ–Ω–∞")

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏ –∑–∞–ø—É—Å–∫–∞ —Ç–∞–π–º–µ—Ä–∞
async def create_timer(timer_id: str, node_id: str, interval: int, workflow_info: Dict[str, Any]):
    """–°–æ–∑–¥–∞–µ—Ç –∏ –∑–∞–ø—É—Å–∫–∞–µ—Ç –Ω–æ–≤—ã–π —Ç–∞–π–º–µ—Ä"""
    # –û—Ç–º–µ–Ω—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ç–∞–π–º–µ—Ä, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
    if timer_id in active_timers and active_timers[timer_id]["task"] is not None:
        active_timers[timer_id]["task"].cancel()
        logger.info(f"üõë –û—Ç–º–µ–Ω–µ–Ω —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ç–∞–π–º–µ—Ä {timer_id}")
    
    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —Ç–∞–π–º–µ—Ä
    next_execution = datetime.now() + timedelta(minutes=interval)
    
    # –°–æ–∑–¥–∞–µ–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º –∑–∞–¥–∞—á—É —Ç–∞–π–º–µ—Ä–∞
    task = asyncio.create_task(timer_task(timer_id, node_id, interval, workflow_info))
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–∞–π–º–µ—Ä–µ
    active_timers[timer_id] = {
        "node_id": node_id,
        "interval": interval,
        "next_execution": next_execution,
        "task": task,
        "status": "active",
        "workflow": workflow_info
    }
    
    return {
        "id": timer_id,
        "node_id": node_id,
        "interval": interval,
        "next_execution": next_execution.isoformat(),
        "status": "active"
    }

async def update_timer(timer_id: str, interval: int):
    """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ç–∞–π–º–µ—Ä"""
    if timer_id not in active_timers:
        raise Exception(f"Timer {timer_id} not found")
    
    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–º —Ç–∞–π–º–µ—Ä–µ
    timer_info = active_timers[timer_id]
    
    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —Ç–∞–π–º–µ—Ä —Å –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–º –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–º
    await create_timer(
        timer_id, 
        timer_info["node_id"], 
        interval, 
        timer_info["workflow"]
    )
    
    return {
        "id": timer_id,
        "node_id": timer_info["node_id"],
        "interval": interval,
        "next_execution": active_timers[timer_id]["next_execution"].isoformat(),
        "status": "active"
    }

async def timer_task(timer_id: str, node_id: str, interval: int, workflow_info: Dict[str, Any]):
    """–ó–∞–¥–∞—á–∞ —Ç–∞–π–º–µ—Ä–∞, –∫–æ—Ç–æ—Ä–∞—è –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ"""
    try:
        while True:
            # –ñ–¥–µ–º —É–∫–∞–∑–∞–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª
            logger.info(f"‚è∞ –ó–∞–ø—É—â–µ–Ω–∞ –∑–∞–¥–∞—á–∞ —Ç–∞–π–º–µ—Ä–∞ {timer_id} –¥–ª—è –Ω–æ–¥—ã {node_id} —Å –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–º {interval} –º–∏–Ω—É—Ç")
            logger.info(f"‚è∞ –¢–∞–π–º–µ—Ä {timer_id} –æ–∂–∏–¥–∞–µ—Ç {interval} –º–∏–Ω—É—Ç –¥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ –∑–∞–ø—É—Å–∫–∞")
            await asyncio.sleep(interval * 60)  # –ü–µ—Ä–µ–≤–æ–¥–∏–º –º–∏–Ω—É—Ç—ã –≤ —Å–µ–∫—É–Ω–¥—ã
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –≤—Ä–µ–º—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
            active_timers[timer_id]["next_execution"] = datetime.now() + timedelta(minutes=interval)

            # –ü–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è workflow
            if timer_id in active_timers and active_timers[timer_id].get("is_executing_workflow", False):
                logger.warning(f"‚ö†Ô∏è –¢–∞–π–º–µ—Ä {timer_id} —É–∂–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç workflow, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç–æ—Ç —Ü–∏–∫–ª")
                continue

            # üî¥ –î–û–ë–ê–í–ò–¢–¨: –û—Ç–º–µ—á–∞–µ–º –Ω–∞—á–∞–ª–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è workflow
            active_timers[timer_id]["is_executing_workflow"] = True
            
            try:
                # –í—ã–ø–æ–ª–Ω—è–µ–º workflow
                logger.info(f"üöÄ –¢–∞–π–º–µ—Ä {timer_id} –∑–∞–ø—É—Å–∫–∞–µ—Ç workflow")
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç—É –∂–µ –ª–æ–≥–∏–∫—É, —á—Ç–æ –∏ execute_workflow
                workflow_request = WorkflowExecuteRequest(
                    nodes=workflow_info["nodes"],
                    connections=workflow_info["connections"],
                    startNodeId=node_id  # –ù–∞—á–∏–Ω–∞–µ–º —Å –Ω–æ–¥—ã —Ç–∞–π–º–µ—Ä–∞
                )
                
                # –í—ã–ø–æ–ª–Ω—è–µ–º workflow –ø—Ä–∞–≤–∏–ª—å–Ω–æ
                start_time = datetime.now()
                result = await execute_workflow_internal(workflow_request)
                execution_time = (datetime.now() - start_time).total_seconds()
                logger.info(f"‚è±Ô∏è Workflow –≤—ã–ø–æ–ª–Ω–µ–Ω –∑–∞ {execution_time:.2f} —Å–µ–∫—É–Ω–¥")
                
                if result.success:
                    logger.info(f"‚úÖ –¢–∞–π–º–µ—Ä {timer_id} —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–∏–ª workflow")
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ UI
                    if result.result:
                        for node_id_result, node_result in result.result.items():
                            node_execution_results[node_id_result] = {
                                "result": node_result,
                                "timestamp": datetime.now().isoformat(),
                                "status": "success"
                            }
                else:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è workflow —Ç–∞–π–º–µ—Ä–æ–º {timer_id}: {result.error}")
            
                    
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ workflow —Ç–∞–π–º–µ—Ä–æ–º {timer_id}: {str(e)}")
                import traceback
                logger.error(f"üîç –¢—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞: {traceback.format_exc()}")
            finally:
                # üî¥ –î–û–ë–ê–í–ò–¢–¨: –°–Ω–∏–º–∞–µ–º —Ñ–ª–∞–≥ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
                if timer_id in active_timers:
                    active_timers[timer_id]["is_executing_workflow"] = False
                    logger.info(f"üîì –§–ª–∞–≥ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Å–Ω—è—Ç –¥–ª—è —Ç–∞–π–º–µ—Ä–∞ {timer_id}")
                else:
                    logger.warning(f"‚ö†Ô∏è –¢–∞–π–º–µ—Ä {timer_id} –±—ã–ª —É–¥–∞–ª–µ–Ω –≤–æ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è workflow")
                
    except asyncio.CancelledError:
        logger.info(f"üõë –¢–∞–π–º–µ—Ä {timer_id} –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –∑–∞–¥–∞—á–µ —Ç–∞–π–º–µ—Ä–∞ {timer_id}: {str(e)}")
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å —Ç–∞–π–º–µ—Ä–∞
        if timer_id in active_timers:
            active_timers[timer_id]["status"] = "error"



def replace_templates(text: str, data: Dict[str, Any]) -> str:
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –∑–∞–º–µ–Ω–∞ —à–∞–±–ª–æ–Ω–æ–≤ –≤–∏–¥–∞ {{path.to.value}}"""
        
        def get_nested_value(obj: Dict[str, Any], path: str) -> Any:
            """–ü–æ–ª—É—á–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ –ø—É—Ç–∏ —Ç–∏–ø–∞ 'input.output.text'"""
            keys = path.split('.')
            current = obj
            
            for key in keys:
                if isinstance(current, dict) and key in current:
                    current = current[key]
                else:
                    return f"{{{{ {path} }}}}"  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —à–∞–±–ª–æ–Ω –µ—Å–ª–∏ –ø—É—Ç—å –Ω–µ –Ω–∞–π–¥–µ–Ω
            
            return str(current)
        
        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —à–∞–±–ª–æ–Ω—ã –≤–∏–¥–∞ {{—á—Ç–æ-—Ç–æ}}
        pattern = r'\{\{([^}]+)\}\}'
        
        def replacer(match):
            path = match.group(1).strip()
            
            # –ï—Å–ª–∏ –ø—É—Ç—å –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å 'input.', —É–±–∏—Ä–∞–µ–º —ç—Ç–æ
            if path.startswith('input.'):
                path = path[6:]  # –£–±–∏—Ä–∞–µ–º 'input.'
            
            value = get_nested_value(data, path)
            logger.info(f"üîÑ –ó–∞–º–µ–Ω–∞ —à–∞–±–ª–æ–Ω–∞: {{{{{match.group(1)}}}}} -> {value}")
            return value
        
        return re.sub(pattern, replacer, text)

# Add this helper function within the NodeExecutors class or globally if preferred
# For simplicity, let's add it before the NodeExecutors class or as a static method

async def _make_single_http_request(
    session: aiohttp.ClientSession,
    method: str,
    url: str,
    params: Optional[Dict[str, Any]] = None,
    json_body: Optional[Dict[str, Any]] = None,
    headers: Optional[Dict[str, str]] = None
) -> Dict[str, Any]:
    """
    Makes a single HTTP request and returns a structured response or mock on error.
    """
    request_details = {
        "request_url": url,
        "request_method": method,
        "request_params": params if method == "GET" else None,
        "request_body": json_body if method in ["POST", "PUT", "PATCH"] else None,
        "request_headers": headers,
    }
    try:
        logger.info(f"üåç Making {method} request to {url} with params={params}, body={json_body}, headers={headers}")
        async with session.request(
            method,
            url,
            params=params if method == "GET" else None, # Query params for GET
            json=json_body if method in ["POST", "PUT", "PATCH"] else None, # JSON body for POST/PUT etc.
            headers=headers,
            timeout=aiohttp.ClientTimeout(total=10), # 10-second timeout
            ssl=False
        ) as response:
            response_data = None
            try:
                # Try to parse as JSON, but handle cases where it might not be
                if response.content_type == 'application/json':
                    response_data = await response.json()
                else:
                    response_data = await response.text()
            except (aiohttp.ContentTypeError, json.JSONDecodeError) as json_err:
                logger.warning(f"‚ö†Ô∏è Could not parse JSON response from {url}: {json_err}. Reading as text.")
                response_data = await response.text() # Fallback to text
            except Exception as e:
                logger.error(f"üö® Error reading response content from {url}: {e}")
                response_data = f"Error reading response: {e}"


            logger.info(f"‚úÖ Response from {url}: {response.status}")
            return {
                **request_details,
                "status_code": response.status,
                "response_headers": dict(response.headers),
                "response_data": response_data,
                "success": 200 <= response.status < 300,
            }
    except aiohttp.ClientConnectorError as e:
        logger.error(f"‚ùå Connection error for {url}: {e}")
        return {
            **request_details,
            "status_code": 503, # Service Unavailable
            "response_data": {"error": "Connection Error", "details": str(e)},
            "success": False,
            "mock_reason": "Connection Error",
        }
    except asyncio.TimeoutError:
        logger.error(f"‚è∞ Timeout error for {url}")
        return {
            **request_details,
            "status_code": 504, # Gateway Timeout
            "response_data": {"error": "Timeout Error", "details": "Request timed out after 10 seconds"},
            "success": False,
            "mock_reason": "Timeout Error",
        }
    except Exception as e:
        logger.error(f"üí• Unexpected error for {url}: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return {
            **request_details,
            "status_code": 500, # Internal Server Error
            "response_data": {"error": "Unexpected Error", "details": str(e)},
            "success": False,
            "mock_reason": "Unexpected Error",
        }
    
# –ò—Å–ø–æ–ª–Ω–∏—Ç–µ–ª–∏ –Ω–æ–¥
class NodeExecutors:
    def __init__(self):
        self.gigachat_api = GigaChatAPI()
    
    
    async def execute_request_iterator(self, node: Node, input_data: Dict[str, Any]) -> Dict[str, Any]:
        logger.info(f"Executing Request Iterator node: {node.id}")
        config = node.data.get('config', {})
        base_url = config.get('baseUrl', '').rstrip('/') # Ensure no trailing slash
        execution_mode = config.get('executionMode', 'sequential')
        common_headers_str = config.get('commonHeaders', '{}')

        parsed_common_headers = {}
        try:
            parsed_common_headers = json.loads(common_headers_str) if common_headers_str else {}
            if not isinstance(parsed_common_headers, dict):
                logger.warning("Common headers is not a valid JSON object, ignoring.")
                parsed_common_headers = {}
        except json.JSONDecodeError:
            logger.warning("Failed to parse common headers JSON, ignoring.")
            parsed_common_headers = {}

        requests_to_make_json_str = ""
        if input_data and 'output' in input_data and 'text' in input_data['output']:
            requests_to_make_json_str = input_data['output']['text']
        elif input_data and isinstance(input_data.get('requests_array'), list):
            requests_to_make_json_str = json.dumps(input_data.get('requests_array'))
        elif isinstance(input_data, list): # If the direct input_data is a list
             requests_to_make_json_str = json.dumps(input_data)
        elif isinstance(input_data, str): # If the direct input_data is a JSON string
             requests_to_make_json_str = input_data
        else:
            logger.error("Request Iterator: Input data must contain a JSON string or array of requests.")
            raise Exception("Request Iterator: Input data must contain a JSON string or array of requests.")

        try:
            requests_list = json.loads(requests_to_make_json_str)
            if not isinstance(requests_list, list):
                # If it's a single object, wrap it in a list
                if isinstance(requests_list, dict):
                    logger.info("Request Iterator: Received a single JSON object, wrapping it into a list.")
                    requests_list = [requests_list]
                else:
                    raise ValueError("Parsed JSON is not a list or a single request object.")
        except (json.JSONDecodeError, ValueError) as e:
            logger.error(f"Request Iterator: Failed to parse JSON input: {str(e)}")
            raise Exception(f"Request Iterator: Invalid JSON input for requests: {str(e)}")

        if not requests_list:
            logger.info("Request Iterator: No requests to process from input.")
            return {
                "success": True,
                "executed_requests_count": 0,
                "responses": [],
                "output": {"text": "[]"}
            }

        all_responses = []
        tasks = []

        # It's better to create one session for all requests from this node execution
        async with aiohttp.ClientSession() as session:
            for req_info in requests_list:
                if not isinstance(req_info, dict):
                    logger.warning(f"Skipping invalid request item (not a dict): {req_info}")
                    all_responses.append({
                        "error": "Invalid request item format",
                        "item_data": req_info,
                        "success": False
                    })
                    continue

                endpoint = req_info.get('endpoint', '')
                if not endpoint:
                    logger.warning(f"Request Iterator: Skipping request with no endpoint: {req_info}")
                    all_responses.append({
                        "error": "Missing endpoint",
                        "item_data": req_info,
                        "success": False
                    })
                    continue
                
                # Ensure endpoint starts with a slash if base_url is present, or is a full URL
                if base_url and not endpoint.startswith('/') and not endpoint.lower().startswith(('http://', 'https://')):
                    final_url = f"{base_url}/{endpoint.lstrip('/')}"
                elif not base_url and not endpoint.lower().startswith(('http://', 'https://')):
                    logger.warning(f"Request Iterator: Endpoint '{endpoint}' is relative but no baseUrl is configured. Skipping.")
                    all_responses.append({
                        "error": "Relative endpoint with no baseUrl",
                        "item_data": req_info,
                        "success": False
                    })
                    continue
                elif endpoint.lower().startswith(('http://', 'https://')):
                    final_url = endpoint # Endpoint is already a full URL
                else: # base_url is present and endpoint starts with /
                    final_url = f"{base_url}{endpoint}"


                method = req_info.get('method', 'GET').upper()
                
                # Prepare params for GET and body for POST/PUT etc.
                get_params = req_info.get('params') if method == 'GET' else None
                json_body = req_info.get('body') if method in ['POST', 'PUT', 'PATCH'] else None
                
                # Merge headers: common_headers < specific_request_headers
                specific_headers = req_info.get('headers', {})
                if not isinstance(specific_headers, dict):
                    logger.warning(f"Specific headers for {final_url} is not a dict, ignoring.")
                    specific_headers = {}

                final_headers = {**parsed_common_headers, **specific_headers}

                # Create a coroutine for the request
                task = _make_single_http_request(
                    session,
                    method,
                    final_url,
                    params=get_params,
                    json_body=json_body,
                    headers=final_headers
                )
                tasks.append(task)

            if execution_mode == 'parallel' and tasks:
                logger.info(f"Request Iterator: Executing {len(tasks)} requests in PARALLEL mode.")
                # asyncio.gather executes tasks concurrently
                # return_exceptions=True ensures that if one task fails, others continue and we get the exception
                gathered_results = await asyncio.gather(*tasks, return_exceptions=True)
                for result_or_exc in gathered_results:
                    if isinstance(result_or_exc, Exception):
                        # This case should ideally be handled within _make_single_http_request itself
                        # by returning a structured error. If it still gets here, log it.
                        logger.error(f"Request Iterator: Unhandled exception from parallel task: {result_or_exc}")
                        all_responses.append({
                            "error": "Unhandled parallel execution error",
                            "details": str(result_or_exc),
                            "success": False,
                        })
                    else:
                        all_responses.append(result_or_exc)
            elif tasks: # Sequential mode (default)
                logger.info(f"Request Iterator: Executing {len(tasks)} requests in SEQUENTIAL mode.")
                for task_coro in tasks:
                    result = await task_coro # Await each task one by one
                    all_responses.append(result)
            
        # Filter out any initial error placeholders if they were added before task creation
        final_responses_list = [r for r in all_responses if r.get("request_url") or r.get("error")]


        logger.info(f"Request Iterator: Processed {len(final_responses_list)} requests.")
        return {
            "success": True, # The iterator node itself succeeded in processing
            "executed_requests_count": len(final_responses_list),
            "responses": final_responses_list,
            "output": { # Provide the responses as text for downstream nodes
                "text": json.dumps(final_responses_list, ensure_ascii=False, indent=2),
                "json_array": final_responses_list # Also provide as a direct array
            }
        }

    async def execute_gigachat(self, node: Node, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ GigaChat –Ω–æ–¥—ã"""
        logger.info(f"Executing GigaChat node: {node.id}")
        config = node.data.get('config', {})
        auth_token = config.get('authToken')
        system_message = config.get('systemMessage', '–¢—ã –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç')
        user_message = config.get('userMessage', '')
        clear_history = config.get('clearHistory', False)

        # –î–û–ë–ê–í–õ–Ø–ï–ú –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –í–•–û–î–ù–´–• –î–ê–ù–ù–´–•
        logger.info(f"üì• –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–π –Ω–æ–¥—ã: {json.dumps(input_data, ensure_ascii=False, indent=2)[:500]}...")
        # –£–ù–ò–í–ï–†–°–ê–õ–¨–ù–ê–Ø –ó–ê–ú–ï–ù–ê –®–ê–ë–õ–û–ù–û–í
        if input_data:
            original_message = user_message
            user_message = replace_templates(user_message, input_data)
            # –¢–∞–∫–∂–µ –∑–∞–º–µ–Ω—è–µ–º —à–∞–±–ª–æ–Ω—ã –≤ system_message –µ—Å–ª–∏ –µ—Å—Ç—å
            system_message = replace_templates(system_message, input_data)
            
            if original_message != user_message:
                logger.info(f"üìù –°–æ–æ–±—â–µ–Ω–∏–µ –¥–æ –∑–∞–º–µ–Ω—ã: {original_message}")
                logger.info(f"üìù –°–æ–æ–±—â–µ–Ω–∏–µ –ø–æ—Å–ª–µ –∑–∞–º–µ–Ω—ã: {user_message}")

        if not auth_token or not user_message:
            raise Exception("GigaChat: Auth token and user message are required")

        logger.info(f"ü§ñ –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ GigaChat –Ω–æ–¥—ã: {node.id}")
        logger.info(f"üìù –í–æ–ø—Ä–æ—Å: {user_message}")
        # –û—á–∏—â–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if clear_history:
            self.gigachat_api.clear_history()

        # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–∫–µ–Ω
        if not await self.gigachat_api.get_token(auth_token):
            raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ç–æ–∫–µ–Ω –¥–æ—Å—Ç—É–ø–∞")

        # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å
        result = await self.gigachat_api.get_chat_completion(system_message, user_message)
        
        if not result.get('success'):
            raise Exception(result.get('error', 'Unknown error'))

        # --- –ö–õ–Æ–ß–ï–í–û–ï –ò–ó–ú–ï–ù–ï–ù–ò–ï ---
        # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –æ—Ç–≤–µ—Ç –∫–∞–∫ JSON, –Ω–æ –Ω–µ –ª–æ–º–∞–µ–º—Å—è, –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ –æ–Ω.
        raw_response_text = result.get('response', '')
        parsed_json = None
        try:
            # –ü–æ–ø—ã—Ç–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞
            parsed_json = json.loads(raw_response_text)
            logger.info("‚úÖ –û—Ç–≤–µ—Ç –æ—Ç GigaChat —É—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω –∫–∞–∫ JSON.")
        except json.JSONDecodeError:
            # –ï—Å–ª–∏ —ç—Ç–æ –Ω–µ JSON, –Ω–∏—á–µ–≥–æ —Å—Ç—Ä–∞—à–Ω–æ–≥–æ. –ü—Ä–æ—Å—Ç–æ –ª–æ–≥–∏—Ä—É–µ–º —ç—Ç–æ.
            logger.info("‚ÑπÔ∏è –û—Ç–≤–µ—Ç –æ—Ç GigaChat –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –≤–∞–ª–∏–¥–Ω—ã–º JSON. –ë—É–¥–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∫–∞–∫ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç.")
            pass # parsed_json –æ—Å—Ç–∞–Ω–µ—Ç—Å—è None

        # –§–æ—Ä–º–∏—Ä—É–µ–º –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–ª–µ–¥—É—é—â–∏—Ö –Ω–æ–¥
        # –¢–µ–ø–µ—Ä—å output —Å–æ–¥–µ—Ä–∂–∏—Ç –∏ text, –∏ json
        return {
            **result, # –í–∫–ª—é—á–∞–µ–º –≤—Å–µ –∏—Å—Ö–æ–¥–Ω—ã–µ –ø–æ–ª—è –∏–∑ result (success, response –∏ —Ç.–¥.)
            "output": {
                "text": raw_response_text, # –í—Å–µ–≥–¥–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç —Å—ã—Ä–æ–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç
                "json": parsed_json,      # –°–æ–¥–µ—Ä–∂–∏—Ç —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–π –æ–±—ä–µ–∫—Ç –∏–ª–∏ null
                "question": user_message,
                "length": len(raw_response_text),
                "words": len(raw_response_text.split()),
                "timestamp": datetime.now().isoformat()
            }
        }


    async def execute_email(self, node: Node, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ Email –Ω–æ–¥—ã"""
        config = node.data.get('config', {})
        to = config.get('to', '')
        subject = config.get('subject', '')
        body = config.get('body', '')

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –Ω–æ–¥ –µ—Å–ª–∏ –ø–æ–ª—è –ø—É—Å—Ç—ã–µ
        if not body and input_data and 'output' in input_data:
            body = input_data['output'].get('text', 'No content from previous node')
        
        if not subject and input_data and 'output' in input_data:
            subject = f"Response: {input_data['output'].get('question', 'Workflow Result')}"

        logger.info(f"üìß –û—Ç–ø—Ä–∞–≤–∫–∞ email –Ω–∞ {to}")
        logger.info(f"üìã –¢–µ–º–∞: {subject}")
        logger.info(f"üìÑ –¢–µ–ª–æ: {body[:100]}...")

        # –ó–¥–µ—Å—å –±—É–¥–µ—Ç —Ä–µ–∞–ª—å–Ω–∞—è –æ—Ç–ø—Ä–∞–≤–∫–∞ email (–Ω–∞–ø—Ä–∏–º–µ—Ä, —á–µ—Ä–µ–∑ SendGrid, SMTP)
        # –ü–æ–∫–∞ —Å–∏–º—É–ª–∏—Ä—É–µ–º
        await asyncio.sleep(1)

        return {
            "success": True,
            "sent": True,
            "to": to,
            "subject": subject,
            "body": body,
            "messageId": f"msg_{int(datetime.now().timestamp())}",
            "inputData": input_data
        }

    async def execute_database(self, node: Node, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ Database –Ω–æ–¥—ã"""
        config = node.data.get('config', {})
        query = config.get('query', '')
        connection = config.get('connection', 'postgres')

        # –ê–≤—Ç–æ–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–∞ –µ—Å–ª–∏ –ø—É—Å—Ç–æ–π
        if not query and input_data and 'output' in input_data:
            text_data = input_data['output'].get('text', 'No data')[:100]
            query = f"INSERT INTO responses (text, timestamp) VALUES ('{text_data}', NOW())"

        logger.info(f"üóÑÔ∏è –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ SQL –∑–∞–ø—Ä–æ—Å–∞")
        logger.info(f"üîó –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ: {connection}")
        logger.info(f"üìù –ó–∞–ø—Ä–æ—Å: {query}")

        # –ó–¥–µ—Å—å –±—É–¥–µ—Ç —Ä–µ–∞–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ SQL –∑–∞–ø—Ä–æ—Å–∞
        # –ü–æ–∫–∞ —Å–∏–º—É–ª–∏—Ä—É–µ–º
        await asyncio.sleep(1)

        return {
            "success": True,
            "rows": [
                {
                    "id": 1,
                    "text": input_data.get('output', {}).get('text', 'Sample Data')[:100],
                    "created_at": datetime.now().isoformat()
                }
            ],
            "rowCount": 1,
            "query": query,
            "connection": connection,
            "inputData": input_data
        }

    async def execute_webhook(self, node: Node, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ Webhook –Ω–æ–¥—ã - –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç HTTP –∑–∞–ø—Ä–æ—Å"""
        config = node.data.get('config', {})
        url = config.get('url', '')
        method = config.get('method', 'POST').upper()
        headers_str = config.get('headers', 'Content-Type: application/json')
        
        # –ó–∞–º–µ–Ω—è–µ–º —à–∞–±–ª–æ–Ω—ã –≤ URL (–Ω–∞–ø—Ä–∏–º–µ—Ä, https://api.com/user/{{input.output.user_id}})
        if input_data:
            url = replace_templates(url, input_data)

        if not url:
            raise Exception("Webhook URL is required")
        
        # –ü–∞—Ä—Å–∏–º –∑–∞–≥–æ–ª–æ–≤–∫–∏
        headers = {}
        if headers_str:
            for line in headers_str.strip().split('\n'):
                if ':' in line:
                    key, value = line.split(':', 1)
                    headers[key.strip()] = value.strip()
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏
        # –ï—Å–ª–∏ –µ—Å—Ç—å output.text –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–π –Ω–æ–¥—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
        payload = input_data
        if input_data and 'output' in input_data:
            # –ú–æ–∂–Ω–æ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –ª–∏–±–æ –≤–µ—Å—å output, –ª–∏–±–æ —Ç–æ–ª—å–∫–æ text
            payload = input_data['output']
        
        logger.info(f"üåê –û—Ç–ø—Ä–∞–≤–∫–∞ {method} –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞ {url}")
        if method in ['POST', 'PUT', 'PATCH'] and payload:
            logger.info(f"üì¶ Payload: {json.dumps(payload, ensure_ascii=False)[:200]}...")
        
        try:
            # –†–µ–∞–ª—å–Ω–∞—è –æ—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞
            async with aiohttp.ClientSession() as session:
                # ‚¨áÔ∏è –ì–õ–ê–í–ù–û–ï –ò–ó–ú–ï–ù–ï–ù–ò–ï: –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
                request_params = {
                    'method': method,
                    'url': url,
                    'headers': headers,
                    'timeout': aiohttp.ClientTimeout(total=30),
                    'ssl': False
                }
            
                # –î–æ–±–∞–≤–ª—è–µ–º body —Ç–æ–ª—å–∫–æ –¥–ª—è –º–µ—Ç–æ–¥–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –µ–≥–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç
                if method in ['POST', 'PUT', 'PATCH'] and payload:
                    request_params['json'] = payload
                
                # –î–ª—è GET –∑–∞–ø—Ä–æ—Å–æ–≤ –ù–ï –¥–æ–±–∞–≤–ª—è–µ–º params –∏–∑ payload
                # (–µ—Å–ª–∏ –Ω—É–∂–Ω—ã query –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –æ–Ω–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ URL)
            
                async with session.request(**request_params) as response:
                    response_text = await response.text()
                    response_json = None
                    
                    try:
                        response_json = await response.json()
                    except:
                        pass  # –ù–µ –≤—Å–µ –æ—Ç–≤–µ—Ç—ã –≤ JSON
                    
                    logger.info(f"‚úÖ Webhook –æ—Ç–≤–µ—Ç: {response.status}")
                    
                    # ‚¨áÔ∏è –î–û–ë–ê–í–õ–ï–ù–û: –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                    if response_json and isinstance(response_json, list):
                        logger.info(f"üìä –ü–æ–ª—É—á–µ–Ω–æ {len(response_json)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
                    
                    return {
                        "success": response.status < 400,
                        "status_code": response.status,
                        "response": response_text,
                        "response_json": response_json,
                        "url": url,
                        "method": method,
                        "timestamp": datetime.now().isoformat(),
                        "output": {
                            "text": response_text,
                            "status": response.status,
                            "json": response_json
                        }
                    }

                    
        except aiohttp.ClientError as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ webhook: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "error_type": "connection_error",
                "url": url,
                "method": method,
                "timestamp": datetime.now().isoformat(),
                "output": {
                    "text": f"Error: {str(e)}",
                    "status": 0
                }
            }
        except Exception as e:
            logger.error(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ webhook: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "error_type": "unexpected_error",
                "url": url,
                "method": method,
                "timestamp": datetime.now().isoformat()
            }


    async def execute_timer(self, node: Node, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ Timer –Ω–æ–¥—ã"""
        try:
            config = node.data.get('config', {})
            
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ç–µ—Ä–≤–∞–ª —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
            try:
                interval = int(config.get('interval', 5))
            except ValueError:
                logger.warning(f"‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞: {config.get('interval')}. –ò—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 5")
                interval = 5
                
            timezone = config.get('timezone', 'UTC')

            logger.info(f"‚è∞ Timer –Ω–æ–¥–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞: –∏–Ω—Ç–µ—Ä–≤–∞–ª {interval} –º–∏–Ω—É—Ç, —á–∞—Å–æ–≤–æ–π –ø–æ—è—Å {timezone}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —É–∂–µ —Ç–∞–π–º–µ—Ä –¥–ª—è —ç—Ç–æ–π –Ω–æ–¥—ã
            timer_id = f"timer_{node.id}"
            
            # üî¥ –î–û–ë–ê–í–ò–¢–¨ –ó–î–ï–°–¨ –ü–†–û–í–ï–†–ö–£
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –≤—ã–ø–æ–ª–Ω—è–µ—Ç –ª–∏ —Ç–∞–π–º–µ—Ä —Å–µ–π—á–∞—Å workflow
            if timer_id in active_timers:
                current_timer = active_timers[timer_id]
                if current_timer.get("is_executing_workflow", False):
                    logger.info(f"‚è≥ Timer {timer_id} —Å–µ–π—á–∞—Å –≤—ã–ø–æ–ª–Ω—è–µ—Ç workflow, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ")
                    
                    # –§–æ—Ä–º–∏—Ä—É–µ–º –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –±–µ–∑ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏—è —Ç–∞–π–º–µ—Ä–∞
                    current_time = datetime.now()
                    next_execution = current_timer.get("next_execution", current_time + timedelta(minutes=interval))
                    
                    return {
                        "success": True,
                        "message": f"Timer is currently executing workflow",
                        "interval": interval,
                        "timezone": timezone,
                        "timestamp": current_time.isoformat(),
                        "next_execution": next_execution.isoformat() if isinstance(next_execution, datetime) else next_execution,
                        "timer_id": timer_id,
                        "output": {
                            "text": f"Timer triggered at {current_time.isoformat()}. Timer is currently busy.",
                            "timestamp": current_time.isoformat(),
                            "interval": interval,
                            "timezone": timezone
                        }
                    }
            # üî¥ –ö–û–ù–ï–¶ –î–û–ë–ê–í–õ–ï–ù–ù–û–ì–û –ë–õ–û–ö–ê

            # –ü–æ–ª—É—á–∞–µ–º ID —Ç–µ–∫—É—â–µ–≥–æ workflow –∏–∑ –∏–º–µ–Ω–∏
            workflow_id = None
            for wf_id, wf_data in saved_workflows.items():
                if any(n['id'] == node.id for n in wf_data["nodes"]):
                    workflow_id = wf_id
                    break
            
            if not workflow_id:
                logger.warning(f"‚ö†Ô∏è Workflow –¥–ª—è –Ω–æ–¥—ã {node.id} –Ω–µ –Ω–∞–π–¥–µ–Ω. –¢–∞–π–º–µ—Ä –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ.")
            
            # –ï—Å–ª–∏ —ç—Ç–æ –ø–µ—Ä–≤–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —Ç–∞–π–º–µ—Ä
            if timer_id not in active_timers:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π workflow –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π
                workflow_info = None
                if workflow_id and workflow_id in saved_workflows:
                    workflow_info = {
                        "nodes": saved_workflows[workflow_id]["nodes"],
                        "connections": saved_workflows[workflow_id]["connections"],
                        "startNodeId": node.id
                    }
                else:
                    # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π workflow —Å —Ç–µ–∫—É—â–µ–π –Ω–æ–¥–æ–π
                    workflow_info = {
                        "nodes": [node],
                        "connections": [],
                        "startNodeId": node.id
                    }
                
                # –°–æ–∑–¥–∞–µ–º —Ç–∞–π–º–µ—Ä
                await create_timer(timer_id, node.id, interval, workflow_info)
                
                logger.info(f"üïí –°–æ–∑–¥–∞–Ω –Ω–æ–≤—ã–π —Ç–∞–π–º–µ—Ä {timer_id} —Å –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–º {interval} –º–∏–Ω—É—Ç")
            else:
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ç–∞–π–º–µ—Ä
                await update_timer(timer_id, interval)
                logger.info(f"üïí –û–±–Ω–æ–≤–ª–µ–Ω —Ç–∞–π–º–µ—Ä {timer_id} —Å –Ω–æ–≤—ã–º –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–º {interval} –º–∏–Ω—É—Ç")
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–ª–µ–¥—É—é—â–∏—Ö –Ω–æ–¥
            current_time = datetime.now()
            next_execution = current_time + timedelta(minutes=interval)
            
            return {
                "success": True,
                "message": f"Timer triggered at {current_time.isoformat()}",
                "interval": interval,
                "timezone": timezone,
                "timestamp": current_time.isoformat(),
                "next_execution": next_execution.isoformat(),
                "timer_id": timer_id,
                "output": {
                    "text": f"Timer triggered at {current_time.isoformat()}. Next execution at {next_execution.isoformat()}",
                    "timestamp": current_time.isoformat(),
                    "interval": interval,
                    "timezone": timezone
                }
            }
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è Timer –Ω–æ–¥—ã: {str(e)}")
            raise Exception(f"Timer execution failed: {str(e)}")
    
    async def execute_join(self, node: Node, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ Join/Merge –Ω–æ–¥—ã"""
        config = node.data.get('config', {})
        wait_for_all = config.get('waitForAll', True)
        merge_strategy = config.get('mergeStrategy', 'combine_text')
        separator = config.get('separator', '\n\n---\n\n').replace('\\n', '\n')
        
        logger.info(f"üîÄ –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ Join/Merge –Ω–æ–¥—ã: {node.id}")
        logger.info(f"üì• –°—Ç—Ä–∞—Ç–µ–≥–∏—è: {merge_strategy}")
        
        # input_data –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å–ª–æ–≤–∞—Ä—å inputs —Å –¥–∞–Ω–Ω—ã–º–∏ –æ—Ç –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        inputs = input_data.get('inputs', {})
        
        if not inputs:
            raise Exception("Join node requires input data from at least one source")
        
        logger.info(f"üìä –ü–æ–ª—É—á–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –æ—Ç {len(inputs)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {list(inputs.keys())}")
        
        result = {}
        
        if merge_strategy == 'combine_text':
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Ç–µ–∫—Å—Ç—ã
            texts = []
            for i, (source_id, data) in enumerate(inputs.items()):
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ —Ä–∞–∑–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω—ã—Ö –º–µ—Å—Ç
                text = ""
                if isinstance(data, dict):
                    if 'output' in data and 'text' in data['output']:
                        text = data['output']['text']
                    elif 'response' in data:
                        text = data['response']
                    elif 'text' in data:
                        text = data['text']
                    else:
                        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Ç–µ–∫—Å—Ç, –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å—Ç—Ä–æ–∫—É
                        text = json.dumps(data, ensure_ascii=False, indent=2)
                else:
                    text = str(data)
                
                texts.append(f"=== –ò—Å—Ç–æ—á–Ω–∏–∫ {i+1} ({source_id}) ===\n{text}")
            
            combined_text = separator.join(texts)
            
            result = {
                'output': {
                    'text': combined_text,
                    'source_count': len(inputs),
                    'sources': list(inputs.keys())
                },
                'success': True
            }
            
            logger.info(f"‚úÖ –û–±—ä–µ–¥–∏–Ω–µ–Ω–æ {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤")
            
        elif merge_strategy == 'merge_json':
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –≤ –µ–¥–∏–Ω—ã–π JSON
            merged_data = {
                'sources': {},
                'metadata': {
                    'source_count': len(inputs),
                    'merge_time': datetime.now().isoformat(),
                    'source_ids': list(inputs.keys())
                }
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –æ—Ç –∫–∞–∂–¥–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
            for source_id, data in inputs.items():
                merged_data['sources'][source_id] = data
            
            result = {
                'output': {
                    'text': json.dumps(merged_data, ensure_ascii=False, indent=2),
                    'json': merged_data,
                    'source_count': len(inputs),
                    'sources': list(inputs.keys())
                },
                'success': True
            }
            
            logger.info(f"‚úÖ –û–±—ä–µ–¥–∏–Ω–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –≤ JSON –æ—Ç {len(inputs)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
        
        else:
            raise Exception(f"Unknown merge strategy: {merge_strategy}")
        
        return result

    async def execute_if_else(self, node: Node, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ If/Else –Ω–æ–¥—ã"""
        config = node.data.get('config', {})
        condition_type = config.get('conditionType', 'equals')
        field_path = config.get('fieldPath', 'output.text')
        compare_value = config.get('compareValue', '')
        case_sensitive = config.get('caseSensitive', False)
        
        logger.info(f"üîÄ –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ If/Else –Ω–æ–¥—ã: {node.id}")
        logger.info(f"üìã –£—Å–ª–æ–≤–∏–µ: {field_path} {condition_type} {compare_value}")
        
        # –ü–æ–ª—É—á–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ –ø—É—Ç–∏
        def get_value_by_path(data: Dict[str, Any], path: str) -> Any:
            keys = path.split('.')
            current = data
            
            for key in keys:
                if isinstance(current, dict) and key in current:
                    current = current[key]
                elif isinstance(current, list) and key.isdigit():
                    index = int(key)
                    if 0 <= index < len(current):
                        current = current[index]
                    else:
                        return None
                else:
                    return None
            
            return current
        
        actual_value = get_value_by_path(input_data, field_path)
        
        if actual_value is None and condition_type not in ['exists', 'is_empty']:
            logger.warning(f"‚ö†Ô∏è –ü–æ–ª–µ {field_path} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –¥–∞–Ω–Ω—ã—Ö")
            actual_value = ""
        
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ —Å—Ç—Ä–æ–∫–µ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è (–µ—Å–ª–∏ –Ω–µ —á–∏—Å–ª–æ)
        if condition_type not in ['greater', 'greater_equal', 'less', 'less_equal']:
            actual_value_str = str(actual_value) if actual_value is not None else ""
            compare_value_str = str(compare_value)
            
            if not case_sensitive:
                actual_value_str = actual_value_str.lower()
                compare_value_str = compare_value_str.lower()
        else:
            # –î–ª—è —á–∏—Å–ª–æ–≤—ã—Ö —Å—Ä–∞–≤–Ω–µ–Ω–∏–π
            try:
                actual_value_str = float(actual_value)
                compare_value_str = float(compare_value)
            except (ValueError, TypeError):
                actual_value_str = 0
                compare_value_str = 0
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Å–ª–æ–≤–∏–µ
        result = False
        
        if condition_type == 'equals':
            result = actual_value_str == compare_value_str
        elif condition_type == 'not_equals':
            result = actual_value_str != compare_value_str
        elif condition_type == 'contains':
            result = compare_value_str in actual_value_str
        elif condition_type == 'not_contains':
            result = compare_value_str not in actual_value_str
        elif condition_type == 'greater':
            result = actual_value_str > compare_value_str
        elif condition_type == 'greater_equal':
            result = actual_value_str >= compare_value_str
        elif condition_type == 'less':
            result = actual_value_str < compare_value_str
        elif condition_type == 'less_equal':
            result = actual_value_str <= compare_value_str
        elif condition_type == 'regex':
            import re
            try:
                result = bool(re.search(compare_value, str(actual_value)))
            except re.error:
                result = False
        elif condition_type == 'exists':
            result = actual_value is not None
        elif condition_type == 'is_empty':
            result = actual_value is None or str(actual_value).strip() == ""
        elif condition_type == 'is_not_empty':
            result = actual_value is not None and str(actual_value).strip() != ""
        
        branch = 'true' if result else 'false'
        
        logger.info(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏: {result} (–≤–µ—Ç–∫–∞: {branch})")
        logger.info(f"üìç –§–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: {actual_value}")
        logger.info(f"üìç –û–∂–∏–¥–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: {compare_value}")
        
        # --- –ö–õ–Æ–ß–ï–í–û–ï –ò–ó–ú–ï–ù–ï–ù–ò–ï ---
        # –ú—ã –Ω–µ —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π output, –∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ,
        # –¥–æ–±–∞–≤–ª—è—è –∫ –Ω–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–ª—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏.
        # –≠—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –Ω–æ–¥ –Ω–µ –±—É–¥—É—Ç –ø–æ—Ç–µ—Ä—è–Ω—ã.
        return {
            **input_data,  # <--- –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –≤—Å–µ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–∞–ª—å—à–µ
            'success': True,
            'branch': branch, # <--- –≠—Ç–æ –ø–æ–ª–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –≤—ã–±–æ—Ä–∞ —Å–ª–µ–¥—É—é—â–µ–π –Ω–æ–¥—ã
            'if_else_result': { # <--- –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–π –±–ª–æ–∫ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ If/Else –¥–ª—è —è—Å–Ω–æ—Å—Ç–∏
                'condition_met': result,
                'checked_value': str(actual_value),
                'condition': f"{field_path} {condition_type} {compare_value}",
            }
        }

    # –ù–û–í–û–ï: –õ–æ–≥–∏–∫–∞ –¥–ª—è –Ω–æ–¥—ã-–¥–∏—Å–ø–µ—Ç—á–µ—Ä–∞
    async def execute_dispatcher(self, node: Node, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """–ê–≥–µ–Ω—Ç-–¥–∏—Å–ø–µ—Ç—á–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å –∏ –≤—ã–±–∏—Ä–∞–µ—Ç –Ω—É–∂–Ω—ã–π workflow"""
        logger.info(f"Executing Dispatcher node: {node.id}")
        config = node.data.get('config', {})
        
        # –ü–æ–ª—É—á–∞–µ–º –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏–∑ —Ä–∞–∑–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        user_query = ""
        if input_data and 'output' in input_data and 'text' in input_data['output']:
            user_query = input_data['output']['text']
        elif input_data and 'user_query' in input_data:
            user_query = input_data['user_query']
        elif isinstance(input_data, dict): # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –∫–ª—é—á —Å –∑–∞–ø—Ä–æ—Å–æ–º
            user_query = input_data.get('query', input_data.get('text', ''))

        if not user_query:
            raise Exception("Dispatcher: User query not found in input data.")

        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–∞—Ä—à—Ä—É—Ç–æ–≤
        workflow_routes = config.get('routes', {})
        if not workflow_routes:
            raise Exception("Dispatcher: Routes are not configured.")

        category = 'default'
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º GigaChat –¥–ª—è —É–º–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        if config.get('useAI', True):
            auth_token = config.get('dispatcherAuthToken')
            if not auth_token:
                raise Exception("Dispatcher: GigaChat auth token is required for AI mode.")

            classification_prompt = f"""–û–ø—Ä–µ–¥–µ–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –≤—ã–±–µ—Ä–∏ –ø–æ–¥—Ö–æ–¥—è—â–∏–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫.
–î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {json.dumps(list(workflow_routes.keys()), ensure_ascii=False)}
–ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {user_query}
–û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û –æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º - –Ω–∞–∑–≤–∞–Ω–∏–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏."""
            
            # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–∫–µ–Ω –∏ –¥–µ–ª–∞–µ–º –∑–∞–ø—Ä–æ—Å
            if await self.gigachat_api.get_token(auth_token):
                gigachat_result = await self.gigachat_api.get_chat_completion(
                    "–¢—ã - –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∑–∞–ø—Ä–æ—Å–æ–≤. –û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º - –Ω–∞–∑–≤–∞–Ω–∏–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏.",
                    classification_prompt
                )
                response_text = gigachat_result.get('response', 'default').strip().lower()
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ GigaChat –≤–µ—Ä–Ω—É–ª –≤–∞–ª–∏–¥–Ω—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é
                if response_text in workflow_routes:
                    category = response_text
                else:
                    logger.warning(f"GigaChat returned an unknown category '{response_text}', using default.")
            else:
                logger.error("Dispatcher: Failed to get GigaChat token.")

        else:
            # –ü—Ä–æ—Å—Ç–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
            query_lower = user_query.lower()
            for cat_name, cat_info in workflow_routes.items():
                if cat_name != 'default' and 'keywords' in cat_info:
                    if any(keyword.lower() in query_lower for keyword in cat_info['keywords']):
                        category = cat_name
                        break
        
        selected_route = workflow_routes.get(category, workflow_routes.get('default'))
        if not selected_route:
            raise Exception(f"Dispatcher: No route found for category '{category}' and no default route is set.")

        workflow_id = selected_route['workflow_id']
        logger.info(f"üéØ –î–∏—Å–ø–µ—Ç—á–µ—Ä –≤—ã–±—Ä–∞–ª –∫–∞—Ç–µ–≥–æ—Ä–∏—é: {category} -> –ó–∞–ø—É—Å–∫ workflow: {workflow_id}")

        if workflow_id not in saved_workflows:
            raise Exception(f"Dispatcher: Target workflow '{workflow_id}' not found.")

        # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—ã–±—Ä–∞–Ω–Ω—ã–π workflow
        workflow_data = saved_workflows[workflow_id]
        workflow_request = WorkflowExecuteRequest(
            nodes=workflow_data["nodes"],
            connections=workflow_data["connections"]
        )
        
        # –ü–µ—Ä–µ–¥–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –≤—ã–±—Ä–∞–Ω–Ω—ã–π workflow
        sub_workflow_result = await execute_workflow_internal(
            workflow_request, 
            initial_input_data={
                **input_data,
                "dispatcher_info": {
                    "category": category,
                    "original_query": user_query,
                    "selected_workflow": workflow_id
                }
            }
        )

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ä–∞–±–æ—Ç—ã –ø–æ–¥-–ø—Ä–æ—Ü–µ—Å—Å–∞
        return {
            **sub_workflow_result.result,
            "success": sub_workflow_result.success,
            "dispatcher_category": category,
            "executed_workflow_id": workflow_id,
            "output": {
                "text": f"–†–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ç workflow '{workflow_id}': {sub_workflow_result.result}",
                "json": sub_workflow_result.result
            }
        }
    
# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª–µ–π
executors = NodeExecutors()

# API —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã
@app.get("/")
async def root():
    return {"message": "N8N Clone API Server", "version": "1.0.0"}

@app.get("/health")
async def health_check():
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}

@app.post("/execute-node")
async def execute_node(
    node_type: str,
    node_data: Dict[str, Any],
    input_data: Dict[str, Any] = None
):
    """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω–æ–π –Ω–æ–¥—ã"""
    try:
        logger.info(f"Received request for node_type: {node_type}")
        logger.info(f"node_data: {json.dumps(node_data, indent=2)}")
        # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç –Ω–æ–¥—ã
        node = Node(
            id=node_data.get('id', 'temp'),
            type=node_type,
            position=node_data.get('position', {'x': 0, 'y': 0}),
            data=node_data.get('data', {})
        )
        logger.info(f"Created node: {node}")
        logger.info(f"Node data: {node.data}")
        logger.info(f"Config: {node.data.get('config', {})}")

        # –í—ã–±–∏—Ä–∞–µ–º –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å
        executor_map = {
            'gigachat': executors.execute_gigachat,
            'email': executors.execute_email,
            'database': executors.execute_database,
            'webhook': executors.execute_webhook,
            'timer': executors.execute_timer,
            'join': executors.execute_join,
            'request_iterator': executors.execute_request_iterator,
            'if_else': executors.execute_if_else,  # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–π –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å
            'dispatcher': executors.execute_dispatcher # –ù–û–í–û–ï
            
        }

        executor = executor_map.get(node_type)
        if not executor:
            raise HTTPException(status_code=400, detail=f"Unknown node type: {node_type}")

        # –í—ã–ø–æ–ª–Ω—è–µ–º –Ω–æ–¥—É
        result = await executor(node, input_data or {})
        
        return ExecutionResult(
            success=True,
            result=result,
            logs=[{
                "message": f"Node {node_type} executed successfully",
                "timestamp": datetime.now().isoformat(),
                "level": "info"
            }]
        )

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –Ω–æ–¥—ã {node_type}: {str(e)}")
        return ExecutionResult(
            success=False,
            error=str(e),
            logs=[{
                "message": f"Error executing node {node_type}: {str(e)}",
                "timestamp": datetime.now().isoformat(),
                "level": "error"
            }]
        )

# --- –ù–û–í–û–ï: CRUD –≠–Ω–¥–ø–æ–∏–Ω—Ç—ã –¥–ª—è Workflows ---

@app.get("/workflows")
async def list_workflows():
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö workflows."""
    return {
        "workflows": [
            {"id": wf_id, "name": wf_data.get("name", wf_id)}
            for wf_id, wf_data in saved_workflows.items()
        ]
    }

@app.get("/workflows/{workflow_id}")
async def get_workflow(workflow_id: str):
    """–ü–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ workflow –ø–æ –µ–≥–æ ID."""
    if workflow_id not in saved_workflows:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Workflow not found")
    return saved_workflows[workflow_id]

@app.post("/workflows", status_code=status.HTTP_201_CREATED)
async def create_workflow(request: WorkflowSaveRequest):
    """–°–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—ã–π workflow."""
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º ID –∏–∑ –∏–º–µ–Ω–∏, –¥–µ–ª–∞—è –µ–≥–æ –±–µ–∑–æ–ø–∞—Å–Ω—ã–º –¥–ª—è URL
    workflow_id = re.sub(r'[^a-z0-9_]+', '', request.name.lower().replace(" ", "_"))
    if not workflow_id:
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="Invalid workflow name, results in empty ID.")
    if workflow_id in saved_workflows:
        raise HTTPException(status_code=status.HTTP_409_CONFLICT, detail=f"Workflow with ID '{workflow_id}' already exists.")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º workflow –≤ –ø–∞–º—è—Ç–∏
    saved_workflows[workflow_id] = {
        "name": request.name,
        "nodes": [node.dict() for node in request.nodes],
        "connections": [conn.dict() for conn in request.connections],
        "created_at": datetime.now().isoformat(),
        "updated_at": datetime.now().isoformat()
    }
    _save_workflows_to_disk() # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞ –¥–∏—Å–∫
    logger.info(f"‚úÖ Workflow '{request.name}' (ID: {workflow_id}) —Å–æ–∑–¥–∞–Ω.")
    return {"success": True, "workflow_id": workflow_id, "name": request.name}

@app.put("/workflows/{workflow_id}")
async def update_workflow(workflow_id: str, request: WorkflowUpdateRequest):
    """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π workflow."""
    if workflow_id not in saved_workflows:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Workflow not found")
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
    saved_workflows[workflow_id]["nodes"] = [node.dict() for node in request.nodes]
    saved_workflows[workflow_id]["connections"] = [conn.dict() for conn in request.connections]
    saved_workflows[workflow_id]["updated_at"] = datetime.now().isoformat()
    _save_workflows_to_disk() # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞ –¥–∏—Å–∫
    logger.info(f"üîÑ Workflow '{workflow_id}' –æ–±–Ω–æ–≤–ª–µ–Ω.")
    return {"success": True, "message": f"Workflow '{workflow_id}' updated successfully."}

@app.delete("/workflows/{workflow_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_workflow(workflow_id: str):
    """–£–¥–∞–ª—è–µ—Ç workflow."""
    if workflow_id not in saved_workflows:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Workflow not found")
    
    del saved_workflows[workflow_id]
    _save_workflows_to_disk() # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞ –¥–∏—Å–∫
    logger.info(f"üóëÔ∏è Workflow '{workflow_id}' —É–¥–∞–ª–µ–Ω.")
# –î–æ–±–∞–≤—å—Ç–µ —ç—Ç–æ—Ç —ç–Ω–¥–ø–æ–∏–Ω—Ç –ø–æ—Å–ª–µ –¥—Ä—É–≥–∏—Ö —ç–Ω–¥–ø–æ–∏–Ω—Ç–æ–≤ (–ø—Ä–∏–º–µ—Ä–Ω–æ —Å—Ç—Ä–æ–∫–∞ 600-650)
# @app.post("/save-workflow")
# async def save_workflow(request: WorkflowSaveRequest):
#     """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç workflow –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ"""
#     try:
#         workflow_id = request.name.lower().replace(" ", "_")
        
#         # –°–æ—Ö—Ä–∞–Ω—è–µ–º workflow –≤ –ø–∞–º—è—Ç–∏
#         saved_workflows[workflow_id] = {
#             "name": request.name,
#             "nodes": request.nodes,
#             "connections": request.connections,
#             "updated_at": datetime.now().isoformat()
#         }
        
#         # –ú–æ–∂–Ω–æ —Ç–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –Ω–∞ –¥–∏—Å–∫ –¥–ª—è –ø–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
#         # with open(f"workflows/{workflow_id}.json", "w") as f:
#         #     json.dump(saved_workflows[workflow_id], f)
        
#         logger.info(f"‚úÖ Workflow '{request.name}' —Å–æ—Ö—Ä–∞–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ")
        
#         return {
#             "success": True,
#             "workflow_id": workflow_id,
#             "message": f"Workflow '{request.name}' saved successfully"
#         }
#     except Exception as e:
#         logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è workflow: {str(e)}")
#         return {
#             "success": False,
#             "error": str(e)
#         }

@app.post("/execute-workflow/{workflow_id}")
async def execute_saved_workflow(workflow_id: str, initial_input_data: Optional[Dict[str, Any]] = Body(None)):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π workflow –ø–æ –µ–≥–æ ID."""
    if workflow_id not in saved_workflows:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Workflow not found")

    workflow_data = saved_workflows[workflow_id]
    
    workflow_request = WorkflowExecuteRequest(
        nodes=workflow_data["nodes"],
        connections=workflow_data["connections"]
    )

    return await execute_workflow_internal(workflow_request, initial_input_data=initial_input_data)


# –î–æ–±–∞–≤—å—Ç–µ –Ω–æ–≤—ã–µ —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã –ø–æ—Å–ª–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö

@app.post("/webhooks/create")
async def create_webhook(request: WebhookCreateRequest):
    """–°–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—ã–π –≤–µ–±—Ö—É–∫ –¥–ª—è workflow"""
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ workflow
        if request.workflow_id not in saved_workflows:
            raise HTTPException(status_code=404, detail=f"Workflow {request.workflow_id} not found")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –¥–ª—è –≤–µ–±—Ö—É–∫–∞
        webhook_id = str(uuid.uuid4())
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤–µ–±—Ö—É–∫–µ
        webhook_info = {
            "webhook_id": webhook_id,
            "workflow_id": request.workflow_id,
            "name": request.name,
            "description": request.description,
            "created_at": datetime.now().isoformat(),
            "auth_required": request.auth_required,
            "allowed_ips": request.allowed_ips or [],
            "call_count": 0,
            "last_called": None
        }
        
        webhook_triggers[webhook_id] = webhook_info
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        webhook_stats[webhook_id] = {
            "total_calls": 0,
            "successful_calls": 0,
            "failed_calls": 0,
            "last_error": None,
            "call_history": []  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 10 –≤—ã–∑–æ–≤–æ–≤
        }
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π URL
        base_url = "http://localhost:8000"  # –í –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ –±–µ—Ä–∏—Ç–µ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
        webhook_url = f"{base_url}/webhooks/{webhook_id}"
        
        logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω –≤–µ–±—Ö—É–∫ {webhook_id} –¥–ª—è workflow {request.workflow_id}")
        
        return WebhookInfo(
            webhook_id=webhook_id,
            workflow_id=request.workflow_id,
            name=request.name,
            description=request.description,
            created_at=webhook_info["created_at"],
            url=webhook_url,
            auth_required=request.auth_required,
            allowed_ips=request.allowed_ips,
            call_count=0,
            last_called=None
        )
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≤–µ–±—Ö—É–∫–∞: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/webhooks/{webhook_id}")
async def trigger_webhook(
    webhook_id: str,
    request: Request,
    body: Dict[str, Any] = Body(...),
    authorization: Optional[str] = Header(None)
):
    logger.info(f"üîî Webhook {webhook_id} triggered")
    logger.info(f"üì¶ Received data: {json.dumps(body, ensure_ascii=False)[:200]}...")
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –≤—ã–∑–æ–≤–∞ –≤–µ–±—Ö—É–∫–∞"""
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –≤–µ–±—Ö—É–∫–∞
        if webhook_id not in webhook_triggers:
            raise HTTPException(status_code=404, detail="Webhook not found")
        
        webhook_info = webhook_triggers[webhook_id]
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ IP-–∞–¥—Ä–µ—Å–∞ –µ—Å–ª–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞
        client_ip = request.client.host
        if webhook_info.get("allowed_ips") and client_ip not in webhook_info["allowed_ips"]:
            logger.warning(f"‚ö†Ô∏è –ü–æ–ø—ã—Ç–∫–∞ –≤—ã–∑–æ–≤–∞ –≤–µ–±—Ö—É–∫–∞ {webhook_id} —Å –Ω–µ—Ä–∞–∑—Ä–µ—à–µ–Ω–Ω–æ–≥–æ IP: {client_ip}")
            raise HTTPException(status_code=403, detail="IP not allowed")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è
        if webhook_info.get("auth_required") and not authorization:
            raise HTTPException(status_code=401, detail="Authorization required")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        webhook_triggers[webhook_id]["call_count"] += 1
        webhook_triggers[webhook_id]["last_called"] = datetime.now().isoformat()
        webhook_stats[webhook_id]["total_calls"] += 1
        
        # –õ–æ–≥–∏—Ä—É–µ–º –≤—ã–∑–æ–≤
        logger.info(f"üîî –í–µ–±—Ö—É–∫ {webhook_id} –≤—ã–∑–≤–∞–Ω —Å –¥–∞–Ω–Ω—ã–º–∏: {json.dumps(body, ensure_ascii=False)[:200]}...")
        
        # –ü–æ–ª—É—á–∞–µ–º workflow
        workflow_id = webhook_info["workflow_id"]
        if workflow_id not in saved_workflows:
            raise HTTPException(status_code=404, detail="Associated workflow not found")
        
        workflow_data = saved_workflows[workflow_id]
        
        # –ù–∞—Ö–æ–¥–∏–º –Ω–æ–¥—É webhook_trigger –≤ workflow
        webhook_trigger_node = None
        for node in workflow_data["nodes"]:
            if node.get("type") == "webhook_trigger":
                webhook_trigger_node = node
                break
        
        if not webhook_trigger_node:
            # –ï—Å–ª–∏ –Ω–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–π –Ω–æ–¥—ã webhook_trigger, –Ω–∞—á–∏–Ω–∞–µ–º —Å –ø–µ—Ä–≤–æ–π –¥–æ—Å—Ç—É–ø–Ω–æ–π
            logger.warning(f"‚ö†Ô∏è –í workflow {workflow_id} –Ω–µ—Ç –Ω–æ–¥—ã webhook_trigger")
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–ø—Ä–æ—Å –Ω–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ workflow
        workflow_request = WorkflowExecuteRequest(
            nodes=workflow_data["nodes"],
            connections=workflow_data["connections"],
            startNodeId=webhook_trigger_node.get("id") if webhook_trigger_node else None
        )
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º workflow —Å –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        result = await execute_workflow_internal(workflow_request, initial_input_data=body)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —É—Å–ø–µ—à–Ω—ã—Ö –≤—ã–∑–æ–≤–æ–≤
        webhook_stats[webhook_id]["successful_calls"] += 1
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 10)
        call_record = {
            "timestamp": datetime.now().isoformat(),
            "success": True,
            "input_data": body,
            "result": result.result if result.success else None,
            "error": result.error if not result.success else None
        }
        
        webhook_stats[webhook_id]["call_history"].insert(0, call_record)
        webhook_stats[webhook_id]["call_history"] = webhook_stats[webhook_id]["call_history"][:10]
        
        if result.success:
            logger.info(f"‚úÖ –í–µ–±—Ö—É–∫ {webhook_id} —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–∏–ª workflow")
            return {
                "success": True,
                "webhook_id": webhook_id,
                "workflow_id": workflow_id,
                "result": result.result,
                "execution_time": datetime.now().isoformat()
            }
        else:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è workflow –¥–ª—è –≤–µ–±—Ö—É–∫–∞ {webhook_id}: {result.error}")
            webhook_stats[webhook_id]["failed_calls"] += 1
            webhook_stats[webhook_id]["last_error"] = result.error
            raise HTTPException(status_code=500, detail=result.error)
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–µ–±—Ö—É–∫–∞ {webhook_id}: {str(e)}")
        webhook_stats[webhook_id]["failed_calls"] += 1
        webhook_stats[webhook_id]["last_error"] = str(e)
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/webhooks")
async def list_webhooks():
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –≤–µ–±—Ö—É–∫–æ–≤"""
    webhooks = []
    for webhook_id, info in webhook_triggers.items():
        base_url = "http://localhost:8000"
        webhook_url = f"{base_url}/webhooks/{webhook_id}"
        
        webhooks.append(WebhookInfo(
            webhook_id=webhook_id,
            workflow_id=info["workflow_id"],
            name=info["name"],
            description=info.get("description"),
            created_at=info["created_at"],
            url=webhook_url,
            auth_required=info.get("auth_required", False),
            allowed_ips=info.get("allowed_ips"),
            call_count=info.get("call_count", 0),
            last_called=info.get("last_called")
        ))
    
    return {"webhooks": webhooks}

@app.get("/webhooks/{webhook_id}/info")
async def get_webhook_info(webhook_id: str):
    """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º –≤–µ–±—Ö—É–∫–µ"""
    if webhook_id not in webhook_triggers:
        raise HTTPException(status_code=404, detail="Webhook not found")
    
    info = webhook_triggers[webhook_id]
    stats = webhook_stats.get(webhook_id, {})
    
    base_url = "http://localhost:8000"
    webhook_url = f"{base_url}/webhooks/{webhook_id}"
    
    return {
        "webhook": WebhookInfo(
            webhook_id=webhook_id,
            workflow_id=info["workflow_id"],
            name=info["name"],
            description=info.get("description"),
            created_at=info["created_at"],
            url=webhook_url,
            auth_required=info.get("auth_required", False),
            allowed_ips=info.get("allowed_ips"),
            call_count=info.get("call_count", 0),
            last_called=info.get("last_called")
        ),
        "statistics": {
            "total_calls": stats.get("total_calls", 0),
            "successful_calls": stats.get("successful_calls", 0),
            "failed_calls": stats.get("failed_calls", 0),
            "last_error": stats.get("last_error"),
            "recent_calls": stats.get("call_history", [])[:5]
        }
    }

@app.delete("/webhooks/{webhook_id}")
async def delete_webhook(webhook_id: str):
    """–£–¥–∞–ª–∏—Ç—å –≤–µ–±—Ö—É–∫"""
    if webhook_id not in webhook_triggers:
        raise HTTPException(status_code=404, detail="Webhook not found")
    
    del webhook_triggers[webhook_id]
    if webhook_id in webhook_stats:
        del webhook_stats[webhook_id]
    
    logger.info(f"üóëÔ∏è –í–µ–±—Ö—É–∫ {webhook_id} —É–¥–∞–ª–µ–Ω")
    
    return {"message": f"Webhook {webhook_id} deleted successfully"}

@app.put("/webhooks/{webhook_id}")
async def update_webhook(webhook_id: str, request: WebhookCreateRequest):
    """–û–±–Ω–æ–≤–∏—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≤–µ–±—Ö—É–∫–∞"""
    if webhook_id not in webhook_triggers:
        raise HTTPException(status_code=404, detail="Webhook not found")
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
    webhook_triggers[webhook_id].update({
        "name": request.name,
        "description": request.description,
        "auth_required": request.auth_required,
        "allowed_ips": request.allowed_ips or []
    })
    
    logger.info(f"üîÑ –í–µ–±—Ö—É–∫ {webhook_id} –æ–±–Ω–æ–≤–ª–µ–Ω")
    
    return {"message": "Webhook updated successfully"}

async def execute_workflow_internal(request: WorkflowExecuteRequest, initial_input_data: Optional[Dict[str, Any]] = None):
    """–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è —Ñ—É–Ω–∫—Ü–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è workflow (–±–µ–∑ HTTP –æ–±–µ—Ä—Ç–∫–∏)"""
    logs = []
    # –ù–û–í–û–ï: –û—á–∏—â–∞–µ–º —Å—á–µ—Ç—á–∏–∫–∏ goto –ø—Ä–∏ –Ω–æ–≤–æ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏
    global goto_execution_counter
    goto_execution_counter.clear()

    try:
        logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ workflow —Å {len(request.nodes)} –Ω–æ–¥–∞–º–∏")
        if initial_input_data:
            logger.info(f"üí° Workflow –∑–∞–ø—É—â–µ–Ω —Å –Ω–∞—á–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏: {json.dumps(initial_input_data, default=str, indent=2)[:300]}...")
        
        # –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–¥ –∏ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π (–∏–∑ —Å—Ç–∞—Ä–æ–π –≤–µ—Ä—Å–∏–∏)
        for node in request.nodes:
            logger.info(f"üìã –ù–æ–¥–∞ {node.id} —Ç–∏–ø–∞ {node.type}: {node.data.get('label', '–ë–µ–∑ –º–µ—Ç–∫–∏')}")
        
        logger.info(f"üîó –°–æ–µ–¥–∏–Ω–µ–Ω–∏—è: {len(request.connections)}")
        for conn in request.connections:
            logger.info(f"üîó –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ: {conn.source} -> {conn.target}")
        
        # –•—Ä–∞–Ω–∏–ª–∏—â–µ –¥–ª—è –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è join –Ω–æ–¥
        join_node_data = {}
        
        # –ù–∞—Ö–æ–¥–∏–º —Å—Ç–∞—Ä—Ç–æ–≤—É—é –Ω–æ–¥—É
        start_node = None
        if request.startNodeId:
            start_node = next((n for n in request.nodes if n.id == request.startNodeId), None)
            logger.info(f"üéØ –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–∫–∞–∑–∞–Ω–Ω—É—é —Å—Ç–∞—Ä—Ç–æ–≤—É—é –Ω–æ–¥—É: {request.startNodeId}")
        else:
            # –ò—â–µ–º –Ω–æ–¥—É –±–µ–∑ –≤—Ö–æ–¥—è—â–∏—Ö —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
            node_ids_with_inputs = {conn.target for conn in request.connections}
            startable_types = ['gigachat', 'webhook', 'timer']
            
            start_candidates = [
                n for n in request.nodes 
                if n.type in startable_types and n.id not in node_ids_with_inputs
            ]
            
            if start_candidates:
                start_node = start_candidates[0]
                logger.info(f"üîç –ù–∞–π–¥–µ–Ω–∞ —Å—Ç–∞—Ä—Ç–æ–≤–∞—è –Ω–æ–¥–∞ –±–µ–∑ –≤—Ö–æ–¥—è—â–∏—Ö —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π: {start_node.id}")
            else:
                # –ï—Å–ª–∏ –≤—Å–µ –Ω–æ–¥—ã –∏–º–µ—é—Ç –≤—Ö–æ–¥—è—â–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è, –±–µ—Ä–µ–º –ª—é–±—É—é —Å—Ç–∞—Ä—Ç–æ–≤—É—é
                start_node = next((n for n in request.nodes if n.type in startable_types), None)
                logger.info(f"‚ö†Ô∏è –í—Å–µ –Ω–æ–¥—ã –∏–º–µ—é—Ç –≤—Ö–æ–¥—è—â–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è, –±–µ—Ä–µ–º –ø–µ—Ä–≤—É—é –¥–æ—Å—Ç—É–ø–Ω—É—é: {start_node.id if start_node else 'None'}")

        if not start_node:
            raise Exception("No startable node found")

        # –ï—Å–ª–∏ —ç—Ç–æ –Ω–æ–¥–∞ Timer, –æ–±–Ω–æ–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ workflow –≤ —Ç–∞–π–º–µ—Ä–µ
        if start_node.type == "timer":
            timer_id = f"timer_{start_node.id}"
            if timer_id in active_timers:
                active_timers[timer_id]["workflow"] = {
                    "nodes": request.nodes,
                    "connections": request.connections,
                    "startNodeId": start_node.id
                }

        # –í—ã–ø–æ–ª–Ω—è–µ–º workflow
        results = {}
        
        
        async def execute_node_recursive(node_id: str, input_data: Dict[str, Any] = None, source_node_id: str = None, is_first_node: bool = False):
            node = next((n for n in request.nodes if n.id == node_id), None)
            if not node:
                return None

            # –ï—Å–ª–∏ —ç—Ç–æ –ø–µ—Ä–≤–∞—è –Ω–æ–¥–∞ –ò –µ—Å—Ç—å initial_input_data, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö
            if is_first_node and initial_input_data:
                input_data = initial_input_data
                logger.info(f"üí° –°—Ç–∞—Ä—Ç–æ–≤–∞—è –Ω–æ–¥–∞ {node_id} –ø–æ–ª—É—á–∏–ª–∞ –Ω–∞—á–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ—Ç –≤–µ–±—Ö—É–∫–∞")
            
            # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è webhook –Ω–æ–¥—ã
            if node.type == 'webhook_trigger' and is_first_node:
                logger.info(f"üîî Webhook –Ω–æ–¥–∞ {node_id} –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω–∞")
                # Webhook –Ω–æ–¥–∞ –ø—Ä–æ—Å—Ç–æ –ø–µ—Ä–µ–¥–∞–µ—Ç –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–∞–ª—å—à–µ
                result = {
                    "success": True,
                    "output": input_data,
                    "message": "Webhook data received"
                }
                results[node_id] = result
                
                # –ü–µ—Ä–µ–¥–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å–ª–µ–¥—É—é—â–∏–º –Ω–æ–¥–∞–º
                next_connections = [c for c in request.connections if c.source == node_id]
                for connection in next_connections:
                    await execute_node_recursive(connection.target, result, node_id, False)
                
                return result
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ join –Ω–æ–¥–æ–π —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –≤—Ö–æ–¥–∞–º–∏
            incoming_connections = [c for c in request.connections if c.target == node_id]
            
            if node.type == 'join' and len(incoming_connections) > 1:
                # –≠—Ç–æ join –Ω–æ–¥–∞ - –Ω–∞–∫–∞–ø–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
                if node_id not in join_node_data:
                    join_node_data[node_id] = {
                        'expected_sources': set(c.source for c in incoming_connections),
                        'received_data': {}
                    }
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –æ—Ç —Ç–µ–∫—É—â–µ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
                if source_node_id:
                    join_node_data[node_id]['received_data'][source_node_id] = input_data
                    logger.info(f"üîÄ Join –Ω–æ–¥–∞ {node_id} –ø–æ–ª—É—á–∏–ª–∞ –¥–∞–Ω–Ω—ã–µ –æ—Ç {source_node_id}")
                    logger.info(f"üìä –û–∂–∏–¥–∞–µ—Ç—Å—è: {join_node_data[node_id]['expected_sources']}")
                    logger.info(f"üìä –ü–æ–ª—É—á–µ–Ω–æ –æ—Ç: {set(join_node_data[node_id]['received_data'].keys())}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ–ª—É—á–∏–ª–∏ –ª–∏ –º—ã –¥–∞–Ω–Ω—ã–µ –æ—Ç –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
                received_sources = set(join_node_data[node_id]['received_data'].keys())
                expected_sources = join_node_data[node_id]['expected_sources']
                
                if node.data.get('config', {}).get('waitForAll', True):
                    if received_sources != expected_sources:
                        # –ï—â–µ –Ω–µ –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –ø–æ–ª—É—á–µ–Ω—ã
                        logger.info(f"‚è≥ Join –Ω–æ–¥–∞ {node_id} –∂–¥–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ—Ç {expected_sources - received_sources}")
                        return None
                
                # –í—Å–µ –¥–∞–Ω–Ω—ã–µ –ø–æ–ª—É—á–µ–Ω—ã –∏–ª–∏ –Ω–µ –∂–¥–µ–º –≤—Å–µ—Ö - –≤—ã–ø–æ–ª–Ω—è–µ–º join
                input_data = {'inputs': join_node_data[node_id]['received_data']}
                
                # –û—á–∏—â–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                del join_node_data[node_id]
            
            # –õ–æ–≥–∏—Ä—É–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
            logs.append({
                "message": f"Executing {node.data.get('label', node.type)}...",
                "timestamp": datetime.now().isoformat(),
                "level": "info",
                "nodeId": node_id
            })

            # –í—ã–ø–æ–ª–Ω—è–µ–º –Ω–æ–¥—É
            executor_map = {
                'gigachat': executors.execute_gigachat,
                'email': executors.execute_email,
                'database': executors.execute_database,
                'webhook': executors.execute_webhook,
                'timer': executors.execute_timer,
                'join': executors.execute_join,
                'request_iterator': executors.execute_request_iterator,
                'if_else': executors.execute_if_else,
                'dispatcher': executors.execute_dispatcher # –ù–û–í–û–ï
            }

            executor = executor_map.get(node.type)
            if executor:
                result = await executor(node, input_data or {})
                results[node_id] = result
                
                logs.append({
                    "message": f"{node.data.get('label', node.type)} completed successfully",
                    "timestamp": datetime.now().isoformat(),
                    "level": "success",
                    "nodeId": node_id
                })

                # –ù–∞—Ö–æ–¥–∏–º —Å–ª–µ–¥—É—é—â–∏–µ –Ω–æ–¥—ã
                next_connections = [c for c in request.connections if c.source == node_id]

                # –ù–û–í–û–ï: –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è If/Else –Ω–æ–¥—ã
                if node.type == 'if_else' and 'branch' in result:
                    branch = result['branch']  # 'true' –∏–ª–∏ 'false'
                    
                    # –î–û–ë–ê–í–¨–¢–ï –ó–î–ï–°–¨ (–ø–æ—Å–ª–µ —Å—Ç—Ä–æ–∫–∏ 1097):
                    logger.info(f"üîç If/Else –Ω–æ–¥–∞ {node_id} –≤–µ—Ä–Ω—É–ª–∞ branch: {branch}")
                    logger.info(f"üîç –ù–∞–π–¥–µ–Ω–æ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π: {len(next_connections)}")
                    for conn in next_connections:
                        logger.info(f"  - {conn.id}: label='{conn.data.get('label', 'none')}'")

                    # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è –ø–æ –º–µ—Ç–∫–µ
                    filtered_connections = []
                    goto_connections = []
                    
                    for conn in next_connections:
                        conn_label = conn.data.get('label', '').lower() if conn.data else ''
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º goto —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
                        if conn_label == f"{branch}:goto":
                            goto_connections.append(conn)
                        elif conn_label == branch:
                            filtered_connections.append(conn)
                    
                    # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º goto —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
                    if goto_connections:
                        for goto_conn in goto_connections:
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞—â–∏—Ç—É –æ—Ç —Ü–∏–∫–ª–æ–≤
                            goto_key = f"{node_id}_to_{goto_conn.target}"
                            if goto_key not in goto_execution_counter:
                                goto_execution_counter[goto_key] = 0
                            
                            goto_execution_counter[goto_key] += 1
                            max_iterations = node.data.get('config', {}).get('maxGotoIterations', 10)
                            
                            if goto_execution_counter[goto_key] > max_iterations:
                                logger.error(f"üîÑ –ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç goto –ø–µ—Ä–µ—Ö–æ–¥–æ–≤ ({max_iterations}) –¥–ª—è {goto_key}")
                                return {
                                    'success': False,
                                    'error': f'Exceeded max goto iterations ({max_iterations})',
                                    'branch': branch,
                                    'goto_iterations': goto_execution_counter[goto_key]
                                }
                            
                            logger.info(f"üîÑ –ê–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω goto –ø–µ—Ä–µ—Ö–æ–¥ –∫ {goto_conn.target} (–∏—Ç–µ—Ä–∞—Ü–∏—è {goto_execution_counter[goto_key]})")
                            await execute_node_recursive(goto_conn.target, result, node_id, False)
                        
                        # –ü–æ—Å–ª–µ goto –Ω–µ –≤—ã–ø–æ–ª–Ω—è–µ–º –æ–±—ã—á–Ω—ã–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
                        return result
                    
                    # –ï—Å–ª–∏ –Ω–µ—Ç goto, –≤—ã–ø–æ–ª–Ω—è–µ–º –æ–±—ã—á–Ω—ã–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
                    for connection in filtered_connections:
                        await execute_node_recursive(connection.target, result, node_id)
                else:
                    # –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Ç–∏–ø–æ–≤ –Ω–æ–¥ - –æ–±—ã—á–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
                    for connection in next_connections:
                        await execute_node_recursive(connection.target, result, node_id)

                return result
            else:
                raise Exception(f"Unknown node type: {node.type}")

        # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
        await execute_node_recursive(start_node.id, is_first_node=True)

        return ExecutionResult(
            success=True,
            result=results,
            logs=logs
        )

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è workflow: {str(e)}")
        return ExecutionResult(
            success=False,
            error=str(e),
            logs=logs + [{
                "message": f"Workflow execution failed: {str(e)}",
                "timestamp": datetime.now().isoformat(),
                "level": "error"
            }]
        )


@app.post("/execute-workflow")
async def execute_workflow(request: WorkflowExecuteRequest):
    return await execute_workflow_internal(request)

# –î–æ–±–∞–≤—å—Ç–µ –Ω–æ–≤—ã–π —ç–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–æ–≤ –Ω–æ–¥
@app.post("/node-status")
async def get_node_status(node_ids: List[str]):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –Ω–æ–¥"""
    results = {}
    
    for node_id in node_ids:
        if node_id in node_execution_results:
            # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —É—Å—Ç–∞—Ä–µ–ª –ª–∏ –æ–Ω (–Ω–µ —Å—Ç–∞—Ä—à–µ 5 –º–∏–Ω—É—Ç)
            result_data = node_execution_results[node_id]
            timestamp = datetime.fromisoformat(result_data["timestamp"])
            
            if datetime.now() - timestamp < timedelta(minutes=5):
                results[node_id] = result_data
                
                # –û—á–∏—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ—Å–ª–µ –æ—Ç–ø—Ä–∞–≤–∫–∏, —á—Ç–æ–±—ã –Ω–µ –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å –µ–≥–æ –ø–æ–≤—Ç–æ—Ä–Ω–æ
                # –ø—Ä–∏ —Å–ª–µ–¥—É—é—â–µ–º –∑–∞–ø—Ä–æ—Å–µ (—ç—Ç–æ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç –ø–æ–≤—Ç–æ—Ä–Ω–æ–µ –ø–æ–¥—Å–≤–µ—á–∏–≤–∞–Ω–∏–µ –Ω–æ–¥—ã)
                del node_execution_results[node_id]
    
    return {"results": results}

# –ù–æ–≤—ã–µ —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ç–∞–π–º–µ—Ä–∞–º–∏

@app.get("/timers")
async def get_timers():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –≤—Å–µ—Ö –∞–∫—Ç–∏–≤–Ω—ã—Ö —Ç–∞–π–º–µ—Ä–æ–≤"""
    timers = []
    for timer_id, timer in active_timers.items():
        timers.append({
            "id": timer_id,
            "node_id": timer["node_id"],
            "interval": timer["interval"],
            "next_execution": timer["next_execution"].isoformat(),
            "status": timer["status"]
        })
    return {"timers": timers}

@app.get("/timers/{timer_id}")
async def get_timer(timer_id: str):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º —Ç–∞–π–º–µ—Ä–µ"""
    if timer_id not in active_timers:
        raise HTTPException(status_code=404, detail=f"Timer {timer_id} not found")
    
    timer = active_timers[timer_id]
    return {
        "id": timer_id,
        "node_id": timer["node_id"],
        "interval": timer["interval"],
        "next_execution": timer["next_execution"].isoformat(),
        "status": timer["status"]
    }

@app.post("/timers/{timer_id}/pause")
async def pause_timer(timer_id: str):
    """–ü—Ä–∏–æ—Å—Ç–∞–Ω–æ–≤–∫–∞ —Ç–∞–π–º–µ—Ä–∞"""
    if timer_id not in active_timers:
        raise HTTPException(status_code=404, detail=f"Timer {timer_id} not found")
    
    # –û—Ç–º–µ–Ω—è–µ–º –∑–∞–¥–∞—á—É —Ç–∞–π–º–µ—Ä–∞
    if active_timers[timer_id]["task"] is not None:
        active_timers[timer_id]["task"].cancel()
    
    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å
    active_timers[timer_id]["status"] = "paused"
    
    logger.info(f"‚è∏Ô∏è –¢–∞–π–º–µ—Ä {timer_id} –ø—Ä–∏–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    
    return {
        "id": timer_id,
        "status": "paused",
        "message": "Timer paused successfully"
    }

@app.post("/timers/{timer_id}/resume")
async def resume_timer(timer_id: str):
    """–í–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç–∞–π–º–µ—Ä–∞"""
    if timer_id not in active_timers:
        raise HTTPException(status_code=404, detail=f"Timer {timer_id} not found")
    
    timer = active_timers[timer_id]
    
    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –∑–∞–¥–∞—á—É —Ç–∞–π–º–µ—Ä–∞
    task = asyncio.create_task(timer_task(
        timer_id, 
        timer["node_id"], 
        timer["interval"], 
        timer["workflow"]
    ))
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–∞–π–º–µ—Ä–µ
    timer["task"] = task
    timer["status"] = "active"
    timer["next_execution"] = datetime.now() + timedelta(minutes=timer["interval"])
    
    logger.info(f"‚ñ∂Ô∏è –¢–∞–π–º–µ—Ä {timer_id} –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω")
    
    return {
        "id": timer_id,
        "status": "active",
        "next_execution": timer["next_execution"].isoformat(),
        "message": "Timer resumed successfully"
    }

@app.delete("/timers/{timer_id}")
async def delete_timer(timer_id: str):
    """–£–¥–∞–ª–µ–Ω–∏–µ —Ç–∞–π–º–µ—Ä–∞"""
    if timer_id not in active_timers:
        raise HTTPException(status_code=404, detail=f"Timer {timer_id} not found")
    
    # –û—Ç–º–µ–Ω—è–µ–º –∑–∞–¥–∞—á—É —Ç–∞–π–º–µ—Ä–∞
    if active_timers[timer_id]["task"] is not None:
        active_timers[timer_id]["task"].cancel()
    
    # –£–¥–∞–ª—è–µ–º —Ç–∞–π–º–µ—Ä –∏–∑ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
    del active_timers[timer_id]
    
    logger.info(f"üóëÔ∏è –¢–∞–π–º–µ—Ä {timer_id} —É–¥–∞–ª–µ–Ω")
    
    return {
        "message": f"Timer {timer_id} deleted successfully"
    }

@app.post("/timers/{timer_id}/update")
async def update_timer_endpoint(timer_id: str, interval: int):
    """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ —Ç–∞–π–º–µ—Ä–∞"""
    if timer_id not in active_timers:
        raise HTTPException(status_code=404, detail=f"Timer {timer_id} not found")
    
    result = await update_timer(timer_id, interval)
    
    logger.info(f"üîÑ –¢–∞–π–º–µ—Ä {timer_id} –æ–±–Ω–æ–≤–ª–µ–Ω —Å –Ω–æ–≤—ã–º –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–º {interval} –º–∏–Ω—É—Ç")
    
    return result

@app.post("/timers/{timer_id}/execute-now")
async def execute_timer_now(timer_id: str):
    """–ù–µ–º–µ–¥–ª–µ–Ω–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Ç–∞–π–º–µ—Ä–∞"""
    if timer_id not in active_timers:
        raise HTTPException(status_code=404, detail=f"Timer {timer_id} not found")
    
    timer = active_timers[timer_id]
    
    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ workflow
    workflow_info = timer["workflow"]
    
    try:
        # –°–æ–∑–¥–∞–µ–º –∑–∞–ø—Ä–æ—Å –Ω–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ workflow
        workflow_request = WorkflowExecuteRequest(
            nodes=workflow_info["nodes"],
            connections=workflow_info["connections"],
            startNodeId=timer["node_id"]
        )
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º workflow
        result = await execute_workflow_internal(workflow_request)
        
        logger.info(f"‚úÖ –¢–∞–π–º–µ—Ä {timer_id} –≤—ã–ø–æ–ª–Ω–µ–Ω –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ")
        
        return result
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ —Ç–∞–π–º–µ—Ä–∞ {timer_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∑–∞–ø—É—Å–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞
@app.on_event("startup")
async def startup_event():
    """–í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ —Å–µ—Ä–≤–µ—Ä–∞"""
    _load_workflows_from_disk() # –ù–û–í–û–ï: –ó–∞–≥—Ä—É–∂–∞–µ–º workflows
    logger.info("üöÄ –°–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω")

# –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ —Å–µ—Ä–≤–µ—Ä–∞
@app.on_event("shutdown")
async def shutdown_event():
    """–í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø—Ä–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–µ —Å–µ—Ä–≤–µ—Ä–∞"""
    logger.info("üõë –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤—Å–µ—Ö —Ç–∞–π–º–µ—Ä–æ–≤...")
    
    # –û—Ç–º–µ–Ω—è–µ–º –≤—Å–µ –∞–∫—Ç–∏–≤–Ω—ã–µ —Ç–∞–π–º–µ—Ä—ã
    for timer_id, timer in active_timers.items():
        if timer["task"] is not None:
            timer["task"].cancel()
    
    logger.info("üëã –°–µ—Ä–≤–µ—Ä –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

if __name__ == "__main__":
    import uvicorn
    print("üöÄ –ó–∞–ø—É—Å–∫ FastAPI —Å–µ—Ä–≤–µ—Ä–∞...")
    print("üì° API –±—É–¥–µ—Ç –¥–æ—Å—Ç—É–ø–µ–Ω –Ω–∞: http://localhost:8000")
    print("üìö –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: http://localhost:8000/docs")
    uvicorn.run(app, host="0.0.0.0", port=8000)
